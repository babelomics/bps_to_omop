{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New find_visit_occurence_id\n",
    "\n",
    "Resulta que el código de la función find_visit_occurence_id se nos ha quedado corto. En algunos casos las tablas generadas son demasido grandes para que quepan en memoria.\n",
    "\n",
    "En este notebook vamos a intentar:\n",
    "\n",
    "1. Permitir procesar dataframes que no quepan en memoria\n",
    "2. Como objetivo secundario, estaría bien poder paralelizarlo de alguna manera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un dataset que tenga todas las variantes de datos que podamos encontrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input = []\n",
    "visit = []\n",
    "\n",
    "# 1 -> Person that has one event and one visit that match\n",
    "input_rows = [\n",
    "    {\"person_id\": 1, \"event_id\": 1, \"start_date\": \"2020-01-05\", \"expected_visit_id\": 1}\n",
    "]\n",
    "visit_rows = [\n",
    "    {\n",
    "        \"person_id\": 1,\n",
    "        \"visit_id\": 1,\n",
    "        \"start_date\": \"2020-01-01\",\n",
    "        \"end_date\": \"2020-01-10\",\n",
    "    }\n",
    "]\n",
    "input += input_rows\n",
    "visit += visit_rows\n",
    "# 2 -> Person that has one event and one visit that do not match\n",
    "input_rows = [\n",
    "    {\n",
    "        \"person_id\": 2,\n",
    "        \"event_id\": 2,\n",
    "        \"start_date\": \"2020-02-05\",\n",
    "        \"expected_visit_id\": None,\n",
    "    }\n",
    "]\n",
    "visit_rows = [\n",
    "    {\n",
    "        \"person_id\": 2,\n",
    "        \"visit_id\": 2,\n",
    "        \"start_date\": \"2020-01-01\",\n",
    "        \"end_date\": \"2020-01-10\",\n",
    "    }\n",
    "]\n",
    "input += input_rows\n",
    "visit += visit_rows\n",
    "# 3 -> Person that has one event and no visit\n",
    "input_rows = [\n",
    "    {\n",
    "        \"person_id\": 3,\n",
    "        \"event_id\": 3,\n",
    "        \"start_date\": \"2020-02-05\",\n",
    "        \"expected_visit_id\": None,\n",
    "    }\n",
    "]\n",
    "input += input_rows\n",
    "# 4 -> Person that has no event and one visit\n",
    "visit_rows = [\n",
    "    {\n",
    "        \"person_id\": 4,\n",
    "        \"visit_id\": 3,\n",
    "        \"start_date\": \"2020-01-01\",\n",
    "        \"end_date\": \"2020-01-10\",\n",
    "    }\n",
    "]\n",
    "visit += visit_rows\n",
    "# 5 -> Person that has two events and two visits. One match and the rest do not\n",
    "input_rows = [\n",
    "    {\"person_id\": 5, \"event_id\": 4, \"start_date\": \"2020-01-05\", \"expected_visit_id\": 4}\n",
    "]\n",
    "visit_rows = [\n",
    "    {\n",
    "        \"person_id\": 5,\n",
    "        \"visit_id\": 4,\n",
    "        \"start_date\": \"2020-01-01\",\n",
    "        \"end_date\": \"2020-01-10\",\n",
    "    }\n",
    "]\n",
    "input += input_rows\n",
    "visit += visit_rows\n",
    "input_rows = [\n",
    "    {\n",
    "        \"person_id\": 5,\n",
    "        \"event_id\": 5,\n",
    "        \"start_date\": \"2020-02-05\",\n",
    "        \"expected_visit_id\": None,\n",
    "    }\n",
    "]\n",
    "visit_rows = [\n",
    "    {\n",
    "        \"person_id\": 5,\n",
    "        \"visit_id\": 5,\n",
    "        \"start_date\": \"2020-03-01\",\n",
    "        \"end_date\": \"2020-03-10\",\n",
    "    }\n",
    "]\n",
    "input += input_rows\n",
    "visit += visit_rows\n",
    "# 6 -> Person that has two events tha match to a single visit\n",
    "input_rows = [\n",
    "    {\"person_id\": 6, \"event_id\": 6, \"start_date\": \"2020-01-04\", \"expected_visit_id\": 6},\n",
    "    {\"person_id\": 6, \"event_id\": 7, \"start_date\": \"2020-01-05\", \"expected_visit_id\": 6},\n",
    "]\n",
    "visit_rows = [\n",
    "    {\n",
    "        \"person_id\": 6,\n",
    "        \"visit_id\": 6,\n",
    "        \"start_date\": \"2020-01-01\",\n",
    "        \"end_date\": \"2020-01-10\",\n",
    "    }\n",
    "]\n",
    "input += input_rows\n",
    "visit += visit_rows\n",
    "# 7 -> Person that has one event that fits the end of one period and the beginning of the next\n",
    "input_rows = [\n",
    "    {\"person_id\": 7, \"event_id\": 8, \"start_date\": \"2020-01-05\", \"expected_visit_id\": 7}\n",
    "]\n",
    "visit_rows = [\n",
    "    {\n",
    "        \"person_id\": 7,\n",
    "        \"visit_id\": 7,\n",
    "        \"start_date\": \"2020-01-01\",\n",
    "        \"end_date\": \"2020-01-05\",\n",
    "    },\n",
    "    {\n",
    "        \"person_id\": 7,\n",
    "        \"visit_id\": 8,\n",
    "        \"start_date\": \"2020-01-05\",\n",
    "        \"end_date\": \"2020-01-10\",\n",
    "    },\n",
    "]\n",
    "input += input_rows\n",
    "visit += visit_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.DataFrame.from_records(input)\n",
    "visit_df = pd.DataFrame.from_records(visit)\n",
    "visit_df = visit_df.rename(\n",
    "    {\n",
    "        \"visit_id\": \"visit_occurrence_id\",\n",
    "        \"start_date\": \"visit_start_datetime\",\n",
    "        \"end_date\": \"visit_end_datetime\",\n",
    "    },\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Búsqueda de visit_occurrence_id\n",
    "El objetivo consiste en enlazar cada medida del paciente con una visita. Para ello cargaremos la tabla `visit_df`, que ya debería haber sido construida en una sección anterior, y buscaremos para cada measurement_date de la tabla `input_df` un intervalo de fechas de visitas que la contenga. Si existe, le asignaremos el `visit_occurrence_id` correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método implementado actualmente (18/06/2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_visit_occurence_id(\n",
    "    events_df: pd.DataFrame,\n",
    "    event_columns: list,\n",
    "    visits_df: pd.DataFrame,\n",
    "    verbose: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find valid date ranges by merging condition and visit occurrence data.\n",
    "\n",
    "    This function merges input_df and visit occurrence dataframes,\n",
    "    then filters for input_df start dates that fall within visit date ranges.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    events_df : pandas.DataFrame\n",
    "        Input dataframe for which to assign visit_occurrence_id's\n",
    "    event_columns : list\n",
    "        Column names that contains, in this order:\n",
    "        - 'person_id'   Identifier for each person in the dataframe\n",
    "        - 'start_date'  Date to fit between 'visit_start_date' and 'visit_end_date'.\n",
    "        - 'events_id'   Unique identifier for each registry of events_df.\n",
    "    visits_df : pandas.DataFrame\n",
    "        DataFrame containing visit occurrence data.\n",
    "        Must include columns: 'person_id', 'visit_start_date',\n",
    "        'visit_end_date', 'visit_occurrence_id'.\n",
    "        Column names need to be the same. This is to ensure\n",
    "        the correct table (VISIT_OCCURRENCE) is being used.\n",
    "    verbose : int, optional, default 0\n",
    "        Verbosity level for function output.\n",
    "        0: No output\n",
    "        1: Additionally, print state of the processing\n",
    "        2+: Additionally, print all debug information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame containing the original table event_df plus the value for\n",
    "        the visit_occurence_id, visit_start_date and visit_end_date, if found.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If required columns are missing in input DataFrames.\n",
    "    \"\"\"\n",
    "    pd.options.mode.copy_on_write = True\n",
    "    # == Initial message ==============================================\n",
    "    if verbose > 0:\n",
    "        print(\"Looking for visit_occurrence_id matches:\")\n",
    "\n",
    "    # == Initial Checks ===============================================\n",
    "    # Check for required columns in events_df\n",
    "    if verbose > 0:\n",
    "        print(\" Checking input...\")\n",
    "    required_input_columns = event_columns\n",
    "    missing_input_columns = set(required_input_columns) - set(events_df.columns)\n",
    "    if missing_input_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in events_df: {missing_input_columns}\"\n",
    "        )\n",
    "    # Check for required columns in visits_df\n",
    "    required_visit_columns = [\n",
    "        \"person_id\",\n",
    "        \"visit_start_datetime\",\n",
    "        \"visit_end_datetime\",\n",
    "        \"visit_occurrence_id\",\n",
    "    ]\n",
    "    missing_visit_columns = set(required_visit_columns) - set(visits_df.columns)\n",
    "    if missing_visit_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in visits_df: {missing_visit_columns}\"\n",
    "        )\n",
    "    visits_df = visits_df[required_visit_columns]\n",
    "\n",
    "    # == Force dtypes and sort ========================================\n",
    "    # Ensure start_date and visit dates are datetime\n",
    "    if verbose > 0:\n",
    "        print(\" Sorting dataframes...\")\n",
    "    events_df[event_columns[1]] = events_df[event_columns[1]].astype(\"datetime64[ms]\")\n",
    "    visits_df[\"visit_start_datetime\"] = visits_df[\"visit_start_datetime\"].astype(\n",
    "        \"datetime64[ms]\"\n",
    "    )\n",
    "    visits_df[\"visit_end_datetime\"] = visits_df[\"visit_end_datetime\"].astype(\n",
    "        \"datetime64[ms]\"\n",
    "    )\n",
    "\n",
    "    # Drop all duplicates, if visits are not unique we cannot assign them\n",
    "    visits_df = visits_df.drop_duplicates(\n",
    "        subset=[\"person_id\", \"visit_start_datetime\", \"visit_end_datetime\"], keep=False\n",
    "    )\n",
    "\n",
    "    # Sort the neccesary columns of dataframes\n",
    "    events_df = events_df.sort_values([event_columns[0], event_columns[1]])\n",
    "    visits_df = visits_df.sort_values(\n",
    "        [event_columns[0], \"visit_start_datetime\", \"visit_end_datetime\"]\n",
    "    )\n",
    "\n",
    "    # == Merging ======================================================\n",
    "    if verbose > 0:\n",
    "        print(\" Combining results...\")\n",
    "    merged_df = pd.merge(\n",
    "        events_df.reset_index(drop=True),\n",
    "        visits_df.reset_index(drop=True),\n",
    "        on=event_columns[0],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Check if merge resulted in any matches\n",
    "    if merged_df[\"visit_occurrence_id\"].isna().all():\n",
    "        raise ValueError(\n",
    "            (\n",
    "                \"No matching records found after merging.\"\n",
    "                + \"Check if person_id values align between dataframes.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # == Filter for valid ranges ======================================\n",
    "    if verbose > 0:\n",
    "        print(\" Filtering valid ranges...\")\n",
    "    # Create mask for dates within range\n",
    "    date_range_mask = (\n",
    "        merged_df[event_columns[1]] >= merged_df[\"visit_start_datetime\"]\n",
    "    ) & (merged_df[event_columns[1]] <= merged_df[\"visit_end_datetime\"])\n",
    "    # Filter only valid ranges\n",
    "    valid_ranges = merged_df[date_range_mask]\n",
    "\n",
    "    # Merge with original to retrieve events without visit_occurrence_id\n",
    "    final_df = pd.merge(\n",
    "        events_df,\n",
    "        valid_ranges[\n",
    "            [\n",
    "                event_columns[0],\n",
    "                event_columns[2],\n",
    "                \"visit_occurrence_id\",\n",
    "                \"visit_start_datetime\",\n",
    "                \"visit_end_datetime\",\n",
    "            ]\n",
    "        ],\n",
    "        on=[event_columns[0], event_columns[2]],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    # Sometimes, there might be events that land in visits that share a day.\n",
    "    # Those would be duplicated on the event_id. Let's drop those duplicates\n",
    "    # Since they're ordered, we will only lose the second visit, the one\n",
    "    # that starts with the event\n",
    "    final_df = final_df.drop_duplicates([event_columns[0], event_columns[2]])\n",
    "\n",
    "    if verbose > 1:\n",
    "        if valid_ranges.empty:\n",
    "            print(\n",
    "                (\n",
    "                    \" Warning: No valid date ranges found.\"\n",
    "                    + \"All condition start dates are outside visit date ranges.\"\n",
    "                )\n",
    "            )\n",
    "        print(f\"  Shape of events_df: {events_df.shape}\")\n",
    "        print(f\"  Shape of visits_df: {visits_df.shape}\")\n",
    "        print(f\"  Shape of merged_df: {merged_df.shape}\")\n",
    "        print(f\"  Shape of valid_ranges: {valid_ranges.shape}\")\n",
    "        print(f\"  Shape of final_df: {final_df.shape}\")\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\" Done.\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = find_visit_occurence_id(\n",
    "    input_df, [\"person_id\", \"start_date\", \"event_id\"], visit_df, verbose=2\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)    \n",
    "    print('find_visit_occurence_id')\n",
    "    %timeit -n 5 -r 5 find_visit_occurence_id(input_df, [\"person_id\", \"start_date\", \"event_id\"], visit_df, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando fireducks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fireducks.pandas as pdfd\n",
    "\n",
    "def find_visit_occurence_id_fireducks(\n",
    "    events_df: pdfd.DataFrame,\n",
    "    event_columns: list,\n",
    "    visits_df: pdfd.DataFrame,\n",
    "    verbose: int = 0,\n",
    ") -> pdfd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find valid date ranges by merging condition and visit occurrence data.\n",
    "\n",
    "    This function merges input_df and visit occurrence dataframes,\n",
    "    then filters for input_df start dates that fall within visit date ranges.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    events_df : pandas.DataFrame\n",
    "        Input dataframe for which to assign visit_occurrence_id's\n",
    "    event_columns : list\n",
    "        Column names that contains, in this order:\n",
    "        - 'person_id'   Identifier for each person in the dataframe\n",
    "        - 'start_date'  Date to fit between 'visit_start_date' and 'visit_end_date'.\n",
    "        - 'events_id'   Unique identifier for each registry of events_df.\n",
    "    visits_df : pandas.DataFrame\n",
    "        DataFrame containing visit occurrence data.\n",
    "        Must include columns: 'person_id', 'visit_start_date',\n",
    "        'visit_end_date', 'visit_occurrence_id'.\n",
    "        Column names need to be the same. This is to ensure\n",
    "        the correct table (VISIT_OCCURRENCE) is being used.\n",
    "    verbose : int, optional, default 0\n",
    "        Verbosity level for function output.\n",
    "        0: No output\n",
    "        1: Additionally, print state of the processing\n",
    "        2+: Additionally, print all debug information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame containing the original table event_df plus the value for\n",
    "        the visit_occurence_id, visit_start_date and visit_end_date, if found.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If required columns are missing in input DataFrames.\n",
    "    \"\"\"\n",
    "    pdfd.options.mode.copy_on_write = True\n",
    "    # == Initial message ==============================================\n",
    "    if verbose > 0:\n",
    "        print(\"Looking for visit_occurrence_id matches:\")\n",
    "\n",
    "    # == Initial Checks ===============================================\n",
    "    # Check for required columns in events_df\n",
    "    if verbose > 0:\n",
    "        print(\" Checking input...\")\n",
    "    required_input_columns = event_columns\n",
    "    missing_input_columns = set(required_input_columns) - set(events_df.columns)\n",
    "    if missing_input_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in events_df: {missing_input_columns}\"\n",
    "        )\n",
    "    # Check for required columns in visits_df\n",
    "    required_visit_columns = [\n",
    "        \"person_id\",\n",
    "        \"visit_start_datetime\",\n",
    "        \"visit_end_datetime\",\n",
    "        \"visit_occurrence_id\",\n",
    "    ]\n",
    "    missing_visit_columns = set(required_visit_columns) - set(visits_df.columns)\n",
    "    if missing_visit_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in visits_df: {missing_visit_columns}\"\n",
    "        )\n",
    "    visits_df = visits_df[required_visit_columns]\n",
    "\n",
    "    # == Force dtypes and sort ========================================\n",
    "    # Ensure start_date and visit dates are datetime\n",
    "    if verbose > 0:\n",
    "        print(\" Sorting dataframes...\")\n",
    "    events_df[event_columns[1]] = events_df[event_columns[1]].astype(\"datetime64[ms]\")\n",
    "    visits_df[\"visit_start_datetime\"] = visits_df[\"visit_start_datetime\"].astype(\n",
    "        \"datetime64[ms]\"\n",
    "    )\n",
    "    visits_df[\"visit_end_datetime\"] = visits_df[\"visit_end_datetime\"].astype(\n",
    "        \"datetime64[ms]\"\n",
    "    )\n",
    "\n",
    "    # Drop all duplicates, if visits are not unique we cannot assign them\n",
    "    visits_df = visits_df.drop_duplicates(\n",
    "        subset=[\"person_id\", \"visit_start_datetime\", \"visit_end_datetime\"], keep=False\n",
    "    )\n",
    "\n",
    "    # Sort the neccesary columns of dataframes\n",
    "    events_df = events_df.sort_values([event_columns[0], event_columns[1]])\n",
    "    visits_df = visits_df.sort_values(\n",
    "        [event_columns[0], \"visit_start_datetime\", \"visit_end_datetime\"]\n",
    "    )\n",
    "\n",
    "    # == Merging ======================================================\n",
    "    if verbose > 0:\n",
    "        print(\" Combining results...\")\n",
    "    merged_df = pdfd.merge(\n",
    "        events_df.reset_index(drop=True),\n",
    "        visits_df.reset_index(drop=True),\n",
    "        on=event_columns[0],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Check if merge resulted in any matches\n",
    "    if merged_df[\"visit_occurrence_id\"].isna().all():\n",
    "        raise ValueError(\n",
    "            (\n",
    "                \"No matching records found after merging.\"\n",
    "                + \"Check if person_id values align between dataframes.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # == Filter for valid ranges ======================================\n",
    "    if verbose > 0:\n",
    "        print(\" Filtering valid ranges...\")\n",
    "    # Create mask for dates within range\n",
    "    date_range_mask = (\n",
    "        merged_df[event_columns[1]] >= merged_df[\"visit_start_datetime\"]\n",
    "    ) & (merged_df[event_columns[1]] <= merged_df[\"visit_end_datetime\"])\n",
    "    # Filter only valid ranges\n",
    "    valid_ranges = merged_df[date_range_mask]\n",
    "\n",
    "    # Merge with original to retrieve events without visit_occurrence_id\n",
    "    final_df = pdfd.merge(\n",
    "        events_df,\n",
    "        valid_ranges[\n",
    "            [\n",
    "                event_columns[0],\n",
    "                event_columns[2],\n",
    "                \"visit_occurrence_id\",\n",
    "                \"visit_start_datetime\",\n",
    "                \"visit_end_datetime\",\n",
    "            ]\n",
    "        ],\n",
    "        on=[event_columns[0], event_columns[2]],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    # Sometimes, there might be events that land in visits that share a day.\n",
    "    # Those would be duplicated on the event_id. Let's drop those duplicates\n",
    "    # Since they're ordered, we will only lose the second visit, the one\n",
    "    # that starts with the event\n",
    "    final_df = final_df.drop_duplicates([event_columns[0], event_columns[2]])\n",
    "\n",
    "    if verbose > 1:\n",
    "        if valid_ranges.empty:\n",
    "            print(\n",
    "                (\n",
    "                    \" Warning: No valid date ranges found.\"\n",
    "                    + \"All condition start dates are outside visit date ranges.\"\n",
    "                )\n",
    "            )\n",
    "        print(f\"  Shape of events_df: {events_df.shape}\")\n",
    "        print(f\"  Shape of visits_df: {visits_df.shape}\")\n",
    "        print(f\"  Shape of merged_df: {merged_df.shape}\")\n",
    "        print(f\"  Shape of valid_ranges: {valid_ranges.shape}\")\n",
    "        print(f\"  Shape of final_df: {final_df.shape}\")\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\" Done.\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)    \n",
    "    print('find_visit_occurence_id_fireducks')\n",
    "    %timeit -n 5 -r 5 find_visit_occurence_id_fireducks(input_df, [\"person_id\", \"start_date\", \"event_id\"], visit_df, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "def find_visit_occurence_id_polars(\n",
    "    events_df: pd.DataFrame,\n",
    "    event_columns: list,\n",
    "    visits_df: pd.DataFrame,\n",
    "    verbose: int = 0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find valid date ranges by merging condition and visit occurrence data.\n",
    "\n",
    "    This function merges input_df and visit occurrence dataframes,\n",
    "    then filters for input_df start dates that fall within visit date ranges.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    events_df : pandas.DataFrame\n",
    "        Input dataframe for which to assign visit_occurrence_id's\n",
    "    event_columns : list\n",
    "        Column names that contains, in this order:\n",
    "        - 'person_id'   Identifier for each person in the dataframe\n",
    "        - 'start_date'  Date to fit between 'visit_start_date' and 'visit_end_date'.\n",
    "        - 'events_id'   Unique identifier for each registry of events_df.\n",
    "    visits_df : pandas.DataFrame\n",
    "        DataFrame containing visit occurrence data.\n",
    "        Must include columns: 'person_id', 'visit_start_date',\n",
    "        'visit_end_date', 'visit_occurrence_id'.\n",
    "        Column names need to be the same. This is to ensure\n",
    "        the correct table (VISIT_OCCURRENCE) is being used.\n",
    "    verbose : int, optional, default 0\n",
    "        Verbosity level for function output.\n",
    "        0: No output\n",
    "        1: Additionally, print state of the processing\n",
    "        2+: Additionally, print all debug information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame containing the original table event_df plus the value for\n",
    "        the visit_occurence_id, visit_start_date and visit_end_date, if found.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If required columns are missing in input DataFrames.\n",
    "    \"\"\"\n",
    "    # Transform to polars dataframes\n",
    "    events_df = pl.from_pandas(events_df)\n",
    "    visits_df = pl.from_pandas(visits_df)\n",
    "\n",
    "    # == Initial message ==============================================\n",
    "    if verbose > 0:\n",
    "        print(\"Looking for visit_occurrence_id matches:\")\n",
    "\n",
    "    # == Initial Checks ===============================================\n",
    "    # Check for required columns in events_df\n",
    "    if verbose > 0:\n",
    "        print(\" Checking input...\")\n",
    "    required_input_columns = event_columns\n",
    "    missing_input_columns = set(required_input_columns) - set(events_df.columns)\n",
    "    if missing_input_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in events_df: {missing_input_columns}\"\n",
    "        )\n",
    "    # Check for required columns in visits_df\n",
    "    required_visit_columns = [\n",
    "        \"person_id\",\n",
    "        \"visit_start_datetime\",\n",
    "        \"visit_end_datetime\",\n",
    "        \"visit_occurrence_id\",\n",
    "    ]\n",
    "    missing_visit_columns = set(required_visit_columns) - set(visits_df.columns)\n",
    "    if missing_visit_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in visits_df: {missing_visit_columns}\"\n",
    "        )\n",
    "    visits_df = visits_df.select(required_visit_columns)\n",
    "\n",
    "    # == Force dtypes and sort ========================================\n",
    "    # Ensure start_date and visit dates are datetime\n",
    "    if verbose > 0:\n",
    "        print(\" Sorting dataframes...\")\n",
    "    events_df = events_df.with_columns(pl.col(event_columns[1]).cast(pl.Datetime(\"ms\")))\n",
    "    visits_df = visits_df.with_columns(\n",
    "        pl.col(\"visit_start_datetime\").cast(pl.Datetime(\"ms\")),\n",
    "        pl.col(\"visit_end_datetime\").cast(pl.Datetime(\"ms\")),\n",
    "        )\n",
    "\n",
    "    # Drop all duplicates, if visits are not unique we cannot assign them\n",
    "    visits_df = visits_df.unique(\n",
    "        subset=[\"person_id\", \"visit_start_datetime\", \"visit_end_datetime\"], keep=\"none\"\n",
    "    )\n",
    "\n",
    "    # Sort the neccesary columns of dataframes\n",
    "    events_df = events_df.sort([event_columns[0], event_columns[1]])\n",
    "    visits_df = visits_df.sort(\n",
    "        [event_columns[0], \"visit_start_datetime\", \"visit_end_datetime\"]\n",
    "    )\n",
    "\n",
    "    # == Merging ======================================================\n",
    "    if verbose > 0:\n",
    "        print(\" Combining results...\")\n",
    "    merged_df = visits_df.join(events_df, on=event_columns[0], how=\"left\")\n",
    "\n",
    "    # Check if merge resulted in any matches\n",
    "    if merged_df[\"visit_occurrence_id\"].is_null().all():\n",
    "        raise ValueError(\n",
    "            (\n",
    "                \"No matching records found after merging.\"\n",
    "                + \"Check if person_id values align between dataframes.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # == Filter for valid ranges ======================================\n",
    "    if verbose > 0:\n",
    "        print(\" Filtering valid ranges...\")\n",
    "    # Filter only valid ranges within range\n",
    "    valid_ranges = merged_df.filter(\n",
    "        pl.col(event_columns[1]).is_between(\n",
    "            pl.col(\"visit_start_datetime\"), pl.col(\"visit_end_datetime\"), closed=\"both\"\n",
    "        )\n",
    "    )\n",
    "    valid_ranges = valid_ranges[\n",
    "        [\n",
    "            event_columns[0],\n",
    "            event_columns[2],\n",
    "            \"visit_occurrence_id\",\n",
    "            \"visit_start_datetime\",\n",
    "            \"visit_end_datetime\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Merge with original to retrieve events without visit_occurrence_id\n",
    "    final_df = events_df.join(valid_ranges,\n",
    "        on=[event_columns[0], event_columns[2]],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Sometimes, there might be events that land in visits that share a day.\n",
    "    # Those would be duplicated on the event_id. Let's drop those duplicates\n",
    "    # Since they're ordered, we will only lose the second visit, the one\n",
    "    # that starts with the event\n",
    "    final_df = final_df.unique(subset=[event_columns[0], event_columns[2]])\n",
    "\n",
    "    if verbose > 1:\n",
    "        if valid_ranges.is_empty():\n",
    "            print(\n",
    "                (\n",
    "                    \" Warning: No valid date ranges found.\"\n",
    "                    + \"All condition start dates are outside visit date ranges.\"\n",
    "                )\n",
    "            )\n",
    "        print(f\"  Shape of events_df: {events_df.shape}\")\n",
    "        print(f\"  Shape of visits_df: {visits_df.shape}\")\n",
    "        print(f\"  Shape of merged_df: {merged_df.shape}\")\n",
    "        print(f\"  Shape of valid_ranges: {valid_ranges.shape}\")\n",
    "        print(f\"  Shape of final_df: {final_df.shape}\")\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\" Done.\")\n",
    "\n",
    "    return final_df.to_pandas()\n",
    "\n",
    "visit_df[\"visit_start_datetime\"] = pd.to_datetime(visit_df[\"visit_start_datetime\"])\n",
    "visit_df[\"visit_end_datetime\"] = pd.to_datetime(visit_df[\"visit_end_datetime\"])\n",
    "df = find_visit_occurence_id_polars(\n",
    "    input_df, [\"person_id\", \"start_date\", \"event_id\"], visit_df, verbose=0\n",
    ")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba con datasets grandes\n",
    "Vamos a comparar la velocidad de los distintos métodos con datasets grandes.\n",
    "\n",
    "Creamos una función para generar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import parquet\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import bps_to_omop.general as gen\n",
    "\n",
    "\n",
    "def create_sample_df(\n",
    "    n_people: int = 1000,\n",
    "    n_dates: int = 50,\n",
    "    first_date: str = \"2020-01-01\",\n",
    "    last_date: str = \"2023-01-01\",\n",
    "    mean_duration_days: int = 60,\n",
    "    std_duration_days: int = 180,\n",
    "    people_pool=None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataframe of 'n_people' people with 'n_dates' events each.\n",
    "\n",
    "    Events will be restrained to start after 'first_date' and do not\n",
    "    begin after 'last_date'.\n",
    "\n",
    "    The events and their duration will be modelled with a gaussian with\n",
    "    mean = 'mean_duration_days' and std='std_duration_days'.\n",
    "\n",
    "    User can provide a list of person_id using people_pool. If provided\n",
    "    no new users will be created, but 'n_dates' events will be drawn\n",
    "    from each 'person_id' in 'people_pool' that list of ids.\n",
    "    \"\"\"\n",
    "    # == Parameters ==\n",
    "    np.random.seed(42)\n",
    "    pd.options.mode.string_storage = \"pyarrow\"\n",
    "    # Start date from which to start the dates\n",
    "    first_date = pd.to_datetime(first_date)\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    max_days = (last_date - first_date).days\n",
    "\n",
    "    # == Generate IDs randomly ==\n",
    "    # -- Generate the Ids\n",
    "    if people_pool is None:\n",
    "        size = n_people * n_dates\n",
    "        people = np.random.randint(10000000, 99999999 + 1, size=n_people)\n",
    "        person_id = np.random.choice(people, size)\n",
    "    else:\n",
    "        size = n_dates\n",
    "        person_id = np.random.choice(people_pool, size)\n",
    "\n",
    "    # == Generate random dates ==\n",
    "    # Generate random integers for days and convert to timedelta\n",
    "    random_days = np.random.randint(0, max_days, size=size)\n",
    "    # Create the columns\n",
    "    observation_start_date = first_date + pd.to_timedelta(random_days, unit=\"D\")\n",
    "    # Generate a gaussian sample of dates\n",
    "    random_days = np.random.normal(mean_duration_days, std_duration_days, size=size)\n",
    "    random_days = np.int32(random_days)\n",
    "    observation_end_date = observation_start_date + pd.to_timedelta(\n",
    "        random_days, unit=\"D\"\n",
    "    )\n",
    "    # Correct end_dates\n",
    "    # => If they are smaller than start_date, take start_date\n",
    "    observation_end_date = np.where(\n",
    "        observation_end_date < observation_start_date,\n",
    "        observation_start_date,\n",
    "        observation_end_date,\n",
    "    )\n",
    "\n",
    "    # == Generate the code ==\n",
    "    event_id = np.arange(len(person_id))\n",
    "\n",
    "    # == Generate the dataframe ==\n",
    "    df_raw = {\n",
    "        \"event_id\": event_id,\n",
    "        \"person_id\": person_id,\n",
    "        \"start_date\": observation_start_date,\n",
    "        \"end_date\": observation_end_date,\n",
    "    }\n",
    "    df_raw = pd.DataFrame(df_raw)\n",
    "    df_raw[\"start_date\"] = pd.to_datetime(df_raw[\"start_date\"]).astype(\"datetime64[ms]\")\n",
    "    df_raw[\"end_date\"] = pd.to_datetime(df_raw[\"end_date\"]).astype(\"datetime64[ms]\")\n",
    "\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos con datos pequeños que se puedan manejar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_people = 100000\n",
    "n_dates_visit = 100\n",
    "n_dates_input = 100\n",
    "last_date = \"2020-07-31\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input dataset\n",
    "input_df = create_sample_df(\n",
    "    n_people=n_people, n_dates=n_dates_input, last_date=last_date\n",
    ")\n",
    "input_df = input_df.drop(\"end_date\", axis=1).sort_values([\"person_id\", \"start_date\"])\n",
    "input_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visit dataset\n",
    "visit_df = create_sample_df(\n",
    "    n_dates=n_people*n_dates_visit,\n",
    "    last_date=last_date,\n",
    "    people_pool=input_df[\"person_id\"].unique(),\n",
    ")\n",
    "visit_df = visit_df.sort_values([\"person_id\", \"start_date\", \"end_date\"])\n",
    "visit_df = visit_df.rename(\n",
    "    {\n",
    "        \"event_id\": \"visit_occurrence_id\",\n",
    "        \"start_date\": \"visit_start_datetime\",\n",
    "        \"end_date\": \"visit_end_datetime\",\n",
    "    },\n",
    "    axis=1,\n",
    ")\n",
    "visit_df = visit_df[\n",
    "    [\"person_id\", \"visit_start_datetime\", \"visit_end_datetime\", \"visit_occurrence_id\"]\n",
    "]\n",
    "\n",
    "# Remove overlap\n",
    "visit_df = gen.remove_overlap(\n",
    "    visit_df,\n",
    "    [\"person_id\", \"visit_start_datetime\", \"visit_end_datetime\", \"visit_occurrence_id\"],\n",
    "    [True, True, False, True],\n",
    "    verbose=0,\n",
    ")\n",
    "visit_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero comprobamos que los resultados obtenidos tienen sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    df_0 = find_visit_occurence_id(input_df, [\"person_id\", \"start_date\", \"event_id\"], visit_df, verbose=2)\n",
    "    df_1 = find_visit_occurence_id_fireducks(input_df, [\"person_id\", \"start_date\", \"event_id\"], visit_df, verbose=2)\n",
    "    df_2 = find_visit_occurence_id_polars(input_df, [\"person_id\", \"start_date\", \"event_id\"], visit_df, verbose=2)\n",
    "\n",
    "df_0 = df_0.sort_values([\"person_id\", \"start_date\", \"event_id\"]).reset_index(drop=True)\n",
    "df_1 = df_1.sort_values([\"person_id\", \"start_date\", \"event_id\"]).reset_index(drop=True)\n",
    "df_2 = df_2.sort_values([\"person_id\", \"start_date\", \"event_id\"]).reset_index(drop=True)\n",
    "\n",
    "pd.testing.assert_frame_equal(df_0, df_1)\n",
    "pd.testing.assert_frame_equal(df_0, df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego comprobamos la velocidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    print(f\"- {n_people = }\\n- {n_dates_visit = }\\n- {n_dates_input = }\\n- {input_df.shape = }\\n- {visit_df.shape = }\")\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)  \n",
    "    print('\\nfind_visit_occurence_id:')\n",
    "    %timeit -n 5 -r 5 find_visit_occurence_id(input_df, [\"person_id\", \"start_date\", \"event_id\"], visit_df, verbose=0)\n",
    "    print('find_visit_occurence_id_fireducks')\n",
    "    %timeit -n 5 -r 5 find_visit_occurence_id_fireducks(input_df, [\"person_id\", \"start_date\", \"event_id\"], visit_df, verbose=0)\n",
    "    print('find_visit_occurence_id_polars')\n",
    "    %timeit -n 5 -r 5 find_visit_occurence_id_polars(input_df, [\"person_id\", \"start_date\", \"event_id\"], visit_df, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20/06/2025) Funny thing here, if there's lots of visits, polars is slightly slower than pandas. However, both solution are fast because there's not many ppl in the first place:\n",
    "\n",
    "```batch\n",
    "- n_people = 10000\n",
    "- n_dates_visit = 100\n",
    "- n_dates_input = 10000\n",
    "\n",
    "find_visit_occurence_id:\n",
    "24.5 ms ± 1.56 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "find_visit_occurence_id_fireducks\n",
    "23.8 ms ± 222 μs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "find_visit_occurence_id_polars\n",
    "33.1 ms ± 1.5 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "```\n",
    "\n",
    "However, when the number of records increases, polars starts to be faster:\n",
    "\n",
    "- 10.000 ppl\n",
    "```\n",
    "- n_people = 10000\n",
    "- n_dates_visit = 100\n",
    "- n_dates_input = 100\n",
    "- input_df.shape = (1000000, 3)\n",
    "- visit_df.shape = (65055, 4)\n",
    "\n",
    "find_visit_occurence_id:\n",
    "1.36 s ± 11.1 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "find_visit_occurence_id_fireducks\n",
    "1.34 s ± 2.22 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "find_visit_occurence_id_polars\n",
    "539 ms ± 38.9 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "```\n",
    "\n",
    "- 100.000 ppl\n",
    "```\n",
    "- n_people = 100000\n",
    "- n_dates_visit = 100\n",
    "- n_dates_input = 100\n",
    "- input_df.shape = (10000000, 3)\n",
    "- visit_df.shape = (649197, 4)\n",
    "\n",
    "find_visit_occurence_id:\n",
    "22.1 s ± 80.2 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "find_visit_occurence_id_fireducks\n",
    "22.1 s ± 70.1 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "find_visit_occurence_id_polars\n",
    "3.82 s ± 105 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with batch processing\n",
    "\n",
    "Sometimes datasets are too big to be processed at once, Let's try to change the function retrieve_visit_occurrence_id so it can work in paralell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we take the dataframe and use only a certain number of pple\n",
    "ppl_batch = 1000\n",
    "\n",
    "list_ppl = input_df[\"person_id\"].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_procesing(input_df, visit_df, ppl_batch, func):\n",
    "    # Get list of unique ppl\n",
    "    list_ppl = input_df[\"person_id\"].unique()\n",
    "\n",
    "    # Iterate over unique ppl\n",
    "    output = []\n",
    "    for i_init in list(range(0, len(list_ppl), ppl_batch)):\n",
    "        # Retrieve only ppl_batch number of ppl\n",
    "        try:\n",
    "            list_ppl_tmp = list_ppl[i_init:i_init+ppl_batch]\n",
    "        except IndexError:\n",
    "            list_ppl_tmp = list_ppl[i_init:]\n",
    "        \n",
    "        # Restrict datafrmes to those ppl\n",
    "        input_df_tmp = input_df[input_df[\"person_id\"].isin(list_ppl_tmp)]\n",
    "        visit_df_tmp = visit_df[visit_df[\"person_id\"].isin(list_ppl_tmp)]\n",
    "        output_tmp = func(input_df_tmp, [\"person_id\", \"start_date\", \"event_id\"], visit_df_tmp, verbose=0)\n",
    "        output.append(output_tmp)\n",
    "\n",
    "    # Concatenate and return\n",
    "    return pd.concat(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "    df_0 = serial_procesing(input_df, visit_df, 1000, find_visit_occurence_id)\n",
    "    df_1 = serial_procesing(input_df, visit_df, 1000, find_visit_occurence_id_fireducks)\n",
    "    df_2 = serial_procesing(input_df, visit_df, 1000, find_visit_occurence_id_polars)\n",
    "\n",
    "df_0 = df_0.sort_values([\"person_id\", \"start_date\", \"event_id\"]).reset_index(drop=True)\n",
    "df_1 = df_1.sort_values([\"person_id\", \"start_date\", \"event_id\"]).reset_index(drop=True)\n",
    "df_2 = df_2.sort_values([\"person_id\", \"start_date\", \"event_id\"]).reset_index(drop=True)\n",
    "\n",
    "pd.testing.assert_frame_equal(df_0, df_1)\n",
    "pd.testing.assert_frame_equal(df_0, df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    print(f\"- {n_people = }\\n- {n_dates_visit = }\\n- {n_dates_input = }\\n- {input_df.shape = }\\n- {visit_df.shape = }\")\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)  \n",
    "    print('\\nSerial processing with find_visit_occurence_id:')\n",
    "    %timeit -n 5 -r 5 serial_procesing(input_df, visit_df, 10000, find_visit_occurence_id)\n",
    "    print('Serial processing with find_visit_occurence_id_fireducks')\n",
    "    %timeit -n 5 -r 5 serial_procesing(input_df, visit_df, 10000, find_visit_occurence_id_fireducks)\n",
    "    print('Serial processing with find_visit_occurence_id_polars')\n",
    "    %timeit -n 5 -r 5 serial_procesing(input_df, visit_df, 10000, find_visit_occurence_id_polars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20/06/2025)\n",
    "\n",
    "- 10.000 ppl with batches of 1.000 ppl\n",
    "```\n",
    "- n_people = 10000\n",
    "- n_dates_visit = 100\n",
    "- n_dates_input = 100\n",
    "- input_df.shape = (1000000, 3)\n",
    "- visit_df.shape = (65055, 4)\n",
    "\n",
    "Serial processing with find_visit_occurence_id:\n",
    "1.18 s ± 3.9 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "Serial processing with find_visit_occurence_id_fireducks\n",
    "1.18 s ± 3.61 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "Serial processing with find_visit_occurence_id_polars\n",
    "1.01 s ± 15.6 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "```\n",
    "\n",
    "- 100.000 ppl with batches of 1.000 ppl\n",
    "```\n",
    "- n_people = 100000\n",
    "- n_dates_visit = 100\n",
    "- n_dates_input = 100\n",
    "- input_df.shape = (10000000, 3)\n",
    "- visit_df.shape = (649197, 4)\n",
    "\n",
    "Serial processing with find_visit_occurence_id:\n",
    "15.5 s ± 205 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "Serial processing with find_visit_occurence_id_fireducks\n",
    "15.3 s ± 8.8 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "Serial processing with find_visit_occurence_id_polars\n",
    "18.4 s ± 44.4 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "```\n",
    "\n",
    "- 100.000 ppl with batches of 10.000 ppl\n",
    "```\n",
    "- n_people = 100000\n",
    "- n_dates_visit = 100\n",
    "- n_dates_input = 100\n",
    "- input_df.shape = (10000000, 3)\n",
    "- visit_df.shape = (649197, 4)\n",
    "\n",
    "Serial processing with find_visit_occurence_id:\n",
    "14.7 s ± 46.3 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "Serial processing with find_visit_occurence_id_fireducks\n",
    "14.7 s ± 13.1 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "Serial processing with find_visit_occurence_id_polars\n",
    "Serial processing with find_visit_occurence_id_polars\n",
    "6.94 s ± 80.3 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
