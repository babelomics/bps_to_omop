{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if we can do a full implementation of some functions with only pyarrow.\n",
    "\n",
    "See https://github.com/apache/arrow/issues/30950 for drop_duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.compute as pc\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def create_test_parquet(filename: str, num_rows: int) -> None:\n",
    "    \"\"\"Create a test parquet file with synthetic data.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate random dates within a 5-year range\n",
    "    base_date = datetime(2020, 1, 1)\n",
    "    random_days = np.random.randint(0, 365 * 5, size=num_rows)\n",
    "    dates = [base_date + timedelta(days=int(days)) for days in random_days]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'person_id': np.random.randint(1, num_rows // 10, size=num_rows),\n",
    "        'start_date': dates,\n",
    "        'end_date': [d + timedelta(days=np.random.randint(0, 365)) for d in dates],\n",
    "        'type_concept': ['test_type'] * num_rows\n",
    "    })\n",
    "    \n",
    "    df.to_parquet(filename)\n",
    "\n",
    "def measure_performance(func: Callable, filename: str) -> tuple[float, float]:\n",
    "    \"\"\"Measure execution time and peak memory usage of a function.\"\"\"\n",
    "    # Get the current process\n",
    "    process = psutil.Process()\n",
    "    \n",
    "    # Record starting memory\n",
    "    start_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Time the function\n",
    "    start_time = time.time()\n",
    "    result = func(filename)\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Record peak memory\n",
    "    end_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    memory_used = end_memory - start_memory\n",
    "    \n",
    "    return result, execution_time, memory_used\n",
    "\n",
    "def run_comparison_tests(sizes: list[int]) -> tuple[dict, dict]:\n",
    "    \"\"\"Run performance tests for both implementations with different data sizes.\"\"\"\n",
    "    pandas_times = []\n",
    "    pandas_memory = []\n",
    "    pyarrow_times = []\n",
    "    pyarrow_memory = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        test_file = f'test_data_{size}.parquet'\n",
    "        \n",
    "        # Create test data\n",
    "        print(f\"\\nTesting with {size} rows...\")\n",
    "        create_test_parquet(test_file, size)\n",
    "        \n",
    "        # Test pandas implementation\n",
    "        pandas_result, pandas_time, pandas_mem = measure_performance(pandas_implementation, test_file)\n",
    "        pandas_times.append(pandas_time)\n",
    "        pandas_memory.append(pandas_mem)\n",
    "        print(f\"Pandas: {pandas_time:.2f}s, {pandas_mem:.2f}MB\")\n",
    "        \n",
    "        # Test pyarrow implementation\n",
    "        pyarrow_result, pyarrow_time, pyarrow_mem = measure_performance(pyarrow_implementation, test_file)\n",
    "        pyarrow_times.append(pyarrow_time)\n",
    "        pyarrow_memory.append(pyarrow_mem)\n",
    "        print(f\"PyArrow: {pyarrow_time:.2f}s, {pyarrow_mem:.2f}MB\")\n",
    "        \n",
    "        # Make sure they are the same\n",
    "        # print(pandas_result.to_pandas().info())\n",
    "        # print(pyarrow_result.to_pandas().info())\n",
    "        try:\n",
    "            pd.testing.assert_frame_equal(pandas_result.to_pandas(),\n",
    "                                        pyarrow_result.to_pandas())\n",
    "        except:\n",
    "            print(\"Not equal!\")\n",
    "        \n",
    "        # Clean up test file\n",
    "        os.remove(test_file)\n",
    "    \n",
    "    return {\n",
    "        'sizes': sizes,\n",
    "        'pandas': {'time': pandas_times, 'memory': pandas_memory},\n",
    "        'pyarrow': {'time': pyarrow_times, 'memory': pyarrow_memory}\n",
    "    }\n",
    "\n",
    "def plot_results(results: dict) -> None:\n",
    "    \"\"\"Plot performance comparison results.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    sizes = results['sizes']\n",
    "    x_ticks = [f\"{size/1000:.0f}k\" for size in sizes]\n",
    "    \n",
    "    # Time comparison\n",
    "    ax1.plot(x_ticks, results['pandas']['time'], marker='o', label='Pandas')\n",
    "    ax1.plot(x_ticks, results['pyarrow']['time'], marker='o', label='PyArrow')\n",
    "    ax1.set_title('Execution Time Comparison')\n",
    "    ax1.set_xlabel('Dataset Size (rows)')\n",
    "    ax1.set_ylabel('Time (seconds)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Memory comparison\n",
    "    ax2.plot(x_ticks, results['pandas']['memory'], marker='o', label='Pandas')\n",
    "    ax2.plot(x_ticks, results['pyarrow']['memory'], marker='o', label='PyArrow')\n",
    "    ax2.set_title('Memory Usage Comparison')\n",
    "    ax2.set_xlabel('Dataset Size (rows)')\n",
    "    ax2.set_ylabel('Memory (MB)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Copy both implementations here\n",
    "def pandas_implementation(filename: str) -> pa.Table:\n",
    "    \"\"\"Original pandas-based implementation.\"\"\"\n",
    "    df_events = pd.read_parquet(\n",
    "        filename,\n",
    "        columns=[\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "    )\n",
    "    \n",
    "    event_type = df_events[\"type_concept\"].iloc[0]\n",
    "    \n",
    "    df_melted = (df_events[[\"person_id\", \"start_date\", \"end_date\"]]\n",
    "                 .melt(id_vars=[\"person_id\"], value_name=\"event_date\")\n",
    "                 .dropna()\n",
    "                 [[\"person_id\", \"event_date\"]])\n",
    "    \n",
    "    df_output = pd.DataFrame({\n",
    "        \"person_id\": df_melted[\"person_id\"],\n",
    "        \"start_date\": df_melted[\"event_date\"],\n",
    "        \"end_date\": df_melted[\"event_date\"],\n",
    "        \"type_concept\": event_type\n",
    "    })\n",
    "    \n",
    "    return pa.Table.from_pandas(\n",
    "        df_output.drop_duplicates(ignore_index=True),\n",
    "        preserve_index=False\n",
    "    )\n",
    "\n",
    "def pyarrow_implementation(filename: str) -> pa.Table:\n",
    "    \"\"\"Pure PyArrow-based implementation.\"\"\"\n",
    "    table = pa.parquet.read_table(\n",
    "        filename,\n",
    "        columns=['person_id', 'start_date', 'end_date', 'type_concept']\n",
    "    )\n",
    "    \n",
    "    type_concept = table['type_concept'][0].as_py()\n",
    "    \n",
    "    start_dates = table.select(['person_id', 'start_date'])\n",
    "    end_dates = table.select(['person_id', 'end_date'])\n",
    "    end_dates = end_dates.rename_columns(['person_id', 'start_date'])\n",
    "    \n",
    "    combined = pa.concat_tables([start_dates, end_dates])\n",
    "    mask = pc.is_valid(combined['start_date'])\n",
    "    combined = combined.filter(mask)\n",
    "    \n",
    "    combined = combined.append_column('end_date',combined['start_date'])\n",
    "    combined = combined.append_column('type_concept',pa.array([type_concept] * len(combined)))\n",
    "\n",
    "    # Remove duplicates by aggregating\n",
    "    combined = combined.group_by(['person_id', 'start_date', 'end_date', 'type_concept']).aggregate([])\n",
    "    # combined = pc.unique(combined)\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different dataset sizes\n",
    "sizes = [1000, 10000, 100000, 1000000]\n",
    "results = run_comparison_tests(sizes)\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'test.parquet'\n",
    "create_test_parquet(test_file, 20)\n",
    "pandas_result, pandas_time, pandas_mem = measure_performance(pandas_implementation, test_file)\n",
    "pandas_result.to_pandas().info()\n",
    "pyarrow_result, pyarrow_time, pyarrow_mem = measure_performance(pyarrow_implementation, test_file)\n",
    "pyarrow_result.to_pandas().info()\n",
    "os.remove(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.testing.assert_frame_equal(pandas_result.to_pandas(),\n",
    "                            pyarrow_result.to_pandas())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
