{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New find_date_ranges\n",
    "\n",
    "Resulta que el código de la función find_date_ranges está haciendo cosas extrañas. Parece que la implementación con merge_asof() funciona bien, pero vamos a hacer pruebas en este notebook para asegurarnos de que todo funciona correctamente.\n",
    "\n",
    "To Do:\n",
    "- Revisar el notebook a partir de sección 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un dataset que tenga todas las variantes de datos que podamos encontrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input = []\n",
    "visit = []\n",
    "\n",
    "# 1 -> Person that has one event and one visit that match\n",
    "input_rows = [{'person_id': 1, 'event_id': 1, 'start_date': '2020-01-05', 'expected_visit_id': 1}]\n",
    "visit_rows = [{'person_id': 1, 'visit_id': 1, 'start_date': '2020-01-01', 'end_date': '2020-01-10',}]\n",
    "input += input_rows\n",
    "visit += visit_rows\n",
    "# 2 -> Person that has one event and one visit that do not match\n",
    "input_rows = [{'person_id': 2, 'event_id': 2, 'start_date': '2020-02-05', 'expected_visit_id': None}]\n",
    "visit_rows = [{'person_id': 2, 'visit_id': 2, 'start_date': '2020-01-01', 'end_date': '2020-01-10'}]\n",
    "input += input_rows\n",
    "visit += visit_rows\n",
    "# 3 -> Person that has one event and no visit\n",
    "input_rows = [{'person_id': 3, 'event_id': 3, 'start_date': '2020-02-05', 'expected_visit_id': None}]\n",
    "input += input_rows\n",
    "# 4 -> Person that has no event and one visit \n",
    "visit_rows = [{'person_id': 4, 'visit_id': 3, 'start_date': '2020-01-01', 'end_date': '2020-01-10'}]\n",
    "visit += visit_rows\n",
    "# 5 -> Person that has two events and two visits. One match and the rest do not\n",
    "input_rows = [{'person_id': 5, 'event_id': 4, 'start_date': '2020-01-05', 'expected_visit_id': 4}]\n",
    "visit_rows = [{'person_id': 5, 'visit_id': 4, 'start_date': '2020-01-01', 'end_date': '2020-01-10'}]\n",
    "input += input_rows\n",
    "visit += visit_rows\n",
    "input_rows = [{'person_id': 5, 'event_id': 5, 'start_date': '2020-02-05', 'expected_visit_id': None}]\n",
    "visit_rows = [{'person_id': 5, 'visit_id': 5, 'start_date': '2020-03-01', 'end_date': '2020-03-10'}]\n",
    "input += input_rows\n",
    "visit += visit_rows\n",
    "# 6 -> Person that has two events tha match to a single visit\n",
    "input_rows = [{'person_id': 6, 'event_id': 6, 'start_date': '2020-01-04', 'expected_visit_id': 6},\n",
    "              {'person_id': 6, 'event_id': 7, 'start_date': '2020-01-05', 'expected_visit_id': 6},]\n",
    "visit_rows = [{'person_id': 6, 'visit_id': 6, 'start_date': '2020-01-01', 'end_date': '2020-01-10'}]\n",
    "input += input_rows\n",
    "visit += visit_rows\n",
    "# 7 -> Person that has one event that fits the end of one period and the beginning of the next\n",
    "input_rows = [{'person_id': 7, 'event_id': 8, 'start_date': '2020-01-05', 'expected_visit_id': 6}]\n",
    "visit_rows = [{'person_id': 7, 'visit_id': 7, 'start_date': '2020-01-01', 'end_date': '2020-01-05'},\n",
    "              {'person_id': 7, 'visit_id': 8, 'start_date': '2020-01-05', 'end_date': '2020-01-10'}]\n",
    "input += input_rows\n",
    "visit += visit_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.DataFrame.from_records(input)\n",
    "visit_df = pd.DataFrame.from_records(visit)\n",
    "visit_df = visit_df.rename({\n",
    "    'visit_id':'visit_occurrence_id',\n",
    "    'start_date':'visit_start_date',\n",
    "    'end_date':'visit_end_date',\n",
    "}, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de visit_occurrence_id\n",
    "El siguiente paso consiste en enlazar cada medida del paciente con una visita. Para ello cargaremos la tabla VISIT_OCCURRENCE, que ya debería haber sido construida en una sección anterior, y buscaremos para cada measurement_date de la tabla MEASUREMENT un intervalo de fechas de visitas que la contenga. Si existe, le asignaremos el visit_occurrence_id correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_find_date_ranges(input_df, visits_df, debug=False, method='index'):\n",
    "    # Check for required columns in input_df\n",
    "    required_input_columns = {'person_id', 'start_date'}\n",
    "    missing_input_columns = required_input_columns - \\\n",
    "        set(input_df.columns)\n",
    "    if missing_input_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in input_df: {missing_input_columns}\")\n",
    "\n",
    "    # Check for required columns in visits_df\n",
    "    required_visit_columns = [\n",
    "        'person_id', 'visit_start_date', 'visit_end_date', 'visit_occurrence_id']\n",
    "    missing_visit_columns = set(\n",
    "        required_visit_columns) - set(visits_df.columns)\n",
    "    if missing_visit_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in visits_df: {missing_visit_columns}\")\n",
    "\n",
    "    # Merge the dataframes\n",
    "    merged_df = pd.merge(\n",
    "        input_df.reset_index(drop=True),\n",
    "        visits_df[required_visit_columns].reset_index(drop=True),\n",
    "        on='person_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Check if merge resulted in any matches\n",
    "    if merged_df['visit_occurrence_id'].isna().all():\n",
    "        raise ValueError(\n",
    "            (\"No matching records found after merging.\"\n",
    "             + \"Check if person_id values align between dataframes.\"))\n",
    "\n",
    "    # Create mask for dates within range\n",
    "    date_range_mask = ((merged_df['start_date'] >= merged_df['visit_start_date']) &\n",
    "                       (merged_df['start_date'] <= merged_df['visit_end_date']))\n",
    "\n",
    "    # Filter only valid ranges\n",
    "    valid_ranges = merged_df[date_range_mask]\n",
    "\n",
    "    # Now we have two options\n",
    "    # a) Use indexing to retrieve events without visit_id\n",
    "    # b) Merge with the initial datatrame\n",
    "    if method == 'index':\n",
    "        # Get list of events that do have visit_id\n",
    "        event_list_pos = set(valid_ranges['event_id'].unique())\n",
    "        # Get list of events that do not have a visit_id\n",
    "        event_list_neg = set(input_df['event_id'].values)-event_list_pos\n",
    "        # Get subset of input with no id and put the extra columns\n",
    "        leftover_df = input_df[input_df['event_id'].isin(event_list_neg)]\n",
    "        leftover_df['visit_start_date'] = None\n",
    "        leftover_df['visit_end_date'] = None\n",
    "        leftover_df['visit_occurrence_id'] = None\n",
    "        # Build the final df\n",
    "        final_df = pd.concat([valid_ranges, leftover_df], axis=0)\n",
    "    elif method == 'merge':\n",
    "        final_df = pd.merge(\n",
    "            input_df,\n",
    "            valid_ranges[['person_id', 'event_id', 'visit_start_date',\n",
    "                'visit_end_date', 'visit_occurrence_id']],\n",
    "            on=['person_id','event_id'],\n",
    "            how='left',\n",
    "        )\n",
    "    else:\n",
    "        raise SyntaxError(\"method parameter is not correct. Use 'index' or 'method'\")\n",
    "    final_df = final_df.drop_duplicates(['person_id', 'event_id'])\n",
    "    \n",
    "    # Check if any valid ranges were found\n",
    "    if final_df.empty:\n",
    "        print((\"Warning: No valid date ranges found.\"\n",
    "              + \"All condition start dates are outside visit date ranges.\"))\n",
    "    if debug:\n",
    "        # Check for duplicates in input_df\n",
    "        input_duplicates = input_df.duplicated().sum()\n",
    "        print(f\"Number of duplicates in input_df: {input_duplicates}\")\n",
    "\n",
    "        # Check for duplicates in visits_df\n",
    "        visits_duplicates = visits_df.duplicated().sum()\n",
    "        print(f\"Number of duplicates in visits_df: {visits_duplicates}\")\n",
    "        \n",
    "        # Print shape of input dataframes\n",
    "        print(f\"Shape of input_df: {input_df.shape}\")\n",
    "        print(f\"Shape of visits_df: {visits_df.shape}\")        \n",
    "        # Print shape of merged dataframe\n",
    "        print(f\"Shape of merged_df: {merged_df.shape}\")\n",
    "        # Print shape of valid_ranges\n",
    "        print(f\"Shape of valid_ranges: {valid_ranges.shape}\")\n",
    "        # Print shape of valid_ranges\n",
    "        print(f\"Shape of final_df: {final_df.shape}\")\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "\n",
    "df = debug_find_date_ranges(input_df, visit_df, debug=True, method='merge')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la última implementación a somos capaces de recuperar todos los eventos, tengan visit_occurrence_id o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asof_find_date_ranges(input_df, visits_df, debug=False):\n",
    "    pd.options.mode.copy_on_write = True\n",
    "    # Check for required columns in input_df\n",
    "    required_input_columns = {'person_id', 'start_date'}\n",
    "    missing_input_columns = required_input_columns - \\\n",
    "        set(input_df.columns)\n",
    "    if missing_input_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in input_df: {missing_input_columns}\")\n",
    "\n",
    "    # Check for required columns in visits_df\n",
    "    required_visit_columns = [\n",
    "        'person_id', 'visit_start_date', 'visit_end_date', 'visit_occurrence_id']\n",
    "    missing_visit_columns = set(required_visit_columns) - set(visits_df.columns)\n",
    "    if missing_visit_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in visits_df: {missing_visit_columns}\")\n",
    "    if debug:\n",
    "        print(f\"Shape of input_df: {input_df.shape}\")\n",
    "        print(f\"Shape of visits_df: {visits_df.shape}\")\n",
    "\n",
    "    # Ensure start_date and visit dates are datetime\n",
    "    input_df['start_date'] = pd.to_datetime(input_df['start_date'])\n",
    "    visits_df['visit_start_date'] = pd.to_datetime(visits_df['visit_start_date'])\n",
    "    visits_df['visit_end_date'] = pd.to_datetime(visits_df['visit_end_date'])\n",
    "\n",
    "    # Sort the dataframes\n",
    "    input_df = input_df.sort_values(['person_id', 'start_date'])\n",
    "    visits_df = visits_df.sort_values(['person_id', 'visit_start_date', 'visit_end_date'])\n",
    "\n",
    "    def merge_for_person(person_data, person_visits):\n",
    "        merged = pd.merge_asof(\n",
    "            person_data,\n",
    "            person_visits,\n",
    "            left_on='start_date',\n",
    "            right_on='visit_start_date',\n",
    "            direction='backward'\n",
    "        )\n",
    "        return merged\n",
    "\n",
    "    # Group by person_id and apply merge_asof for each person\n",
    "    grouped_input = input_df.groupby('person_id')\n",
    "    grouped_visits = visits_df.groupby('person_id')\n",
    "\n",
    "    result_dfs = []\n",
    "    for person_id, person_data in grouped_input:\n",
    "        person_visits = grouped_visits.get_group(person_id) if person_id in grouped_visits.groups else pd.DataFrame()\n",
    "        if not person_visits.empty:\n",
    "            result_dfs.append(merge_for_person(person_data, person_visits))\n",
    "\n",
    "    # Combine results\n",
    "    if result_dfs:\n",
    "        merged_df = pd.concat(result_dfs, ignore_index=True)\n",
    "    else:\n",
    "        merged_df = pd.DataFrame()\n",
    "\n",
    "    # Filter for valid ranges\n",
    "    valid_ranges = merged_df[\n",
    "        (merged_df['start_date'] >= merged_df['visit_start_date']) &\n",
    "        (merged_df['start_date'] <= merged_df['visit_end_date'])\n",
    "    ]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Shape of valid_ranges: {valid_ranges.shape}\")\n",
    "\n",
    "    if valid_ranges.empty:\n",
    "        print(\"Warning: No valid date ranges found. All condition start dates are outside visit date ranges.\")\n",
    "\n",
    "    return valid_ranges\n",
    "\n",
    "df = asof_find_date_ranges(input_df,visit_df,debug=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este igual, me borra todo lo que no tenga un match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_find_date_ranges(input_df, visits_df, debug=False):\n",
    "    # == Preprocessing ============================================================\n",
    "    # Check for required columns in input_df\n",
    "    required_input_columns = {'person_id', 'start_date'}\n",
    "    missing_input_columns = required_input_columns - \\\n",
    "        set(input_df.columns)\n",
    "    if missing_input_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in input_df: {missing_input_columns}\")\n",
    "\n",
    "    # Check for required columns in visits_df\n",
    "    required_visit_columns = {\n",
    "        'person_id', 'visit_start_date', 'visit_end_date', 'visit_occurrence_id'}\n",
    "    missing_visit_columns = required_visit_columns - set(visits_df.columns)\n",
    "    if missing_visit_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in visits_df: {missing_visit_columns}\")\n",
    "\n",
    "    output_df = input_df.copy()\n",
    "    output_df['visit_occurrence_id'] = None\n",
    "    for input_idx, input_row in output_df.iterrows():\n",
    "        # if debug:\n",
    "        #     print(input_idx)\n",
    "        tmp_visit = visits_df[visits_df['person_id'] == input_row['person_id']]\n",
    "        if tmp_visit.shape[0] == 0:\n",
    "            continue\n",
    "        for visit_idx, visit_row in tmp_visit.iterrows():\n",
    "            # if debug:\n",
    "            #     print(' ', visit_idx)\n",
    "            flag_start = input_row['start_date'] >= visit_row['visit_start_date']\n",
    "            flag_end = input_row['start_date'] <= visit_row['visit_end_date']\n",
    "            flag = flag_start & flag_end\n",
    "            if flag:\n",
    "                output_df.loc[input_idx, 'visit_occurrence_id'] = visit_row['visit_occurrence_id']\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Shape of input_df: {input_df.shape}\")\n",
    "        print(f\"Shape of visits_df: {visits_df.shape}\")\n",
    "        print(f\"Shape of output_df: {output_df.shape}\")\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "df = brute_find_date_ranges(input_df, visit_df, debug=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import bps_to_omop.general as gen\n",
    "\n",
    "df = gen.find_date_ranges(input_df[['person_id','start_date','event_id']],\n",
    "                          visit_df, verbose=2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)    \n",
    "    print('debug_find_date_ranges index')\n",
    "    %timeit -n 5 -r 5 debug_find_date_ranges(input_df.copy(),visit_df.copy(), method='index')\n",
    "    print('debug_find_date_ranges merge')\n",
    "    %timeit -n 5 -r 5 debug_find_date_ranges(input_df.copy(),visit_df.copy(), method='merge')\n",
    "    print('gen.find_date_ranges (eq to debug_*_mege)')\n",
    "    %timeit -n 5 -r 5 gen.find_date_ranges(input_df.copy(),visit_df.copy(), verbose=0)\n",
    "    print('asof_find_date_ranges')\n",
    "    %timeit -n 5 -r 5 asof_find_date_ranges(input_df.copy(),visit_df.copy())\n",
    "    print('brute_find_date_ranges')\n",
    "    %timeit -n 5 -r 5 brute_find_date_ranges(input_df.copy(),visit_df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba con datasets grandes\n",
    "Vamos a comparar si el metodo de pyarrow sigue funcionando más rápido con datasets grandes.\n",
    "\n",
    "Nos traemos la función para generar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import parquet\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import bps_to_omop.general as gen\n",
    "\n",
    "def create_sample_df(n_people: int = 1000, \n",
    "                     n_dates: int = 50,\n",
    "                     first_date: str = '2020-01-01',\n",
    "                     last_date: str = '2023-01-01',\n",
    "                     mean_duration_days: int = 60,\n",
    "                     std_duration_days: int = 180,\n",
    "                     people_pool = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataframe of 'n_people' people with 'n_dates' events each.\n",
    "    \n",
    "    Events will be restrained to start after 'first_date' and do not\n",
    "    begin after 'last_date'.\n",
    "    \n",
    "    The events and their duration will be modelled with a gaussian with\n",
    "    mean = 'mean_duration_days' and std='std_duration_days'.\n",
    "    \n",
    "    User can provide a list of person_id using people_pool. If provided\n",
    "    no new users will be created, but 'n_dates' events will be drawn\n",
    "    from each 'person_id' in 'people_pool' that list of ids.\n",
    "    \"\"\"\n",
    "    # == Parameters ==\n",
    "    np.random.seed(42)\n",
    "    pd.options.mode.string_storage = \"pyarrow\"\n",
    "    # Start date from which to start the dates\n",
    "    first_date = pd.to_datetime(first_date)\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    max_days = (last_date-first_date).days\n",
    "\n",
    "    # == Generate IDs randomly ==\n",
    "    # -- Generate the Ids\n",
    "    if people_pool is None:\n",
    "        size = n_people*n_dates\n",
    "        people = np.random.randint(10000000, 99999999 + 1, size=n_people)\n",
    "        person_id = np.random.choice(people, size)\n",
    "    else:\n",
    "        size = n_dates\n",
    "        person_id = np.random.choice(people_pool, size)\n",
    "\n",
    "    # == Generate random dates ==\n",
    "    # Generate random integers for days and convert to timedelta\n",
    "    random_days = np.random.randint(0, max_days, size=size)\n",
    "    # Create the columns\n",
    "    observation_start_date = first_date + \\\n",
    "        pd.to_timedelta(random_days, unit='D')\n",
    "    # Generate a gaussian sample of dates\n",
    "    random_days = np.random.normal(\n",
    "        mean_duration_days, std_duration_days, size=size)\n",
    "    random_days = np.int32(random_days)\n",
    "    observation_end_date = observation_start_date + \\\n",
    "        pd.to_timedelta(random_days, unit='D')\n",
    "    # Correct end_dates\n",
    "    # => If they are smaller than start_date, take start_date\n",
    "    observation_end_date = np.where(observation_end_date < observation_start_date,\n",
    "                                    observation_start_date, observation_end_date)\n",
    "\n",
    "    # == Generate the code ==\n",
    "    event_id = np.arange(len(person_id))\n",
    "\n",
    "    # == Generate the dataframe ==\n",
    "    df_raw = {'event_id': event_id, \n",
    "              'person_id': person_id, \n",
    "              'start_date': observation_start_date,\n",
    "              'end_date': observation_end_date, }\n",
    "    df_raw = pd.DataFrame(df_raw)\n",
    "    df_raw['start_date'] = pd.to_datetime(df_raw['start_date']).astype('datetime64[ms]')\n",
    "    df_raw['end_date'] = pd.to_datetime(df_raw['end_date']).astype('datetime64[ms]')\n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos con datos pequeños que se puedan manejar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_people = 2\n",
    "n_dates = 10\n",
    "last_date = '2020-07-31'\n",
    "# Create visit dataset\n",
    "visit_df = create_sample_df(n_people=n_people,n_dates=n_dates,last_date=last_date)\n",
    "visit_df = visit_df.sort_values(['person_id','start_date','end_date'])\n",
    "visit_df = visit_df.rename({\n",
    "    'event_id':'visit_occurrence_id',\n",
    "    'start_date':'visit_start_date',\n",
    "    'end_date':'visit_end_date',\n",
    "}, axis=1)\n",
    "visit_df = gen.remove_overlap(visit_df)\n",
    "visit_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dates = 4\n",
    "# Create input dataset\n",
    "input_df = create_sample_df(n_dates=n_dates,\n",
    "                            last_date=last_date,\n",
    "                            people_pool=visit_df['person_id'].unique())\n",
    "input_df = input_df.drop('end_date',axis=1).sort_values(['person_id','start_date'])\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = brute_find_date_ranges(input_df, visit_df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí vemos que el brute find_date_ranges encuentra todo lo que tiene que encontrar. Todas las citas del person_id 66755036 tienen una cita asociada. Mientras que la otra person_id, 75682867, no tiene ninguna visita que incluya las fechas de su único evento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Define parameters\n",
    "n_people = 30000\n",
    "n_dates_visit = 100\n",
    "n_dates_input = 500000\n",
    "last_date = '2020-12-31'\n",
    "\n",
    "# Create visit dataset\n",
    "print('Creating visit dataset...')\n",
    "visit_df = create_sample_df(\n",
    "    n_people=n_people, n_dates=n_dates_visit, last_date=last_date)\n",
    "visit_df = visit_df.rename({\n",
    "    'event_id': 'visit_occurrence_id',\n",
    "    'start_date': 'visit_start_date',\n",
    "    'end_date': 'visit_end_date',\n",
    "}, axis=1)\n",
    "visit_df = gen.remove_overlap(visit_df, verbose=1)\n",
    "visit_df = visit_df[['person_id', 'visit_start_date',\n",
    "                     'visit_end_date', 'visit_occurrence_id']]\n",
    "visit_df = gen.group_dates(visit_df, n_days=60, verbose=1)\n",
    "visit_df['visit_occurrence_id'] = np.arange(len(visit_df))\n",
    "\n",
    "# Create input dataset\n",
    "print('Creating input dataset...')\n",
    "input_df = create_sample_df(n_dates=n_dates_input,\n",
    "                            last_date=last_date,\n",
    "                            people_pool=visit_df['person_id'].unique())\n",
    "input_df = input_df.drop('end_date', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero comprobamos que los resultados obtenidos tienen sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)    \n",
    "    print('\\ndebug_find_date_ranges index')\n",
    "    debug_find_date_ranges(input_df,visit_df, debug=True, method='index')\n",
    "    print('\\ndebug_find_date_ranges merge')\n",
    "    debug_find_date_ranges(input_df,visit_df, debug=True, method='merge')\n",
    "    print('\\ngen.find_date_ranges (eq to debug_*_mege)')\n",
    "    gen.find_date_ranges(input_df,visit_df, verbose=2)\n",
    "    # print('\\nasof_find_date_ranges')\n",
    "    # asof_find_date_ranges(input_df,visit_df, debug=True)\n",
    "    # print('\\nbrute_find_date_ranges')\n",
    "    # brute_find_date_ranges(input_df,visit_df, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego comprobamos la velocidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    print(f\"{n_people = }\\n{n_dates_visit = }\\n{n_dates_input = }\")\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)    \n",
    "    print('\\ndebug_find_date_ranges index')\n",
    "    %timeit -n 5 -r 5 debug_find_date_ranges(input_df,visit_df, method='index')\n",
    "    print('\\ndebug_find_date_ranges merge')\n",
    "    %timeit -n 5 -r 5 debug_find_date_ranges(input_df,visit_df, method='merge')\n",
    "    print('\\ngen.find_date_ranges (eq to debug_*_mege)')\n",
    "    %timeit -n 5 -r 5 gen.find_date_ranges(input_df,visit_df, verbose=0)\n",
    "    # print('\\nasof_find_date_ranges')\n",
    "    # %timeit -n 5 -r 5 asof_find_date_ranges(input_df,visit_df)\n",
    "    # print('\\nbrute_find_date_ranges')\n",
    "    # %timeit -n 5 -r 5 brute_find_date_ranges(input_df,visit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se nota la diferencia entre el método por fuerza bruta y los otros hasta que no aumenta bastante el tamaño de los dataframes (>1000)\n",
    "\n",
    "El método asof no está finalizado, ya que sólo devuelve eventos con visit_occurrence_id. Aún así es más lento, ya que sigue teniendo un bucle.\n",
    "\n",
    "Los primeros tres métodos (debug-index, debug-merge y el implementado en gen) son muy similares, con tiempos similares incluso para grandes volúmenes de datos.\n",
    "\n",
    "Ordenar o no el dataframe dentro de la función find_date_range() parece tener un pequeño efecto en la velocidad, siendo levemente más rápido (190 ms a 170 ms). Esto parece ser relevante sólo para dataframes grandes (>10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = input_df.sort_values(['person_id','event_id','start_date'])\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = debug_find_date_ranges(input_df,visit_df, debug=True, method='index')\n",
    "output_df = output_df.sort_values(['person_id','event_id','start_date'])\n",
    "output_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
