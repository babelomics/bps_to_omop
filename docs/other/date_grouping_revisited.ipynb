{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New observation_period_grouping\n",
    "\n",
    "Resulta que el código que agrupa las fechas en OBSERVATION_PERIOD no es del todo correcto. No tiene en cuenta las citas contenidas una dentro de otra. Esto ya lo hemos hecho para VISIT_OCCURRENCE. Ya que además hemos descubierto que sin depender de concatenar dataframes es todo mucho (**MUCHO**) más rápido, vamos a aprovechar para reescribirlo.\n",
    "\n",
    "Primero hay que eliminar las citas que vengan contenidas en otra previa. Esto ya se hizo para VISIT_OCCURRENCE, pero vamos a intentar reescribirlo como una función recursiva.\n",
    "\n",
    "Luego, con el dataframe limpio de citas que se superponen, haremos otra función que calcule las distancias entre citas y elimine las que estén cerca.\n",
    "\n",
    "**12/09/2024** - Las funciones generadas y descritas en este documento se han movido a `ETL1_transform.general`. Sustituyen a las creadas para las tablas OBSERVATION_PERIOD y VISIT_OCCURRENCE, así que estas se han borrado de sus respectivos archivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Eliminar overlap\n",
    "\n",
    "El problema consiste en identificar si, para un paciente dado, el sistema tiene citas que se superponen. Para comprobarlo habría que ordenar las filas por cada persona. \n",
    "\n",
    "Vamos a ordenar primero por person_id de manera descendente, para que todas las interacciones de una persona estén juntas. Segundo por start_date de manera descedente, para que las fechas iniciales estén ordenadas en el tiempo. **Tercero, vamos a ordenar por end_date pero de manera descendente**. Esto me asegura que, en una serie de citas que empiezan el mismo día, la primera fila sea la más duradera y, por tanto, tendrá más posibilidades de englobar a las demás.\n",
    "\n",
    "Si un start_date de una fila está dentro del intervalo definido en el start_date y end_date anterior, significa que podemos eliminar la fila anterior.\n",
    "\n",
    "La siguiente clave está en la jerarquía de las citas, si dos citas se superponen y una tiene código de visita a hospital (cód. 8756) y otra de receta farmacia (cód. 581458), el evento más general es la visita al hospital, por lo que es el que debe prevalecer si hay que elegir cual quitar.\n",
    "\n",
    "## 1.1 Creación dataset de prueba\n",
    "Vamos a crear un dataframe a mano que tenga todos los problemas que nos podamos encontrar:\n",
    "- Fechas posteriores completamente contenidas en fechas anteriores\n",
    "    * (2020-01-01, 2020-02-01) contiene a (2020-01-02, 2020-02-02) y a (2020-01-04, 2020-02-04) \n",
    "        - Aquí quiero borrar la segunda.\n",
    "- Fechas posteriores que se superpongan parcialmente a fechas anteriores\n",
    "    * (2020-03-01, 2020-04-01) contiene parcialmente a (2020-03-15, 2020-04-15) \n",
    "        - Aquí quiero combinar la más antigua con la más nueva => (2020-03-01, 2020-04-15).\n",
    "- Asegurarse de que no se mezclen datos de dos personas distintas.\n",
    "    * Comprobar que nunca se combinan datos con person_id distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   person_id      15 non-null     int64         \n",
      " 1   start_date     15 non-null     datetime64[ns]\n",
      " 2   end_date       15 non-null     datetime64[ns]\n",
      " 3   type_concept   15 non-null     int64         \n",
      " 4   should_remain  15 non-null     bool          \n",
      "dtypes: bool(1), datetime64[ns](2), int64(2)\n",
      "memory usage: 627.0 bytes\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>type_concept</th>\n",
       "      <th>should_remain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>2020-04-15</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>2021-02-02</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-02-04</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>2021-04-15</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    person_id start_date   end_date  type_concept  should_remain\n",
       "0           1 2020-01-01 2020-02-01             1           True\n",
       "1           1 2020-01-02 2020-01-02             2          False\n",
       "2           1 2020-01-04 2020-01-04             2          False\n",
       "3           1 2020-01-06 2020-01-06             2          False\n",
       "4           1 2020-01-08 2020-01-08             2          False\n",
       "5           1 2020-01-04 2020-01-04             2          False\n",
       "6           1 2020-03-01 2020-04-01             1           True\n",
       "7           1 2020-03-15 2020-04-15             2           True\n",
       "8           1 2021-01-01 2021-02-01             1           True\n",
       "9           2 2021-01-02 2021-02-02             2           True\n",
       "10          3 2021-01-04 2021-02-04             2           True\n",
       "11          1 2021-03-01 2021-04-01             1           True\n",
       "12          2 2021-03-15 2021-04-15             2           True\n",
       "13          4 2022-03-01 2022-04-01             2          False\n",
       "14          4 2022-03-01 2022-04-01             1           True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nombre_columnas = ['person_id', 'start_date',\n",
    "                   'end_date', 'type_concept', 'should_remain']\n",
    "filas = [\n",
    "    # Problema de fechas\n",
    "    (1, '2020-01-01', '2020-02-01', 1, True),\n",
    "    (1, '2020-01-02', '2020-01-02', 2, False),\n",
    "    (1, '2020-01-04', '2020-01-04', 2, False),\n",
    "    (1, '2020-01-06', '2020-01-06', 2, False),\n",
    "    (1, '2020-01-08', '2020-01-08', 2, False),\n",
    "    (1, '2020-01-04', '2020-01-04', 2, False),\n",
    "    (1, '2020-03-01', '2020-04-01', 1, True),\n",
    "    (1, '2020-03-15', '2020-04-15', 2, True),\n",
    "    # Problema de person_id\n",
    "    (1, '2021-01-01', '2021-02-01', 1, True),\n",
    "    (2, '2021-01-02', '2021-02-02', 2, True),\n",
    "    (3, '2021-01-04', '2021-02-04', 2, True),\n",
    "    (1, '2021-03-01', '2021-04-01', 1, True),\n",
    "    (2, '2021-03-15', '2021-04-15', 2, True),\n",
    "    # Problema de type_concept\n",
    "    (4, '2022-03-01', '2022-04-01', 2, False),\n",
    "    (4, '2022-03-01', '2022-04-01', 1, True),\n",
    "]\n",
    "df_raw = pd.DataFrame.from_records(filas, columns=nombre_columnas)\n",
    "df_raw['start_date'] = pd.to_datetime(df_raw['start_date'])\n",
    "df_raw['end_date'] = pd.to_datetime(df_raw['end_date'])\n",
    "(print(df_raw.info()))\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person_id\n",
       "1    1\n",
       "2    2\n",
       "3    2\n",
       "4    2\n",
       "Name: type_concept, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_raw.copy()\n",
    "df.groupby('person_id')['type_concept'].first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Eliminar overlap\n",
    "La idea de este código está clara. Se parte de un dataframe que tiene las columnas `person_id`, `start_date`, `end_date`, `type_concept`. Se ordena en el siguiente orden:\n",
    "1. person_id, ascendente\n",
    "2. start_date, ascendente\n",
    "3. end_date, descendente\n",
    "4. type_concept, ascendente\n",
    "\n",
    "* De este modo nos aseguramos de que para cada `start_date`, la primera fila tiene la `end_date` más alejada, que es la que puede contener a las otras filas que tengan la misma `start_date`.\n",
    "* La columna type_concept tiene que transformarse previamente en tipo categoría con un orden que predefinamos, para que así podamos efectuar el orden. Este orden representará la prioridad del código. Cuando todo lo anterior sea igual, la que permanecerá será aquella fila que esté más arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlap_index(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Finds all rows that are contained with the previous \n",
    "    row, making sure they belong to the same person_id.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas Dataframe with at least three columns.\n",
    "        Assumes first column is person_id, second column is\n",
    "        start_date and third column is end_date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        pandas Series with bools. True if row is contained\n",
    "        with the previous row, False otherwise.\n",
    "    \"\"\"\n",
    "    # 1. Check that current and previous patient are the same\n",
    "    idx_person = df.iloc[:, 0] == df.iloc[:, 0].shift(1)\n",
    "    # 2. Check that current start_date is later that previous start_date\n",
    "    idx_start = df.iloc[:, 1] >= df.iloc[:, 1].shift(1)\n",
    "    # 3.  Check that current end_date is sooner that previous end_date\n",
    "    idx_end = df.iloc[:, 2] <= df.iloc[:, 2].shift(1)\n",
    "    # 4. If everything past is true, I can drop the row\n",
    "    return idx_start & idx_end & idx_person\n",
    "\n",
    "\n",
    "def remove_all_overlap_original(df: pd.DataFrame,\n",
    "                                counter_lim: int = 1000,\n",
    "                                verbose: bool = False) -> pd.DataFrame:\n",
    "\n",
    "    cols_to_show = ['person_id', 'start_date', 'end_date', 'type_concept']\n",
    "    # Prepare the while loop\n",
    "    idx_to_remove_sum = 1\n",
    "    counter = 0\n",
    "    if verbose:\n",
    "        print('Cleaning...')\n",
    "    # Start the loop\n",
    "    while (idx_to_remove_sum > 0) and (counter <= counter_lim):\n",
    "        # Get the rows\n",
    "        idx_to_remove = find_overlap_index(df)\n",
    "        # Prepare next loop\n",
    "        idx_to_remove_sum = idx_to_remove.sum()\n",
    "        counter += 1\n",
    "        # Print the statements\n",
    "        if verbose:\n",
    "            print(f\"{counter} => {idx_to_remove_sum} rows removed. Example:\")\n",
    "        # Show info of first case as an example\n",
    "        if verbose & (idx_to_remove_sum > 0):\n",
    "            idx_first_true = idx_to_remove.idxmax()\n",
    "            print(df.loc[[idx_first_true-1, idx_first_true], cols_to_show])\n",
    "        # Remove the overlapping rows\n",
    "        df = df.loc[~idx_to_remove].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_overlap(\n",
    "        df: pd.DataFrame,\n",
    "        verbose: int = 0,\n",
    "        _counter: int = 0,\n",
    "        _counter_lim: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"Removes all rows that are completely contained within \n",
    "    another row. It will not remove rows that are only partially\n",
    "    contained within the previous one.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas dataframe with at least four columns: \n",
    "        ['person_id', 'start_date', 'end_date'].\n",
    "        Column names do not need to be the same but, the order \n",
    "        must be the same as here. \n",
    "        This allows its use for different tables with columns\n",
    "        that have the same purpose but different names.\n",
    "    verbose : int, optional\n",
    "        Information output, by default 0\n",
    "        - 0 No info\n",
    "        - 1 Show number of iterations\n",
    "        - 2 Show an example of the first row being removed and\n",
    "            the row that contains it.\n",
    "    _counter : int\n",
    "        Iteration control param. Number of iterations. \n",
    "        0 will be used to begin and function will take over.\n",
    "    _counter_lim : int, optional\n",
    "        Iteration control param. Limit of iterations, by default 1000\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of input dataframe with contained rows removed.\n",
    "    \"\"\"\n",
    "    # == Preparation =================================================\n",
    "    # Sort the dataframe if first iteration\n",
    "    if _counter == 0:\n",
    "        df = df.sort_values(\n",
    "            [df.columns[0], df.columns[1], df.columns[2], df.columns[3]],\n",
    "            ascending=[True, True, False, True])\n",
    "\n",
    "    # == Find indexes ================================================\n",
    "    # Get the rows\n",
    "    idx_to_remove = find_overlap_index(df)\n",
    "\n",
    "    # == Main \"loop\" =================================================\n",
    "    # Prepare next loop\n",
    "    idx_to_remove_sum = idx_to_remove.sum()\n",
    "    _counter += 1\n",
    "    # If there's still room to go, go\n",
    "    if (idx_to_remove_sum != 0) and (_counter < _counter_lim):\n",
    "        if verbose > 0:\n",
    "            # Show iteration and number of rows removed\n",
    "            print(f\"Iter {_counter} => {idx_to_remove_sum} rows removed.\")\n",
    "        if verbose > 1:\n",
    "            # Get first removed row and show container and contained row\n",
    "            idx_max = df.index.get_loc(idx_to_remove.idxmax())\n",
    "            print(f\"{df.iloc[(idx_max-1):idx_max+1, :4]}\")\n",
    "\n",
    "        return remove_overlap(df.loc[~idx_to_remove], verbose, _counter)\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bps_to_omop.general as gen\n",
    "import sys\n",
    "import pyarrow as pa\n",
    "sys.path.append('..')\n",
    "\n",
    "table_raw = pa.Table.from_pandas(df_raw)\n",
    "table_raw = table_raw.cast(\n",
    "    pa.schema([\n",
    "        ('person_id', pa.int64()),\n",
    "        ('start_date', pa.date64()),\n",
    "        ('end_date', pa.date64()),\n",
    "        ('type_concept', pa.int64()),\n",
    "        ('should_remain', pa.int64())\n",
    "    ])\n",
    ")\n",
    "df_rare = table_raw.to_pandas()\n",
    "df_done = gen.remove_overlap(df_rare, 1)\n",
    "# df_done = remove_all_overlap_original(df_rare, 10, verbose=True)\n",
    "df_done.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_raw = pa.Table.from_pandas(df_raw)\n",
    "table_raw = table_raw.cast(\n",
    "    pa.schema([\n",
    "        ('person_id', pa.int64()),\n",
    "        ('start_date', pa.timestamp('us')),\n",
    "        ('end_date', pa.timestamp('us')),\n",
    "        ('type_concept', pa.int64()),\n",
    "        ('should_remain', pa.int64())\n",
    "    ])\n",
    ")\n",
    "df_rare = table_raw.to_pandas()\n",
    "df_done = gen.remove_overlap(df_rare, 1)\n",
    "# df_done = remove_all_overlap_original(df_rare, 10, verbose=True)\n",
    "df_done.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done = remove_overlap(df_raw, True)\n",
    "# df_done = remove_all_overlap_original(df_rare, 10, verbose=True)\n",
    "df_done.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 5 remove_overlap(df_raw, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Eliminar filas cercanas\n",
    "Una vez que las filas contenidas en otras se han eliminado, el objetivo ahora es eliminar aquellas que están separadas por un número de días menor al que estipulemos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Creación dataset de prueba\n",
    "Vamos a suponer que vamos a agrupar aquellas fechas a menos de 1 año (365 días exactamente) una de otra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nombre_columnas = ['person_id', 'start_date',\n",
    "                   'end_date', 'type_concept']\n",
    "filas = [\n",
    "    # Estas fechas deberían juntarse porque están a menos de 365 dias\n",
    "    # type_concept debería ser 2\n",
    "    (1, '2020-01-01', '2020-02-01', 1),\n",
    "    (1, '2020-03-01', '2020-04-01', 2),\n",
    "    (1, '2020-05-01', '2020-12-01', 2),\n",
    "    # Esta última de la misma persona no\n",
    "    (1, '2022-01-01', '2022-01-01', 2),\n",
    "    # Estas fechas deberían juntarse porque se pisan\n",
    "    # type_concept debería ser 1\n",
    "    (2, '2020-01-01', '2020-06-01', 1),\n",
    "    (2, '2020-03-01', '2020-09-01', 1),\n",
    "    (2, '2020-06-01', '2020-12-01', 2),\n",
    "    # Estas dos fechas NO deberían juntarse,\n",
    "    # cada uno es su propio periodo\n",
    "    (3, '2021-01-01', '2021-01-01', 1),\n",
    "    (3, '2023-02-01', '2023-02-01', 2),\n",
    "    (3, '2024-03-01', '2024-04-01', 3),\n",
    "    # Se juntarían pero no porque son personas distintas\n",
    "    (4, '2024-01-01', '2024-02-01', 1),\n",
    "    (5, '2025-01-01', '2025-02-01', 2),\n",
    "    # Deberían juntarse porque entras ellas hay poca distancia,\n",
    "    # pero si eliminas una de golpe las otras están muy\n",
    "    # separadas y no se juntan.\n",
    "    # type_concept debería ser 2\n",
    "    (6, '2020-01-01', '2020-12-01', 1),\n",
    "    (6, '2021-01-01', '2021-12-01', 2),\n",
    "    (6, '2022-01-01', '2022-12-01', 2),\n",
    "    (6, '2023-01-01', '2023-12-01', 2),\n",
    "]\n",
    "df_raw = pd.DataFrame.from_records(filas, columns=nombre_columnas)\n",
    "df_raw['start_date'] = pd.to_datetime(df_raw['start_date'])\n",
    "df_raw['end_date'] = pd.to_datetime(df_raw['end_date'])\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponiendo que **agrupamos periodos separados por menos de 365 días** y que **usamos la moda para calcular el `type_concept` final**. El resultado del agrupamiento de los datos creados debería ser el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_columnas = ['person_id', 'start_date',\n",
    "                   'end_date', 'type_concept']\n",
    "filas = [\n",
    "    # Estas fechas deberían juntarse porque están a menos de 365 dias\n",
    "    # type_concept debería ser 2\n",
    "    (1, '2020-01-01', '2020-12-01', 2),\n",
    "    # Esta última de la misma persona no\n",
    "    (1, '2022-01-01', '2022-01-01', 2),\n",
    "    # Estas fechas deberían juntarse porque se pisan\n",
    "    # type_concept debería ser 1\n",
    "    (2, '2020-01-01', '2020-12-01', 1),\n",
    "    # Estas dos fechas NO deberían juntarse,\n",
    "    # cada uno es su propio periodo\n",
    "    (3, '2021-01-01', '2021-01-01', 1),\n",
    "    (3, '2023-02-01', '2023-02-01', 2),\n",
    "    (3, '2024-03-01', '2024-04-01', 3),\n",
    "    # Se juntarían pero no porque son personas distintas\n",
    "    (4, '2024-01-01', '2024-02-01', 1),\n",
    "    (5, '2025-01-01', '2025-02-01', 2),\n",
    "    # Deberían juntarse porque entras ellas hay poca distancia,\n",
    "    # pero si eliminas una de golpe las otras están muy\n",
    "    # separadas y no se juntan.\n",
    "    # type_concept debería ser 2\n",
    "    (6, '2020-01-01', '2023-12-01', 2),\n",
    "]\n",
    "df_result = pd.DataFrame.from_records(filas, columns=nombre_columnas)\n",
    "df_result['start_date'] = pd.to_datetime(df_result['start_date'])\n",
    "df_result['end_date'] = pd.to_datetime(df_result['end_date'])\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Eliminación de filas cercanas\n",
    "\n",
    "Hay varios problemas\n",
    "- Si te encuentras varias filas que cumplen la condición seguidas, puedes perder información si la primera y la última filas están muy separadas.\n",
    "- No se ha encontrado una manera efectiva de hacer esto sin iterar como antes.\n",
    "    - O bien iteras por personas, y no tienes que vigilar que mezclas personas\n",
    "    - O bien lo haces de golpe, pero es muy complejo llevar la cuenta de los `type_concept` y `person_id` que has eliminado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Usando sólo índices\n",
    "\n",
    "Hay que encontrar la primera y última fila de cada persona y también aquellos casos en los que sólo haya una única fila.\n",
    "\n",
    "Luego hay que buscar también aquellos casos en los que la siguiente fila esté muy alejada, lo que implicaría que hemos encontrado una brecha en el periodo de observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw files\n",
    "df_rare = df_raw.sort_values(\n",
    "    ['person_id', 'start_date', 'end_date'],\n",
    "    ascending=[True, True, False])\n",
    "# It is VERY important to reset the index to make sure we can\n",
    "# retrieve them realiably after sorting them.\n",
    "df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "# Create index for first, last or only person in dataset\n",
    "df_rare['idx_person_first'] = (\n",
    "    (df_rare['person_id'] == df_rare['person_id'].shift(-1)) &\n",
    "    (df_rare['person_id'] != df_rare['person_id'].shift(1))\n",
    ")\n",
    "df_rare['idx_person_last'] = (\n",
    "    (df_rare['person_id'] != df_rare['person_id'].shift(-1)) &\n",
    "    (df_rare['person_id'] == df_rare['person_id'].shift(1))\n",
    ")\n",
    "df_rare['idx_person_only'] = (\n",
    "    (df_rare['person_id'] != df_rare['person_id'].shift(-1)) &\n",
    "    (df_rare['person_id'] != df_rare['person_id'].shift(1))\n",
    ")\n",
    "# Create index if the break is too big and needs to be kept\n",
    "n_days = 365\n",
    "df_rare['next_interval'] = (\n",
    "    df_rare['start_date'].shift(-1) - df_rare['end_date']\n",
    ")\n",
    "df_rare['idx_interval'] = (\n",
    "    df_rare['next_interval'] >= pd.Timedelta(n_days, unit='D')\n",
    ")\n",
    "# Combine all to see which rows remain\n",
    "df_rare['to_remain'] = (\n",
    "    df_rare['idx_person_first'] |\n",
    "    df_rare['idx_person_last'] |\n",
    "    df_rare['idx_person_only'] |\n",
    "    df_rare['idx_interval']\n",
    ")\n",
    "\n",
    "df_rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta gente me tengo que quedar seguro :\n",
    "- `idx_person_first == True`\n",
    "- `idx_person_last == True`\n",
    "- `idx_person_only == True`\n",
    "\n",
    "1. Si para una persona sólo hay 1 `idx_person_only == True`, me quedo ese y a correr. En este caso no hay que hacer nada, esa fila tiene la primera y la última fecha del paciente.\n",
    "\n",
    "2. Si para una persona sólo hay 1 `idx_person_first == True` y 1 `idx_person_last == True`, entonces tengo el principio y el final. \n",
    "    1. Si no hay ningún `idx_interval == True`, junto la `start_date` del `idx_person_first == True` y la `end_date` del `idx_person_last == True`.\n",
    "    2. Si hay algún `idx_interval == True`, tengo que tener en cuenta que esas filas indican brechas en el periodo de observación. La fila donde `idx_interval == True` indica que es la última del periodo y que la siguiente es el comienzo de otro.\n",
    "\n",
    "Básicamente hay que registrar por un lado las `start_date`, con sus respectivos `person_id`, y por otro lado las nuevas `end_date`. Vamos a escribir el código que registra esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create an initial dataframe with only person_id and\n",
    "# start_date. The end_date rows and type_concept will be added\n",
    "# later as new columns.\n",
    "\n",
    "# == start_date and person_id ==========================================\n",
    "# To retrieve the start_date we need the indexes of:\n",
    "# - single day periods (idx_person_only == True)\n",
    "# - first dates (idx_person_first == True)\n",
    "# - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "# Get the person condition indexes\n",
    "idx_start = df_rare.index[\n",
    "    df_rare['idx_person_only']\n",
    "    | df_rare['idx_person_first']\n",
    "    | df_rare['idx_interval'].shift(1)\n",
    "]\n",
    "# Get the interval indexes\n",
    "df_done = df_rare.loc[idx_start, ['person_id', 'start_date']]\n",
    "df_done\n",
    "\n",
    "# == end_date ==========================================================\n",
    "# Get the indexes\n",
    "idx_end = df_rare.index[\n",
    "    df_rare['idx_person_only']\n",
    "    | df_rare['idx_person_last']\n",
    "    | df_rare['idx_interval']\n",
    "]\n",
    "# Append values found to final dataframe\n",
    "df_done['end_date'] = df_rare.loc[idx_end, ['end_date']].values\n",
    "\n",
    "df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar los `type_concept`, podemos usar los índices del principio y el final, hacer un zip y, como deberían estar en orden. Tendré una lista con las parejas inicio final de cada periodo.\n",
    "\n",
    "Si busco todos los `type_concept` dentro de esos periodos, puedo hacer la moda y asignar el `type_concept` más común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "# I can iterate over idx_start and idx_end to get the\n",
    "# periods\n",
    "mode_values = []\n",
    "for i in np.arange(len(idx_start)):\n",
    "    df_tmp = df_rare.loc[idx_start[i]:idx_end[i]]\n",
    "    print(f\"{i=}\")\n",
    "    print(df_tmp[['person_id', 'start_date', 'end_date', 'type_concept']])\n",
    "    mode = st.mode(df_tmp['type_concept'].values)\n",
    "    print(f\"mode is {mode}\", '\\n')\n",
    "    mode_values.append(mode[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to dataframe\n",
    "df_done['type_concept'] = mode_values\n",
    "df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y listo, ya tengo el dataframe con sólo los inicios y finales de los periodos, incluyendo el type_concept más común calculado usando la moda. Comprobamos que es igual que los resultados esperados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_person = df_done['person_id'].values == df_result['person_id'].values\n",
    "print(f\"{'Person_id col is correct:':<28} {check_person.all()}\")\n",
    "check_start_date = df_done['start_date'].values == df_result['start_date'].values\n",
    "print(f\"{'start_date col is correct:':<28} {check_start_date.all()}\")\n",
    "check_end_date = df_done['end_date'].values == df_result['end_date'].values\n",
    "print(f\"{'end_date col is correct:':<28} {check_end_date.all()}\")\n",
    "check_type_concept = df_done['type_concept'].values == df_result['type_concept'].values\n",
    "print(f\"{'type_concept col is correct:'::<28} {check_type_concept.all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Prueba con función recursive (NOT FINISHED)\n",
    "\n",
    "ESTO ESTÁ AQUÍ PARA FUTURAS REFERENCIAS. EL CÓDIGO NO ESTÁ TERMINADO PORQUE EL MÉTODO POR ÍNDICES FUNCIONA LO SUFICIENTEMENTE BIEN Y NO HAY GARANTÍAS DE QUE ESTO LO MEJORE.\n",
    "\n",
    "Parece que lo mejor (cof) va a ser repetir la estrategia anterior e iterar recursivamente. Así además nos aseguramos que podemos llevar la cuenta de los type_concept y quedarnos con el más representativo.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_neighbors_index(df: pd.DataFrame,\n",
    "#                          n_days: int) -> pd.Series:\n",
    "\n",
    "#     # 1. Check that current and next patient are the same\n",
    "#     idx_person = df.iloc[:, 0] == df.iloc[:, 0].shift(-1)\n",
    "#     # 2. Check that current end_date and next start_date\n",
    "#     # are closer than n_days\n",
    "#     idx_interval = (\n",
    "#         (df.iloc[:, 2] - df.iloc[:, 1].shift(-1)) <=\n",
    "#         pd.Timedelta(n_days, unit='D')\n",
    "#     )\n",
    "#     # 4. If everything past is true, I can drop the row\n",
    "#     return idx_person & idx_interval\n",
    "\n",
    "\n",
    "# def remove_all_neighbors_recursive_v1(\n",
    "#         df: pd.DataFrame,\n",
    "#         n_days: int,\n",
    "#         verbose: int = 0,\n",
    "#         _counter: int = 0,\n",
    "#         _counter_lim: int = 1000) -> pd.DataFrame:\n",
    "\n",
    "#     # Get the rows\n",
    "#     idx_to_remove = find_neighbors_index(df, n_days)\n",
    "#     # Prepare next loop\n",
    "#     idx_to_remove_sum = idx_to_remove.sum()\n",
    "#     _counter += 1\n",
    "#     # If there's still room to go, go\n",
    "#     if (idx_to_remove_sum != 0) and (_counter < _counter_lim):\n",
    "#         if verbose >= 1:\n",
    "#             print(f\"Iter {_counter} => {idx_to_remove_sum} rows removed.\")\n",
    "#         if verbose >= 2:\n",
    "#             print(df[idx_to_remove].head(10))\n",
    "\n",
    "#         # Modify end_dates\n",
    "#         df.iloc[:,2] = np.where(idx_to_remove,\n",
    "#                                 df.iloc[:,2].shift(-1),\n",
    "#                                 df.iloc[:,2])\n",
    "#         return remove_all_neighbors_recursive_v1(\n",
    "#             df[idx_to_remove], verbose, _counter)\n",
    "#     else:\n",
    "#         return df\n",
    "\n",
    "# n_days = 365\n",
    "# df_rare = df_raw.sort_values(\n",
    "#     ['person_id', 'start_date', 'end_date'],\n",
    "#     ascending=[True, True, True])\n",
    "# df_rare = remove_all_neighbors_recursive_v1(df_rare, n_days, verbose=2)\n",
    "# df_done = df_rare.sort_index()\n",
    "# df_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_days = 365\n",
    "# df = df_raw.sort_values(\n",
    "#     ['person_id', 'start_date', 'end_date'],\n",
    "#     ascending=[True, True, True])\n",
    "\n",
    "# # >>> Iter 1\n",
    "# idx_to_remove = find_neighbors_index(df, n_days)\n",
    "# print(f\"Iter {1} => {idx_to_remove.sum()} rows removed.\")\n",
    "# print(df[idx_to_remove])\n",
    "# # <<<\n",
    "\n",
    "# # Record changes\n",
    "# df['to_join'] = idx_to_remove\n",
    "# df['new_end_date'] = np.where(df['to_join'],\n",
    "#                               df.iloc[:, 2].shift(-1),\n",
    "#                               df.iloc[:, 2])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # >>> # Iter 2\n",
    "# df.iloc[:,2] = np.where(idx_to_remove,\n",
    "#                         df.iloc[:,2].shift(-1),\n",
    "#                         df.iloc[:,2])\n",
    "# idx_to_remove = find_neighbors_index(df, n_days)\n",
    "# # <<<\n",
    "\n",
    "# # Record changes\n",
    "# df['to_join'] = idx_to_remove\n",
    "# df['new_end_date'] = np.where(df['to_join'],\n",
    "#                               df.iloc[:, 2].shift(-1),\n",
    "#                               df.iloc[:, 2])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # >>> # Iter 3\n",
    "# df.iloc[:,2] = np.where(idx_to_remove,\n",
    "#                         df.iloc[:,2].shift(-1),\n",
    "#                         df.iloc[:,2])\n",
    "# idx_to_remove = find_neighbors_index(df, n_days)\n",
    "# # <<<\n",
    "\n",
    "# # Record changes\n",
    "# df['to_join'] = idx_to_remove\n",
    "# df['new_end_date'] = np.where(df['to_join'],\n",
    "#                               df.iloc[:, 2].shift(-1),\n",
    "#                               df.iloc[:, 2])\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Todo junto\n",
    "\n",
    "Ahora juntamos todo en una función, para poder medir el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "def find_person_index(df: pd.DataFrame) -> tuple[pd.Series]:\n",
    "    \"\"\"Finds all rows that are contained with the previous \n",
    "    row, making sure they belong to the same person_id.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas Dataframe with at least three columns.\n",
    "        Assumes first column is person_id, second column is\n",
    "        start_date and third column is end_date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pd.Series]\n",
    "        Tuple with three pandas Series with bools:\n",
    "        - idx_person_first, True if first row of the person\n",
    "        - idx_person_last, True if last row of the person\n",
    "        - idx_person_only, True if only row of the person\n",
    "        False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create index for first, last or only person in dataset\n",
    "    idx_person_first = (\n",
    "        (df.iloc[:, 0] == df.iloc[:, 0].shift(-1)) &\n",
    "        (df.iloc[:, 0] != df.iloc[:, 0].shift(1))\n",
    "    )\n",
    "    idx_person_last = (\n",
    "        (df.iloc[:, 0] != df.iloc[:, 0].shift(-1)) &\n",
    "        (df.iloc[:, 0] == df.iloc[:, 0].shift(1))\n",
    "    )\n",
    "    idx_person_only = (\n",
    "        (df.iloc[:, 0] != df.iloc[:, 0].shift(-1)) &\n",
    "        (df.iloc[:, 0] != df.iloc[:, 0].shift(1))\n",
    "    )\n",
    "    return (idx_person_first, idx_person_last, idx_person_only)\n",
    "\n",
    "\n",
    "def group_dates(df: pd.DataFrame, n_days: int) -> pd.DataFrame:\n",
    "    \"\"\"Groups rows of dates from the same person that are less\n",
    "    than n_days apart, keeping only the first start_date and\n",
    "    the last end_date, respectively. \n",
    "\n",
    "    It will remove rows that are partially contained within \n",
    "    the previous one.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas dataframe with at least four columns: \n",
    "        ['person_id', 'start_date', 'end_date', 'type_concept'].\n",
    "        Column names do not need to be the same but, the order \n",
    "        must be the same as here. \n",
    "        This allows its use for different tables with columns\n",
    "        that have the same purpose but different names.\n",
    "    verbose : int, optional\n",
    "        Information output, by default 0\n",
    "        - 0 No info\n",
    "        - 1 Show number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of input dataframe with grouped rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # == Preparation ==============================================\n",
    "    # Sort so we know for sure the order is right\n",
    "    df_rare = df.copy().sort_values(\n",
    "        [df.columns[0], df.columns[1], df.columns[2]],\n",
    "        ascending=[True, True, False])\n",
    "    # It is VERY important to reset the index to make sure we can\n",
    "    # retrieve them realiably after sorting them.\n",
    "    df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "    # == Index look-up ============================================\n",
    "    (idx_person_first, idx_person_last, idx_person_only) = find_person_index(df_rare)\n",
    "    # Create index if the break is too big and needs to be kept\n",
    "    next_interval = df_rare.iloc[:, 1].shift(-1) - df_rare.iloc[:, 2]\n",
    "    idx_interval = next_interval >= pd.Timedelta(n_days, unit='D')\n",
    "\n",
    "    # == Retrieve relevant rows ===================================\n",
    "    # -- start_date and person_id ---------------------------------\n",
    "    # To retrieve the start_date we need the indexes of:\n",
    "    # - single day periods (idx_person_only == True)\n",
    "    # - first dates (idx_person_first == True)\n",
    "    # - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "    # Get the person condition indexes\n",
    "    idx_start = df_rare.index[\n",
    "        idx_person_only | idx_person_first | idx_interval.shift(1)\n",
    "    ]\n",
    "\n",
    "    # -- end_date -------------------------------------------------\n",
    "    # Get the indexes\n",
    "    idx_end = df_rare.index[\n",
    "        idx_person_only | idx_person_last | idx_interval\n",
    "    ]\n",
    "\n",
    "    # == Compute type_concept =====================================\n",
    "    # Iterate over idx_start and idx_end to get the periods\n",
    "    mode_values = []\n",
    "    for i in np.arange(len(idx_start)):\n",
    "        df_tmp = df_rare.loc[idx_start[i]:idx_end[i]]\n",
    "        mode_values.append(st.mode(df_tmp.iloc[:, 3].values)[0])\n",
    "\n",
    "    # == Build final dataframe ====================================\n",
    "    # Create a copy (.loc) with the first two columns\n",
    "    df_done = df_rare.loc[idx_start, [df.columns[0], df.columns[1]]]\n",
    "    # Append values found to final dataframe\n",
    "    df_done[df.columns[2]] = df_rare.loc[idx_end, [df.columns[2]]].values\n",
    "    # Add to dataframe\n",
    "    df_done[df.columns[3]] = mode_values\n",
    "\n",
    "    return df_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Parametros ==\n",
    "n_days = 365\n",
    "\n",
    "# == Creación de datos ==\n",
    "df_done = group_dates(df_raw, n_days)\n",
    "df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "volvemos a comprobar que sale bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_person = df_done['person_id'].values == df_result['person_id'].values\n",
    "print(f\"{'Person_id col is correct:':<28} {check_person.all()}\")\n",
    "check_start_date = df_done['start_date'].values == df_result['start_date'].values\n",
    "print(f\"{'start_date col is correct:':<28} {check_start_date.all()}\")\n",
    "check_end_date = df_done['end_date'].values == df_result['end_date'].values\n",
    "print(f\"{'end_date col is correct:':<28} {check_end_date.all()}\")\n",
    "check_type_concept = df_done['type_concept'].values == df_result['type_concept'].values\n",
    "print(f\"{'type_concept col is correct:'::<28} {check_type_concept.all()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 10 group_dates(df_rare,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prueba con datasets grandes\n",
    "Vamos a comparar si el metodo de pyarrow sigue funcionando más rápido con datasets grandes.\n",
    "\n",
    "Nos traemos la función para generar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import parquet\n",
    "\n",
    "\n",
    "def create_sample_df(n: int = 1000, n_dates: int = 50,\n",
    "                     first_date: str = '2020-01-01',\n",
    "                     last_date: str = '2023-01-01',\n",
    "                     mean_duration_days: int = 60,\n",
    "                     std_duration_days: int = 180) -> pd.DataFrame:\n",
    "\n",
    "    # == Parameters ==\n",
    "    np.random.seed(42)\n",
    "    pd.options.mode.string_storage = \"pyarrow\"\n",
    "    # Start date from which to start the dates\n",
    "    first_date = pd.to_datetime(first_date)\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    max_days = (last_date-first_date).days\n",
    "\n",
    "    # == Generate IDs randomly ==\n",
    "    # -- Generate the Ids\n",
    "    people = np.random.randint(10000000, 99999999 + 1, size=n)\n",
    "    person_id = np.random.choice(people, n*n_dates)\n",
    "\n",
    "    # == Generate random dates ==\n",
    "    # Generate random integers for days and convert to timedelta\n",
    "    random_days = np.random.randint(0, max_days, size=n*n_dates)\n",
    "    # Create the columns\n",
    "    observation_start_date = first_date + \\\n",
    "        pd.to_timedelta(random_days, unit='D')\n",
    "    # Generate a gaussian sample of dates\n",
    "    random_days = np.random.normal(\n",
    "        mean_duration_days, std_duration_days, size=n*n_dates)\n",
    "    random_days = np.int32(random_days)\n",
    "    observation_end_date = observation_start_date + \\\n",
    "        pd.to_timedelta(random_days, unit='D')\n",
    "    # Correct end_dates\n",
    "    # => If they are smaller than start_date, take start_date\n",
    "    observation_end_date = np.where(observation_end_date < observation_start_date,\n",
    "                                    observation_start_date, observation_end_date)\n",
    "\n",
    "    # == Generate the code ==\n",
    "    period_type_concept_id = np.random.randint(1, 11, size=n*n_dates)\n",
    "\n",
    "    # == Generate the dataframe ==\n",
    "    df_raw = {'person_id': person_id, 'observation_period_start_date': observation_start_date,\n",
    "              'observation_period_end_date': observation_end_date, 'period_type_concept_id': period_type_concept_id}\n",
    "    return pd.DataFrame(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos traemos la función de pyarrow tal y como estaba el 12/09/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bps_to_omop.general as gen\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import numpy as np\n",
    "\n",
    "# Añadimos el directorio superior al path para poder extraer\n",
    "# las funciones de las carpetas ETL*\n",
    "import sys\n",
    "import os\n",
    "# Add the parent directory of func_folder to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "\n",
    "def group_dates_original_pyarrow(table_done, n_days):\n",
    "    # -- Thirdly, group up dates -----------------------------------------------\n",
    "    # Agrupamos las fechas usando group_person_dates(). Básicamente calcula la\n",
    "    # distancia temporal entre las filas adyacentes de cada persona, juntándolas\n",
    "    # si es tan por debajo del límite marcado por n_days.\n",
    "    # Agrupamos\n",
    "    table_OBSERVATION_PERIOD = []\n",
    "\n",
    "    person_list = pc.unique(table_done['person_id'])\n",
    "    # Percentage points where you want to print progress\n",
    "    for i, person in enumerate(person_list[:]):\n",
    "        # --Group person\n",
    "        table_person = group_person_dates(table_done, person, n_days)\n",
    "        # Append table\n",
    "        table_OBSERVATION_PERIOD.append(table_person)\n",
    "    # Concatenate\n",
    "    table_OBSERVATION_PERIOD = pa.concat_tables(table_OBSERVATION_PERIOD)\n",
    "    return table_OBSERVATION_PERIOD\n",
    "\n",
    "\n",
    "def group_person_dates(\n",
    "        table_rare: pa.Table,\n",
    "        person: str | int,\n",
    "        n_days: int) -> pa.Table:\n",
    "    \"\"\"Filters original table for a specific person and reduces\n",
    "    the amount of date records grouping all records that are separated\n",
    "    by n_days or less.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_rare : pa.Table\n",
    "        Table as prepared by 'prepare_table_raw_to_rare()'.\n",
    "    person : str | int\n",
    "        person id, can be an int (the usual) or a string.\n",
    "    n_days : int\n",
    "        number of maximum days between subsequent records.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pa.Table\n",
    "        Table identical to table_rare but with less date records.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter for the current person_id\n",
    "    filt = pc.is_in(table_rare['person_id'],  # pylint: disable=E1101\n",
    "                    pa.array([person]))\n",
    "    table_person = table_rare.filter(filt)\n",
    "    # Retrieve corresponding dates\n",
    "    start_dates = table_person['start_date']\n",
    "    end_dates = table_person['end_date']\n",
    "    # Group dates closer\n",
    "    start_dates, end_dates, _ = group_observation_dates(\n",
    "        start_dates, end_dates, n_days, verbose=False)\n",
    "    # Create person\n",
    "    person_id = gen.create_uniform_int_array(len(start_dates),\n",
    "                                             value=person)\n",
    "    # Retrieve most common period type\n",
    "    period_type_concept_id = pc.mode(  # pylint: disable=E1101\n",
    "        table_person['period_type_concept_id'])[0][0]\n",
    "    period_type_concept_id = gen.create_uniform_int_array(len(start_dates),\n",
    "                                                          value=period_type_concept_id)\n",
    "    # return table\n",
    "    return pa.Table.from_arrays(\n",
    "        [person_id, start_dates, end_dates, period_type_concept_id],\n",
    "        names=['person_id', 'start_date', 'end_date', 'period_type_concept_id'])\n",
    "\n",
    "\n",
    "def group_observation_dates(\n",
    "        start_dates: pa.Array,\n",
    "        end_dates: pa.Array,\n",
    "        n_days: int,\n",
    "        verbose: bool = False) -> tuple[pa.Array, pa.Array, None | pa.Table]:\n",
    "    \"\"\"Given a pair of 'start_dates' and 'end_dates', it will\n",
    "    compute the days between each 'end_date' and the next\n",
    "    'start_date' and remove dates that are smaller that a\n",
    "    given number of days ('n_days').\n",
    "\n",
    "    The new dates will only contain start and end dates that have\n",
    "    more than 'n_days' of difference between them.\n",
    "\n",
    "    If dates contain nans/nulls, they will be ignored and grouped \n",
    "    with the closest dates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_dates : pa.Array\n",
    "        Array of start dates\n",
    "    end_dates : pa.Array\n",
    "        Array of end dates\n",
    "    n_days : int\n",
    "        _description_\n",
    "    verbose : bool, optional\n",
    "        _description_, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pa.Array, pa.Array, None | pa.Table]\n",
    "        Always return a 3-item tuple.\n",
    "        First item is reduced start dates.\n",
    "        Second item is reduced end dates.\n",
    "        Third item is None if verbose=True,\n",
    "        if verbose=False, is table with start_dates,\n",
    "        end_dates and days between them. Usefull when\n",
    "        verifying dates.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        The resulting starting dates should always come before\n",
    "        their corresponding end dates. Return an AssertionError\n",
    "        otherwise.\n",
    "    \"\"\"\n",
    "    # Get an array of end_dates, taking away the last one\n",
    "    # (last date cannot be compared to the next start date)\n",
    "    from_dates = end_dates[:-1]\n",
    "    # Get an array of start_dates, taking away the first one\n",
    "    # (first date cannot be compared to the previous end date)\n",
    "    to_dates = start_dates[1:]\n",
    "\n",
    "    # -- Compute days between\n",
    "    intervals = pc.days_between(  # pylint: disable=E1101\n",
    "        from_dates, to_dates).to_numpy(zero_copy_only=False)\n",
    "    # Create an inner table for the calculations if verbose\n",
    "    inner_table = None\n",
    "    if verbose:\n",
    "        inner_table = pa.Table.from_arrays(\n",
    "            [start_dates, end_dates, pa.array(np.append(intervals, np.nan))],\n",
    "            names=['start', 'end', 'intervals'])\n",
    "\n",
    "    # Filter intervals under some assumption\n",
    "    filt = intervals >= n_days\n",
    "    # => When this filt is 'true', it means that for that index,\n",
    "    # let's call it 'idx', between the end date of 'idx' and the start\n",
    "    # date of 'idx+1' there more than 'n_days' days.\n",
    "    # i.e.:\n",
    "    # (start_date[idx+1] - end_date[idx]).days > n_days\n",
    "\n",
    "    # if no interval is greater, take the first and last rows\n",
    "    if np.nansum(filt) == 0:\n",
    "        idx_end_dates = np.array([len(intervals)])\n",
    "        # Sum 1 to get start dates\n",
    "        idx_start_dates = np.array([0])\n",
    "\n",
    "    # If some filters exist take those\n",
    "    else:\n",
    "        # Get indexes of end_dates\n",
    "        idx_end_dates = filt.nonzero()[0]\n",
    "        # Sum 1 to get corresponding start dates\n",
    "        idx_start_dates = idx_end_dates+1\n",
    "        # Append last entry as last end_date\n",
    "        idx_end_dates = np.append(idx_end_dates, len(intervals))\n",
    "        # Append first entry as first start_date\n",
    "        idx_start_dates = np.append(0, idx_start_dates)\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(f'{idx_start_dates=}')\n",
    "    #     print(f'{idx_end_dates=}')\n",
    "\n",
    "    # Make sure all end values are after start values\n",
    "    new_start = start_dates.take(idx_start_dates)\n",
    "    new_end = end_dates.take(idx_end_dates)\n",
    "    if pc.any(pc.less(new_end, new_start)).as_py():  # pylint: disable=E1101\n",
    "        if verbose:\n",
    "            print(f\"{start_dates=}\", f\"{end_dates=}\")\n",
    "            print(f\"{new_start=}\", f\"{new_end=}\")\n",
    "        raise AssertionError(\n",
    "            'Some end dates happen before start dates. Try sorting the original data.')\n",
    "\n",
    "    return (new_start, new_end, inner_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Sanity check\n",
    "### DATA CREATION\n",
    "\n",
    "Probamos primero que los resultados sean iguales con ambas funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "\n",
    "# Cargamos los datos\n",
    "df_raw = create_sample_df(n=100, n_dates=10,)\n",
    "df_raw.columns = ['person_id', 'start_date', 'end_date', 'type_concept']\n",
    "\n",
    "df_raw = df_raw.sort_values(\n",
    "    ['person_id', 'start_date', 'end_date', 'type_concept'],\n",
    "    ascending=[True, True, False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how quick `remove_overlap()` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 10 remove_overlap(df_raw,0,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite quick!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove contained dates\n",
    "df_raw = remove_overlap(df_raw, 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[df_raw['person_id'] == 10271836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[df_raw['person_id'] == 23315092]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYARROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == pyarrow method ==\n",
    "df_rare = df_raw.copy()\n",
    "df_rare.columns = ['person_id', 'start_date',\n",
    "                   'end_date', 'period_type_concept_id']\n",
    "table_rare = pa.Table.from_pandas(df_rare, preserve_index=False)\n",
    "table_done = group_dates_original_pyarrow(table_rare, n_days)\n",
    "df_done_pyarrow = table_done.to_pandas()\n",
    "df_done_pyarrow = df_done_pyarrow.sort_values(\n",
    "    ['person_id', 'start_date', 'end_date', 'period_type_concept_id'],\n",
    "    ascending=[True, True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_pyarrow[df_done_pyarrow['person_id'] == 10271836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_pyarrow[df_done_pyarrow['person_id'] == 23315092]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyarrow hace la primera persona (10271836), todas las fechas tienen menos de 365 días entre sí, así que se unen en una sola. Las siguientes son todas de una única fecha por persona hasta 23315092, que tiene dos. Esta también la hace bien.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Shift method ==\n",
    "df_rare = df_raw.copy()\n",
    "df_done_shift = group_dates(df_rare, n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_shift[df_done_shift['person_id'] == 10271836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_shift[df_done_shift['person_id'] == 23315092]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el método shift hace bien la primera persona (10271836) la que tiene dos periodos (23315092). El type_concept cambia del método pyarrow al shift, pero me fio más del shift en este momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Time measurement\n",
    "Ahora probamos a medir el tiempo que tarda cada uno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "\n",
    "# Cargamos los datos\n",
    "df_raw = create_sample_df(n=100)\n",
    "df_raw.columns = ['person_id', 'start_date', 'end_date', 'type_concept']\n",
    "\n",
    "df_raw = df_raw.sort_values(\n",
    "    ['person_id', 'start_date', 'end_date', 'type_concept'],\n",
    "    ascending=[True, True, False, True])\n",
    "df_rare = df_raw.reset_index(drop=True).copy()\n",
    "\n",
    "print('\\nshift:')\n",
    "%timeit -n 1 -r 1 group_dates(df_rare,n_days)\n",
    "\n",
    "df_rare = df_raw.reset_index(drop=True).copy()\n",
    "df_rare.columns = ['person_id', 'start_date', 'end_date', 'period_type_concept_id']\n",
    "table_rare = pa.Table.from_pandas(df_rare,preserve_index=False)\n",
    "print('\\npyarrow:')\n",
    "%timeit -n 1 -r 1 group_dates_original_pyarrow(table_rare,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para n = 1000\n",
    "\n",
    "    shift:\n",
    "    375 ms ± 884 μs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n",
    "    pyarrow:\n",
    "    454 ms ± 664 μs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n",
    "\n",
    "Para n = 10000\n",
    "\n",
    "    shift:\n",
    "    3.69 s ± 4.92 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "    pyarrow:\n",
    "    23.6 s ± 565 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "\n",
    "Para n = 30000\n",
    "\n",
    "    shift:\n",
    "    10.2 s ± 18.5 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "    pyarrow:\n",
    "    3min 10s ± 5.35 s per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
    " \n",
    "\n",
    "Ahora está bastante claro que el método shift funciona mucho más rápido si tenemos muchas personas. Al final en el método original estamos pegando tablas una encima de otra, lo cual resta mucho tiempo. Y esto teniendo en cuenta que el método shift está ordenando dentro de la propia función, cosa que en el de pyarrow dejamos fuera.\n",
    "\n",
    "Quizá si se pudiera implementar con pyarrow un modo siguiendo el patrón de los shift, se podría conseguir algo mejor. Con todo, los 10 s con 30000 paciente y 50 fechas por paciente ya me parece un buen resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba timestamp vs datetime\n",
    "\n",
    "Nos hemos encontrado que el código remove_overlap va mucho más rápido si las fechas están en formato timestamp (pa.timestamp('us)) que si están en datetime (pa.date64()).\n",
    "\n",
    "El problema está en que los datos finales en el proyecto `sarscov` no coinciden si se usa un método o el otro.\n",
    "\n",
    "Probamos a lanzar el código aquí para comprobarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = create_sample_df(n=1000)\n",
    "df_raw.columns = ['person_id', 'start_date', 'end_date', 'type_concept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "df_rare = df_raw.loc[:,['person_id','start_date','end_date','type_concept']]\n",
    "table_raw = pa.Table.from_pandas(df_rare, preserve_index=False)\n",
    "table_raw = table_raw.cast(\n",
    "    pa.schema([\n",
    "        ('person_id', pa.int64()),\n",
    "        ('start_date', pa.timestamp('us')),\n",
    "        ('end_date', pa.timestamp('us')),\n",
    "        ('type_concept', pa.int64()),\n",
    "    ])\n",
    ")\n",
    "df_rare = table_raw.to_pandas()\n",
    "remove_overlap(df_rare,2).info()\n",
    "\n",
    "%timeit -n 10 -r 10 group_dates_original_pyarrow(table_rare,n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "df_rare = df_raw.loc[:,['person_id','start_date','end_date','type_concept']]\n",
    "table_raw = pa.Table.from_pandas(df_rare, preserve_index=False)\n",
    "table_raw = table_raw.cast(\n",
    "    pa.schema([\n",
    "        ('person_id', pa.int64()),\n",
    "        ('start_date', pa.date64()),\n",
    "        ('end_date', pa.date64()),\n",
    "        ('type_concept', pa.int64()),\n",
    "    ])\n",
    ")\n",
    "df_rare = table_raw.to_pandas()\n",
    "remove_overlap(df_rare,2).info()\n",
    "\n",
    "%timeit -n 10 -r 10 group_dates_original_pyarrow(table_rare,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambos formatos funcionan bien, dejando el mismo número de filas.\n",
    "\n",
    "Puede que el problema venga de que algunas fechas en los datos del proyecto vienen con hora. Por ejemplo, todas las de farmacia de dispensación. Si paso estos registros a date64 pierdo la información de la hora, por lo que el orden puede que sea distinto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Nuevo método para calcular type_concept\n",
    "\n",
    "Vamos a comparar el método actual de group_dates con hacer groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from bps_to_omop.general import group_dates, find_person_index\n",
    "\n",
    "def create_sample_data():\n",
    "    nombre_columnas = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "    n_days = 365\n",
    "    df_in = [\n",
    "        # Una única fecha\n",
    "        (1, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        # Dos fechas que se juntan con type_concept iguales\n",
    "        (2, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (2, \"2020-03-01\", \"2020-04-01\", 1),\n",
    "        # Dos fechas que se juntan con type_concept distintos\n",
    "        (3, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (3, \"2020-03-01\", \"2020-04-01\", 2),\n",
    "        # tres fechas que se juntan\n",
    "        (4, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (4, \"2020-03-01\", \"2020-04-01\", 1),\n",
    "        (4, \"2020-05-01\", \"2020-12-01\", 2),\n",
    "        # una persona con dos grupos distintos\n",
    "        (5, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (5, \"2020-03-01\", \"2020-04-01\", 1),\n",
    "        (5, \"2020-05-01\", \"2020-12-01\", 2),\n",
    "        (5, \"2022-01-01\", \"2022-02-01\", 3),\n",
    "        (5, \"2022-03-01\", \"2022-04-01\", 3),\n",
    "        (5, \"2022-05-01\", \"2022-12-01\", 2),\n",
    "    ]\n",
    "    df_in = pd.DataFrame.from_records(df_in, columns=nombre_columnas).assign(\n",
    "        start_date=lambda x: pd.to_datetime(x[\"start_date\"]),\n",
    "        end_date=lambda x: pd.to_datetime(x[\"end_date\"]),\n",
    "    )\n",
    "    return df_in\n",
    "\n",
    "def group_dates_v2(df: pd.DataFrame, n_days: int, verbose: int = 0) -> pd.DataFrame:\n",
    "    # == Preparation ==============================================\n",
    "    if verbose > 0:\n",
    "        print(\"Grouping dates:\")\n",
    "        print(\"- Sorting and preparing data...\")\n",
    "    # Sort so we know for sure the order is right\n",
    "    df_rare = df.copy().sort_values(\n",
    "        [df.columns[0], df.columns[1], df.columns[2]], ascending=[True, True, False]\n",
    "    )\n",
    "    # It is VERY important to reset the index to make sure we can\n",
    "    # retrieve them realiably after sorting them.\n",
    "    df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "    # == Index look-up ============================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Looking up indexes...\")\n",
    "    (idx_person_first, idx_person_last, idx_person_only) = find_person_index(df_rare)\n",
    "    # Create index if the break is too big and needs to be kept\n",
    "    next_interval = df_rare.iloc[:, 1].shift(-1) - df_rare.iloc[:, 2]\n",
    "    idx_interval = next_interval >= pd.Timedelta(n_days, unit=\"D\")\n",
    "\n",
    "    # == Retrieve relevant rows ===================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Retrieving rows...\")\n",
    "    # -- start_date and person_id ---------------------------------\n",
    "    # To retrieve the start_date we need the indexes of:\n",
    "    # - single day periods (idx_person_only == True)\n",
    "    # - first dates (idx_person_first == True)\n",
    "    # - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "    # Get the person condition indexes\n",
    "    idx_start = df_rare.index[\n",
    "        idx_person_only | idx_person_first | idx_interval.shift(1)\n",
    "    ]\n",
    "\n",
    "    # -- end_date -------------------------------------------------\n",
    "    # Get the indexes\n",
    "    idx_end = df_rare.index[idx_person_only | idx_person_last | idx_interval]\n",
    "\n",
    "    # == Compute type_concept =====================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Computing type_concept...\")\n",
    "    # Iterate over idx_start and idx_end to get the periods\n",
    "    mode_values = []\n",
    "    for i in np.arange(len(idx_start)):\n",
    "        df_tmp = df_rare.loc[idx_start[i] : idx_end[i]]\n",
    "        mode_values.append(st.mode(df_tmp.iloc[:, 3].values)[0])\n",
    "\n",
    "        if (verbose > 1) and ((i) % int(len(idx_start) / 4) == 0):\n",
    "            print(f\"  - ({(i+1)/len(idx_start)*100:.1f} %) {(i+1)}/{len(idx_start)}\")\n",
    "    if verbose > 1:\n",
    "        print(f\"  - (100.0 %) {len(idx_start)}/{len(idx_start)}\")\n",
    "\n",
    "    # == Build final dataframe ====================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Closing up...\")\n",
    "    # Create a copy (.loc) with the first two columns\n",
    "    df_done = df_rare.loc[idx_start, [df.columns[0], df.columns[1]]]\n",
    "    # Append values found to final dataframe\n",
    "    df_done[df.columns[2]] = df_rare.loc[idx_end, [df.columns[2]]].values\n",
    "    # Add to dataframe\n",
    "    df_done[df.columns[3]] = mode_values\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"- Done!\")\n",
    "    return df_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>type_concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    person_id start_date   end_date  type_concept\n",
       "0           1 2020-01-01 2020-02-01             1\n",
       "1           2 2020-01-01 2020-02-01             1\n",
       "2           2 2020-03-01 2020-04-01             1\n",
       "3           3 2020-01-01 2020-02-01             1\n",
       "4           3 2020-03-01 2020-04-01             2\n",
       "5           4 2020-01-01 2020-02-01             1\n",
       "6           4 2020-03-01 2020-04-01             1\n",
       "7           4 2020-05-01 2020-12-01             2\n",
       "8           5 2020-01-01 2020-02-01             1\n",
       "9           5 2020-03-01 2020-04-01             1\n",
       "10          5 2020-05-01 2020-12-01             2\n",
       "11          5 2022-01-01 2022-02-01             3\n",
       "12          5 2022-03-01 2022-04-01             3\n",
       "13          5 2022-05-01 2022-12-01             2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbose = 1\n",
    "n_days = 365\n",
    "\n",
    "df = create_sample_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(!!)**\n",
    "\n",
    "La idea aquí es que estamos buscando los índice y construyendo manualmente un dataframe con las fechas iniciales y finales.\n",
    "\n",
    "NO podemos usar el truco del groupby para el type concept directamente, ya que no sabemos los intervalos finales.\n",
    "\n",
    "Es decir, podemos agrupar por person_id, pero habría que agrupar también por las fechas, para poder sacar para cada persona y cada observation_period, cuál es el type_concept más frecuente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping dates:\n",
      "- Sorting and preparing data...\n",
      "- Looking up indexes...\n",
      "- Retrieving rows...\n",
      "- Computing type_concept...\n",
      "- Closing up...\n",
      "- Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>type_concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    person_id start_date   end_date  type_concept\n",
       "0           1 2020-01-01 2020-02-01             1\n",
       "1           2 2020-01-01 2020-04-01             1\n",
       "3           3 2020-01-01 2020-04-01             1\n",
       "5           4 2020-01-01 2020-12-01             1\n",
       "8           5 2020-01-01 2020-12-01             1\n",
       "11          5 2022-01-01 2022-12-01             3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# == Preparation ==============================================\n",
    "if verbose > 0:\n",
    "    print(\"Grouping dates:\")\n",
    "    print(\"- Sorting and preparing data...\")\n",
    "# Sort so we know for sure the order is right\n",
    "df_rare = df.copy().sort_values(\n",
    "    [df.columns[0], df.columns[1], df.columns[2]], ascending=[True, True, False]\n",
    ")\n",
    "# It is VERY important to reset the index to make sure we can\n",
    "# retrieve them realiably after sorting them.\n",
    "df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "# == Index look-up ============================================\n",
    "if verbose > 0:\n",
    "    print(\"- Looking up indexes...\")\n",
    "(idx_person_first, idx_person_last, idx_person_only) = find_person_index(df_rare)\n",
    "# Create index if the break is too big and needs to be kept\n",
    "next_interval = df_rare.iloc[:, 1].shift(-1) - df_rare.iloc[:, 2]\n",
    "idx_interval = next_interval >= pd.Timedelta(n_days, unit=\"D\")\n",
    "\n",
    "# == Retrieve relevant rows ===================================\n",
    "if verbose > 0:\n",
    "    print(\"- Retrieving rows...\")\n",
    "# -- start_date and person_id ---------------------------------\n",
    "# To retrieve the start_date we need the indexes of:\n",
    "# - single day periods (idx_person_only == True)\n",
    "# - first dates (idx_person_first == True)\n",
    "# - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "# Get the person condition indexes\n",
    "idx_start = df_rare.index[\n",
    "    idx_person_only | idx_person_first | idx_interval.shift(1)\n",
    "]\n",
    "\n",
    "# -- end_date -------------------------------------------------\n",
    "# Get the indexes\n",
    "idx_end = df_rare.index[idx_person_only | idx_person_last | idx_interval]\n",
    "\n",
    "# == Compute type_concept =====================================\n",
    "if verbose > 0:\n",
    "    print(\"- Computing type_concept...\")\n",
    "# Iterate over idx_start and idx_end to get the periods\n",
    "mode_values = []\n",
    "for i in np.arange(len(idx_start)):\n",
    "    df_tmp = df_rare.loc[idx_start[i] : idx_end[i]]\n",
    "    mode_values.append(st.mode(df_tmp.iloc[:, 3].values)[0])\n",
    "\n",
    "    if (verbose > 1) and ((i) % int(len(idx_start) / 4) == 0):\n",
    "        print(f\"  - ({(i+1)/len(idx_start)*100:.1f} %) {(i+1)}/{len(idx_start)}\")\n",
    "if verbose > 1:\n",
    "    print(f\"  - (100.0 %) {len(idx_start)}/{len(idx_start)}\")\n",
    "\n",
    "# == Build final dataframe ====================================\n",
    "if verbose > 0:\n",
    "    print(\"- Closing up...\")\n",
    "# Create a copy (.loc) with the first two columns\n",
    "df_done = df_rare.loc[idx_start, [df.columns[0], df.columns[1]]]\n",
    "# Append values found to final dataframe\n",
    "df_done[df.columns[2]] = df_rare.loc[idx_end, [df.columns[2]]].values\n",
    "# # Add to dataframe\n",
    "df_done[df.columns[3]] = mode_values\n",
    "\n",
    "if verbose > 0:\n",
    "    print(\"- Done!\")\n",
    "df_done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Unfinished testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Benchmark Results:\n",
      "================================================================================\n",
      "\n",
      "Configuration: Few Large Groups\n",
      "Data Size: 10,000 rows, 5 groups\n",
      "Masking Time: 0.11ms\n",
      "Searchsorted Time: 0.09ms\n",
      "Winner: searchsorted (1.21x faster)\n",
      "\n",
      "Configuration: Medium Groups\n",
      "Data Size: 10,000 rows, 50 groups\n",
      "Masking Time: 0.85ms\n",
      "Searchsorted Time: 0.11ms\n",
      "Winner: searchsorted (7.82x faster)\n",
      "\n",
      "Configuration: Many Small Groups\n",
      "Data Size: 10,000 rows, 500 groups\n",
      "Masking Time: 8.12ms\n",
      "Searchsorted Time: 0.16ms\n",
      "Winner: searchsorted (51.13x faster)\n",
      "\n",
      "Configuration: Few Large Groups\n",
      "Data Size: 100,000 rows, 5 groups\n",
      "Masking Time: 0.78ms\n",
      "Searchsorted Time: 0.51ms\n",
      "Winner: searchsorted (1.54x faster)\n",
      "\n",
      "Configuration: Medium Groups\n",
      "Data Size: 100,000 rows, 50 groups\n",
      "Masking Time: 7.14ms\n",
      "Searchsorted Time: 1.07ms\n",
      "Winner: searchsorted (6.70x faster)\n",
      "\n",
      "Configuration: Many Small Groups\n",
      "Data Size: 100,000 rows, 500 groups\n",
      "Masking Time: 69.66ms\n",
      "Searchsorted Time: 1.21ms\n",
      "Winner: searchsorted (57.56x faster)\n",
      "\n",
      "Configuration: Few Large Groups\n",
      "Data Size: 1,000,000 rows, 5 groups\n",
      "Masking Time: 10.85ms\n",
      "Searchsorted Time: 7.61ms\n",
      "Winner: searchsorted (1.43x faster)\n",
      "\n",
      "Configuration: Medium Groups\n",
      "Data Size: 1,000,000 rows, 50 groups\n",
      "Masking Time: 101.45ms\n",
      "Searchsorted Time: 11.29ms\n",
      "Winner: searchsorted (8.98x faster)\n",
      "\n",
      "Configuration: Many Small Groups\n",
      "Data Size: 1,000,000 rows, 500 groups\n",
      "Masking Time: 819.04ms\n",
      "Searchsorted Time: 14.23ms\n",
      "Winner: searchsorted (57.57x faster)\n",
      "\n",
      "Summary Statistics:\n",
      "================================================================================\n",
      "\n",
      "Few Large Groups:\n",
      "Average speedup using searchsorted: 1.39x\n",
      "\n",
      "Medium Groups:\n",
      "Average speedup using searchsorted: 7.83x\n",
      "\n",
      "Many Small Groups:\n",
      "Average speedup using searchsorted: 55.42x\n",
      "\n",
      "Validating correctness of implementations...\n",
      "⚠ WARNING: Implementations produce different results!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "def assign_groups_masking(indices: np.ndarray, starts: np.ndarray, ends: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Group assignment using boolean masking\"\"\"\n",
    "    group_ids = np.zeros(len(indices), dtype=int)\n",
    "    for group_num, (start, end) in enumerate(zip(starts, ends), 1):\n",
    "        mask = (indices >= start) & (indices <= end)\n",
    "        group_ids[mask] = group_num\n",
    "    return group_ids\n",
    "\n",
    "def assign_groups_searchsorted(indices: np.ndarray, starts: np.ndarray, ends: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Group assignment using searchsorted\"\"\"\n",
    "    boundaries = np.sort(np.concatenate([starts, ends + 1]))\n",
    "    return np.searchsorted(boundaries, indices, side='right') // 2\n",
    "\n",
    "def generate_test_case(n_rows: int, n_groups: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate test data with given size and number of groups\"\"\"\n",
    "    # Create roughly equal-sized groups\n",
    "    group_size = n_rows // n_groups\n",
    "    starts = np.arange(0, n_rows, group_size)\n",
    "    ends = starts + group_size - 1\n",
    "    ends[-1] = n_rows - 1  # Adjust last group\n",
    "    return starts, ends\n",
    "\n",
    "def run_benchmark():\n",
    "    # Test configurations\n",
    "    row_sizes = [10_000, 100_000, 1_000_000]\n",
    "    group_configs = [\n",
    "        ('Few Large Groups', lambda x: max(5, x // 1_000_000)),\n",
    "        ('Medium Groups', lambda x: max(50, x // 100_000)),\n",
    "        ('Many Small Groups', lambda x: max(500, x // 10_000))\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for n_rows in row_sizes:\n",
    "        indices = np.arange(n_rows)\n",
    "        \n",
    "        for group_desc, group_func in group_configs:\n",
    "            n_groups = group_func(n_rows)\n",
    "            starts, ends = generate_test_case(n_rows, n_groups)\n",
    "            \n",
    "            # Warm-up run\n",
    "            _ = assign_groups_masking(indices, starts, ends)\n",
    "            _ = assign_groups_searchsorted(indices, starts, ends)\n",
    "            \n",
    "            # Timing masking approach\n",
    "            start_time = time.perf_counter()\n",
    "            for _ in range(5):  # Multiple runs for more stable results\n",
    "                _ = assign_groups_masking(indices, starts, ends)\n",
    "            masking_time = (time.perf_counter() - start_time) / 5\n",
    "            \n",
    "            # Timing searchsorted approach\n",
    "            start_time = time.perf_counter()\n",
    "            for _ in range(5):\n",
    "                _ = assign_groups_searchsorted(indices, starts, ends)\n",
    "            searchsorted_time = (time.perf_counter() - start_time) / 5\n",
    "            \n",
    "            results.append({\n",
    "                'Rows': n_rows,\n",
    "                'Groups': n_groups,\n",
    "                'Configuration': group_desc,\n",
    "                'Masking Time': masking_time,\n",
    "                'Searchsorted Time': searchsorted_time\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmark\n",
    "results_df = run_benchmark()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nDetailed Benchmark Results:\")\n",
    "print(\"=\" * 80)\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"\\nConfiguration: {row['Configuration']}\")\n",
    "    print(f\"Data Size: {row['Rows']:,} rows, {row['Groups']:,} groups\")\n",
    "    print(f\"Masking Time: {row['Masking Time']*1000:.2f}ms\")\n",
    "    print(f\"Searchsorted Time: {row['Searchsorted Time']*1000:.2f}ms\")\n",
    "    speedup = row['Masking Time'] / row['Searchsorted Time']\n",
    "    faster_method = \"searchsorted\" if speedup > 1 else \"masking\"\n",
    "    print(f\"Winner: {faster_method} ({abs(speedup):,.2f}x {'faster' if speedup > 1 else 'slower'})\")\n",
    "\n",
    "# Calculate and print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "for config in results_df['Configuration'].unique():\n",
    "    config_results = results_df[results_df['Configuration'] == config]\n",
    "    print(f\"\\n{config}:\")\n",
    "    avg_speedup = (config_results['Masking Time'] / config_results['Searchsorted Time']).mean()\n",
    "    print(f\"Average speedup using searchsorted: {avg_speedup:.2f}x\")\n",
    "\n",
    "# Validation of correctness\n",
    "print(\"\\nValidating correctness of implementations...\")\n",
    "test_indices = np.arange(1000)\n",
    "test_starts = np.array([0, 200, 400, 600, 800])\n",
    "test_ends = np.array([199, 399, 599, 799, 999])\n",
    "\n",
    "masking_results = assign_groups_masking(test_indices, test_starts, test_ends)\n",
    "searchsorted_results = assign_groups_searchsorted(test_indices, test_starts, test_ends)\n",
    "\n",
    "if np.array_equal(masking_results, searchsorted_results):\n",
    "    print(\"✓ Both implementations produce identical results\")\n",
    "else:\n",
    "    print(\"⚠ WARNING: Implementations produce different results!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
