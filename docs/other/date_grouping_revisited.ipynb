{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date grouping revisited\n",
    "\n",
    "This notebook describes how the date grouping works in bps_to_omop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Remove overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General rationale\n",
    "\n",
    "Imagine this as a visit in a timeline\n",
    "\n",
    "```python\n",
    "-(- - -)- - - - - - - -\n",
    " |     |\n",
    " |     |-> End of the visit\n",
    " |\n",
    " |-> Start of the visit\n",
    "```\n",
    "We want to find the **main visits**:\n",
    "\n",
    "- A main visit contains other visits. \n",
    "- Main visit can be next to each other, ie: The end of the 1st can be the start of the 2nd\n",
    "- Main visits can not overlap each other, ie: The end of the 1st can not be after the start of the 2nd\n",
    "- Only main visits can populate the VISIT_OCCURRENCE table\n",
    "\n",
    "If we sort the visits by person_id (asc), start_date (asc), end_date (desc) y type_concet (asc), we will have something like this for each person:\n",
    "```python\n",
    "-(- - -)- - - - - - - - \n",
    "-(- -)- - - - - - - - -\n",
    "-(-)- - - - - - - - - -\n",
    "- -(- - -)- - - - - - -\n",
    "- -(- -)- - - - - - - -\n",
    "- -(-)- - - - - - - - - \n",
    "- - -(- - -)- - - - - -\n",
    "- - -(- -)- - - - - - -\n",
    "- - -(-)- - - - - - - -\n",
    "- - - - - -(- - -)- - - \n",
    "- - - - - -(- -)- - - -\n",
    "- - - - - -(-)- - - - -\n",
    "```\n",
    "\n",
    "If we compare each visit with the **FIRST ONE**, there are different cases here:\n",
    "1. **COMPLETELY CONTAINED VISITS**: Contained visits are completely contained in the first visit we are considering (This includes consecutive single day visits)\n",
    "2. **PARTIALLY CONTAINED VISITS**:The start of the visit happens after the start of the 1st, but end of the happens after the end of the 1st\n",
    "    - Starts afterwards, but extends further into the future.\n",
    "3. **NOT CONTAINED VISITS**: The start of the visit is after the 1st. It is a \"new\" main visit.\n",
    "\n",
    "```python\n",
    "-(- - -)- - - - - - - - # This is a main visit\n",
    "-(- -)- - - - - - - - - 1. contained\n",
    "-(-)- - - - - - - - - - 1. contained\n",
    "- -(- - -)- - - - - - - 2. partial\n",
    "- -(- -)- - - - - - - - 1. contained\n",
    "- -(-)- - - - - - - - - 1. contained\n",
    "- - -(- - -)- - - - - - 2. partial\n",
    "- - -(- -)- - - - - - - 2. partial\n",
    "- - -(-)- - - - - - - - 1. contained\n",
    "- - - - - -(- - -)- - - # This is the next main visit\n",
    "- - - - - -(- -)- - - - 1. contained\n",
    "- - - - - -(-)- - - - - 1. contained\n",
    "```\n",
    "\n",
    "Completely contained visits are the easy ones. We can link them to the main visit and remove them.\n",
    "\n",
    "Partially contained visits are problematic. These will force us to extend our initial visit further into the future, essentially creating a new record.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode explanation\n",
    "\n",
    "First we join together all files to be used. This essentialy is the VISIT_DETAIL table.\n",
    "\n",
    "With this initial table we do the following:\n",
    "\n",
    "1. Build the frame for the VISIT_DETAIL table (`build_visit_detail()`).\n",
    "   1. Sort by `person_id`(asc), `start_date` (asc), `end_date` (desc) and `type_concept` (asc).\n",
    "      * This way, we make sure that, for each `start_date`, the first column will have the latest `end_date`. This latest `end_date` is the visit that might contain other visits with the same `start_date`.\n",
    "      * The `type_concept` column should be transformed first to a category datatype with a predefined order. This way the sorting will take that into account as well.\n",
    "   2. Generate the field `visit_detail_id` with every visit.\n",
    "   3. Rename columns to fit the VISIT_DETAIL structure.\n",
    "   \n",
    "With this, we have the VISIT_DETAIL table. From here we can build VISIT_OCCURRENCE on top of VISIT_DETAIL.\n",
    "\n",
    "2. Extend the VISIT_DETAIL table to prepare for VISIT_OCCURRENCE generation (`build_visit_detail_extended()`):\n",
    "   1. Generate placeholders for future VISIT_OCCURRENCE columns and for temporary columns.\n",
    "      1. Group by each patient, generating:\n",
    "        - **visit_start_datetime**: Initially the first **visit_detail_start_datetime**. It will be the **visit_start_datetime** VISIT_OCCCURRENCE column.\n",
    "        - **visit_end_datetime**: Initially the first **visit_detail_end_datetime**. It will be the **visit_start_datetime** VISIT_OCCCURRENCE column. \n",
    "          - If visits needs to be joined together, this record will have the latest joined visit.\n",
    "        - **visit_detail_id_original**: Initially the first **visit_detail_id**. It will be the **visit_detail_id** VISIT_OCCCURRENCE column.\n",
    "        - **main_visit**: Initially will have a constant *\"Unknown\"* value for every record. It will be updated to identify main visits (Changed to \"Yes\") and the rest (Changed to \"No\").\n",
    "      2. Join with VISIT_DETAIL to get the first row (earliest start_date, latest end_date) for each patient.\n",
    "   2. Generate the *main_visit* column.\n",
    "      - This field has 3 possibilities: \n",
    "         - *Unknown:* We do not know if it is **main_visit** yet.\n",
    "         - *Yes:* It was checked and visit is a **main_visit**.\n",
    "         - *No:* It was checked and visit is NOT a **main_visit**.\n",
    "       - We create this columns as a category. Initially all records are *Unknown*. \n",
    "   3. Generate the auxiliary columns:\n",
    "      - `is_contained`, to mark completely contained records.\n",
    "      - `is_partial`, to mark partialy contained records.\n",
    "      - `is_not_contained`, to marked not contained records, which could be main visits.\n",
    "      - `parent_visit_detail_id`, to record the associated **main_visit** when it is found.\n",
    "\n",
    "The previous operations only need to be done once at the start.\n",
    "\n",
    "Afterwards, we will need to do the following operations in a loop until no *Unknown* visits are left:\n",
    "\n",
    "3. Identify the next batch of **main_visit** (`identify_next_main_visits()`):\n",
    "   1. They are the most recent that verify:\n",
    "      1. They are *\"Unknown\"*\n",
    "      2. **visit_detail_id** == **visit_detail_id_original**\n",
    "4. Check the if the other visits are:\n",
    "   1. Completely contained (`identify_contained_rows()`):\n",
    "      - **main_visit** -> *\"No\"*. \n",
    "      - Assign the value of **visit_detail_id** to the column **parent_visit_detail** (`update_contained_rows()`).\n",
    "   2. Partially contained (`identify_partial_rows()`):\n",
    "      - **main_visit** -> *\"No\"*. \n",
    "      - Assign the value of the latest **visit_detail_end_datetime** to the column **visit_end_datetime**. **(!)** Only for partially contained (`update_partial_rows()`).\n",
    "   3. Not contained (`identify_not_contained_rows()`):\n",
    "      - These records have to be reanalyzed. They are **main_visit** candidates.\n",
    "      1. For not_contained records, we group by patient and extract columns **visit_detail_start_datetime**, **visit_detail_end_datetime** and **visit_detail_id** from the earliest *Unknown* record.\n",
    "      2. Join with previous table, regenerating columns **visit_start_datetime**, **visit_end_datetime** and **visit_detail_id_original**.\n",
    "5. Go back to step 3. Repeat until there are no more **main_visit** == *\"Unknown\"* columns or a safe iteration threshold is surpassed.\n",
    "6. Assign a unique visit_occurrence_id to main visits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed step by step test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test dataset creation\n",
    "\n",
    "We are going to create a dataset with all the issues we can find.\n",
    "\n",
    "- Posterior visits completely contained in a previous visit.\n",
    "  * (2020-01-01, 2020-02-01) contains (2020-01-02, 2020-02-02) and (2020-01-04, 2020-02-04) \n",
    "    - We want to keep only the first.\n",
    "- Posterior dates that are partially contained in a previous visit.\n",
    "  * (2020-03-01, 2020-04-01) partially contains (2020-03-15, 2020-04-15) \n",
    "    - Here we want to combine both => (2020-03-01, 2020-04-15). \n",
    "    - \"Multi-day visits must not overlap, i.e. share days other than start and end days\". See [here](https://ohdsi.github.io/CommonDataModel/cdm54.html#visit_occurrence).\n",
    "- Another issue is when we have several visits partially contained.\n",
    "  - When we repeat the process, we need to make sure that previous main visits keep the end_dates\n",
    "  * (2020-06-01, 2020-07-01) partially contains (2020-06-10, 2020-07-10) and (2020-06-20, 2020-07-20)\n",
    "- Make sure we do not mix visits from different patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_names = [\n",
    "    \"person_id\",\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "    \"type_concept\",\n",
    "    \"should_remain\",\n",
    "    \"visit_concept_id\",\n",
    "    \"provider_id\",\n",
    "]\n",
    "filas = [\n",
    "    # == Problema de fechas ==\n",
    "    # We want two main visits, so we know we can distinguish between them\n",
    "    # Each main visit will have a completely contained visit and a partially contained visit\n",
    "    # Each main will have\n",
    "    # -- Main visit 1 --\n",
    "    (1, \"2020-01-01\", \"2020-02-01\", 1, True, 9202, 0),\n",
    "    # Completely contained, same type_concept\n",
    "    (1, \"2020-01-02\", \"2020-01-02\", 1, False, 9202, 0),\n",
    "    # Completely contained, different type_concept\n",
    "    (1, \"2020-01-04\", \"2020-01-04\", 2, False, 9202, 0),\n",
    "    # Partially contained, same type_concept\n",
    "    (1, \"2020-01-06\", \"2020-02-06\", 1, False, 9202, 0),\n",
    "    # Partially contained, different type_concept\n",
    "    (1, \"2020-01-08\", \"2020-02-08\", 2, False, 9202, 0),\n",
    "    #\n",
    "    # -- Main visit 2 --\n",
    "    (1, \"2020-03-01\", \"2020-04-01\", 1, True, 9202, 0),\n",
    "    # Completely contained, same type_concept\n",
    "    (1, \"2020-03-02\", \"2020-03-02\", 1, False, 9202, 0),\n",
    "    # Completely contained, different type_concept\n",
    "    (1, \"2020-03-04\", \"2020-03-04\", 2, False, 9202, 0),\n",
    "    # Partially contained, same type_concept\n",
    "    (1, \"2020-03-06\", \"2020-04-06\", 1, False, 9202, 0),\n",
    "    # Partially contained, different type_concept\n",
    "    (1, \"2020-03-08\", \"2020-04-08\", 2, False, 9202, 0),\n",
    "    #\n",
    "    # == Problema de person_id ==\n",
    "    # Tres personas distintas\n",
    "    (2, \"2021-01-01\", \"2021-01-01\", 1, True, 9202, 0),\n",
    "    (2, \"2021-02-01\", \"2021-02-01\", 1, True, 9202, 0),\n",
    "    # Comparte fecha con 1\n",
    "    (3, \"2021-02-01\", \"2021-02-01\", 2, True, 9202, 0),\n",
    "    # No omparte fecha con 1\n",
    "    (3, \"2021-03-01\", \"2021-03-01\", 2, True, 9202, 0),\n",
    "    #\n",
    "    # == Problema de type_concept ==\n",
    "    (4, \"2022-03-01\", \"2022-04-01\", 1, True, 9202, 0),\n",
    "    # Misma persona y fecha, sólo debe quedar type_concept == 1\n",
    "    (4, \"2022-03-01\", \"2022-04-01\", 2, False, 9202, 0),\n",
    "]\n",
    "df_raw = pd.DataFrame.from_records(filas, columns=column_names)\n",
    "df_raw[\"start_date\"] = pd.to_datetime(df_raw[\"start_date\"])\n",
    "df_raw[\"end_date\"] = pd.to_datetime(df_raw[\"end_date\"])\n",
    "(print(df_raw.info()))\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "pl.Config(\n",
    "    tbl_formatting=\"MARKDOWN\",\n",
    "    tbl_rows=20,\n",
    "    set_tbl_width_chars=400,\n",
    "    set_tbl_cols=-1,\n",
    ")\n",
    "\n",
    "df_raw_pl = pl.DataFrame(df_raw)\n",
    "print(df_raw_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build the frame for the VISIT_DETAIL table (`build_visit_detail()`).\n",
    "   1. Sort by `person_id`(asc), `start_date` (asc), `end_date` (desc) and `type_concept` (asc).\n",
    "      * This way, we make sure that, for each `start_date`, the first column will have the latest `end_date`. This latest `end_date` is the visit that might contain other visits with the same `start_date`.\n",
    "      * The `type_concept` column should be transformed first to a category datatype with a predefined order. This way the sorting will take that into account as well.\n",
    "   2. Generate the field `visit_detail_id` with every visit.\n",
    "   3. Rename columns to fit the VISIT_DETAIL structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_visit_detail(df):\n",
    "    return (\n",
    "        # First we do the sorting\n",
    "        df.sort(\n",
    "            [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"],\n",
    "            descending=[False, False, True, False],\n",
    "        )\n",
    "        # Assign the visit_detail_id\n",
    "        .with_columns(visit_detail_id=pl.int_range(pl.len()))\n",
    "        # Rename columns\n",
    "        .rename(\n",
    "            {\n",
    "                \"start_date\": \"visit_detail_start_datetime\",\n",
    "                \"end_date\": \"visit_detail_end_datetime\",\n",
    "                \"type_concept\": \"visit_detail_type_concept_id\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "visit_detail = build_visit_detail(df_raw_pl)\n",
    "print(visit_detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extend the VISIT_DETAIL table to prepare for VISIT_OCCURRENCE generation (`build_visit_detail_extended()`):\n",
    "   1. Generate placeholders for future VISIT_OCCURRENCE columns and for temporary columns.\n",
    "      1. Group by each patient, generating:\n",
    "        - **visit_start_datetime**: Initially the first **visit_detail_start_datetime**. It will be the **visit_start_datetime** VISIT_OCCCURRENCE column.\n",
    "        - **visit_end_datetime**: Initially the first **visit_detail_end_datetime**. It will be the **visit_start_datetime** VISIT_OCCCURRENCE column. \n",
    "          - If visits needs to be joined together, this record will have the latest joined visit.\n",
    "        - **visit_detail_id_original**: Initially the first **visit_detail_id**. It will be the **visit_detail_id** VISIT_OCCCURRENCE column.\n",
    "        - **main_visit**: Initially will have a constant *\"Unknown\"* value for every record. It will be updated to identify main visits (Changed to \"Yes\") and the rest (Changed to \"No\").\n",
    "      2. Join with VISIT_DETAIL to get the first row (earliest start_date, latest end_date) for each patient.\n",
    "   2. Generate the *main_visit* column.\n",
    "      - This field has 3 possibilities: \n",
    "         - *Unknown:* We do not know if it is **main_visit** yet.\n",
    "         - *Yes:* It was checked and visit is a **main_visit**.\n",
    "         - *No:* It was checked and visit is NOT a **main_visit**.\n",
    "       - We create this columns as a category. Initially all records are *Unknown*. \n",
    "   3. Generate the auxiliary columns:\n",
    "      - `is_contained`, to mark completely contained records.\n",
    "      - `is_partial`, to mark partialy contained records.\n",
    "      - `is_not_contained`, to marked not contained records, which could be main visits.\n",
    "      - `parent_visit_detail_id`, to record the associated **main_visit** when it is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_visit_detail_extended(visit_detail):\n",
    "    # Get the first date of every person\n",
    "    visit_occurrence_dates = visit_detail.group_by(\n",
    "        \"person_id\", maintain_order=True\n",
    "    ).agg(\n",
    "        [\n",
    "            pl.col(\"visit_detail_start_datetime\").first().alias(\"visit_start_datetime\"),\n",
    "            pl.col(\"visit_detail_end_datetime\").first().alias(\"visit_end_datetime\"),\n",
    "            pl.col(\"visit_detail_id\").first().alias(\"visit_detail_id_original\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Join and return\n",
    "    return visit_detail.join(\n",
    "        visit_occurrence_dates, on=\"person_id\", how=\"left\"\n",
    "    ).with_columns(\n",
    "        main_visit=pl.lit(\"Unknown\").cast(pl.Enum([\"Yes\", \"No\", \"Unknown\"])),\n",
    "        is_contained=pl.lit(False),\n",
    "        is_partial=pl.lit(False),\n",
    "        not_contained=pl.lit(False),\n",
    "        parent_visit_detail_id=pl.lit(None),\n",
    "    )\n",
    "\n",
    "\n",
    "df = build_visit_detail_extended(visit_detail)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Identify the next batch of **main_visit** (`identify_next_main_visits()`):\n",
    "   1. They are the most recent that verify:\n",
    "      1. They are *\"Unknown\"*\n",
    "      2. **visit_detail_id** == **visit_detail_id_original**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify main_visits\n",
    "def identify_next_main_visits(df):\n",
    "    return df.with_columns(\n",
    "        main_visit=(\n",
    "            pl.when(\n",
    "                (pl.col(\"visit_detail_id_original\") == pl.col(\"visit_detail_id\"))\n",
    "                & (pl.col(\"main_visit\") == \"Unknown\")\n",
    "            )\n",
    "            .then(pl.lit(\"Yes\"))\n",
    "            .otherwise(pl.col(\"main_visit\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "df = identify_next_main_visits(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Check the if the other visits are:\n",
    "\n",
    "   1. Completely contained (`identify_contained_rows()`):\n",
    "      - **main_visit** -> *\"No\"*. \n",
    "      - Assign the value of **visit_detail_id** to the column **parent_visit_detail** (`update_contained_rows()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_contained_rows(df):\n",
    "    return df.with_columns(\n",
    "        is_contained=pl.when(\n",
    "            (pl.col(\"main_visit\") == \"Unknown\")\n",
    "            & (pl.col(\"visit_start_datetime\") <= pl.col(\"visit_detail_start_datetime\"))\n",
    "            & (pl.col(\"visit_end_datetime\") >= pl.col(\"visit_detail_end_datetime\"))\n",
    "        )\n",
    "        .then(True)\n",
    "        .otherwise(pl.col(\"is_contained\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def update_contained_rows(df):\n",
    "\n",
    "    return df.with_columns(\n",
    "        # Update main_visit to mark contained visits\n",
    "        main_visit=(\n",
    "            pl.when((pl.col(\"is_contained\") == True))\n",
    "            .then(pl.lit(\"No\"))\n",
    "            .otherwise(pl.col(\"main_visit\"))\n",
    "        ),\n",
    "        # Build the parent_visit_detail_id, since we are here\n",
    "        parent_visit_detail_id=(\n",
    "            pl.when(\n",
    "                (pl.col(\"is_contained\") == True)\n",
    "                & (pl.col(\"visit_detail_id\") != pl.col(\"visit_detail_id_original\"))\n",
    "            )\n",
    "            .then(pl.col(\"visit_detail_id_original\"))\n",
    "            .otherwise(pl.col(\"parent_visit_detail_id\"))\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "df = identify_contained_rows(df)\n",
    "df = update_contained_rows(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Check the if the other visits are:\n",
    "   \n",
    "   2. Partially contained (`identify_partial_rows()`):\n",
    "      - **main_visit** -> *\"No\"*. \n",
    "      - Assign the value of the latest **visit_detail_end_datetime** to the column **visit_end_datetime**. **(!)** Only for partially contained (`update_partial_rows()`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_partial_rows(df):\n",
    "    return df.with_columns(\n",
    "        is_partial=pl.when(\n",
    "            (pl.col(\"main_visit\") == \"Unknown\")\n",
    "            & (pl.col(\"visit_detail_start_datetime\") <= pl.col(\"visit_end_datetime\"))\n",
    "            & (pl.col(\"visit_detail_end_datetime\") > pl.col(\"visit_end_datetime\"))\n",
    "        )\n",
    "        .then(True)\n",
    "        .otherwise(pl.col(\"is_partial\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def update_partial_rows(df):\n",
    "\n",
    "    latest_date = (\n",
    "        df.filter(pl.col(\"is_partial\") == True)\n",
    "        .group_by(\"person_id\", maintain_order=True)\n",
    "        .agg(pl.col(\"visit_detail_end_datetime\").max().alias(\"latest_end_datetime\"))\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        # Join back to the main dataframe and update visit_end_datetime\n",
    "        df.join(\n",
    "            latest_date,\n",
    "            on=\"person_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .with_columns(\n",
    "            main_visit=(\n",
    "                pl.when((pl.col(\"is_partial\") == True))\n",
    "                .then(pl.lit(\"No\"))\n",
    "                .otherwise(pl.col(\"main_visit\"))\n",
    "            ),\n",
    "            visit_end_datetime=pl.when(pl.col(\"main_visit\") == \"Yes\")\n",
    "            .then(\n",
    "                pl.coalesce(\n",
    "                    [pl.col(\"latest_end_datetime\"), pl.col(\"visit_detail_end_datetime\")]\n",
    "                )\n",
    "            )\n",
    "            .otherwise(pl.col(\"visit_end_datetime\")),\n",
    "        )\n",
    "        .drop(\"latest_end_datetime\")\n",
    "    )\n",
    "\n",
    "\n",
    "df = identify_partial_rows(df)\n",
    "df = update_partial_rows(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Check the if the other visits are:\n",
    "\n",
    "   3. Not contained (`identify_not_contained_rows()`):\n",
    "      - These records have to be reanalyzed. They are **main_visit** candidates.\n",
    "\n",
    "      1. For not_contained records, we group by patient and extract columns **visit_detail_start_datetime**, **visit_detail_end_datetime** and **visit_detail_id** from the earliest (first) *Unknown* record.\n",
    "      2. Join with previous table, regenerating columns **visit_start_datetime**, **visit_end_datetime** and **visit_detail_id_original**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_not_contained_rows(df):\n",
    "    return df.with_columns(\n",
    "        not_contained=pl.when(\n",
    "            (pl.col(\"main_visit\") == \"Unknown\")\n",
    "            & (pl.col(\"visit_detail_start_datetime\") >= pl.col(\"visit_end_datetime\"))\n",
    "        )\n",
    "        .then(True)\n",
    "        .otherwise(pl.col(\"not_contained\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def update_not_contained_rows(df):\n",
    "\n",
    "    newest_not_contained = (\n",
    "        df.filter(pl.col(\"not_contained\") == True)\n",
    "        .group_by(\"person_id\", maintain_order=True)\n",
    "        .agg(\n",
    "            pl.col(\"visit_detail_id\").first().alias(\"visit_detail_id_newest\"),\n",
    "            pl.col(\"visit_detail_start_datetime\")\n",
    "            .first()\n",
    "            .alias(\"visit_detail_start_datetime_newest\"),\n",
    "            pl.col(\"visit_detail_end_datetime\")\n",
    "            .first()\n",
    "            .alias(\"visit_detail_end_datetime_newest\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        # Join back to the main dataframe and update visit_end_datetime\n",
    "        df.join(\n",
    "            newest_not_contained,\n",
    "            on=\"person_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        # Update the values on visit_detail_id_original, visit_start_datetime and visit_end_datetime\n",
    "        .with_columns(\n",
    "            visit_detail_id_original=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "            .then(pl.col(\"visit_detail_id_newest\"))\n",
    "            .otherwise(pl.col(\"visit_detail_id_original\")),\n",
    "            visit_start_datetime=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "            .then(pl.col(\"visit_detail_start_datetime_newest\"))\n",
    "            .otherwise(pl.col(\"visit_start_datetime\")),\n",
    "            visit_end_datetime=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "            .then(pl.col(\"visit_detail_end_datetime_newest\"))\n",
    "            .otherwise(pl.col(\"visit_end_datetime\")),\n",
    "        ).drop(\n",
    "            pl.col(\n",
    "                \"visit_detail_id_newest\",\n",
    "                \"visit_detail_start_datetime_newest\",\n",
    "                \"visit_detail_end_datetime_newest\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "df = identify_not_contained_rows(df)\n",
    "df = update_not_contained_rows(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Go back to step 3. Repeat until there are no more **main_visit** == *\"Unknown\"* columns or a safe iteration threshold is surpassed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify main_visits\n",
    "df = identify_next_main_visits(df)\n",
    "# Update completely contained rows\n",
    "df = identify_contained_rows(df)\n",
    "df = update_contained_rows(df)\n",
    "# Update partially contained rows\n",
    "df = update_partial_rows(df)\n",
    "df = identify_partial_rows(df)\n",
    "# Update not contained rows\n",
    "df = update_not_contained_rows(df)\n",
    "df = identify_not_contained_rows(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Assign a unique visit_occurrence_id to main visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_visit_occurrence_id(visit_occurrence):\n",
    "\n",
    "    return (\n",
    "        # Create a helper column to track main visit sequence\n",
    "        visit_occurrence.with_columns(is_main_visit=(pl.col(\"main_visit\") == \"Yes\"))\n",
    "        # Assign a unique identifier only to main visits using row_number and clean the helper\n",
    "        .with_columns(\n",
    "            visit_occurrence_id=pl.when(pl.col(\"is_main_visit\"))\n",
    "            .then(pl.col(\"is_main_visit\").cast(pl.Int32).cum_sum() - 1)\n",
    "            .otherwise(None)\n",
    "        ).drop(\"is_main_visit\")\n",
    "        # Fill the rest using forward fill (ffill)\n",
    "        .with_columns(visit_occurrence_id=pl.col(\"visit_occurrence_id\").forward_fill())\n",
    "    )\n",
    "\n",
    "\n",
    "visit_occurrence = assign_visit_occurrence_id(df)\n",
    "visit_occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build the VISIT_DETAIL and VISIT_OCCURRENCE table by removing all the helper columns we have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_visit_occurrence(df, verbose=0, n_iter_max=1000):\n",
    "    # -- Initialization --\n",
    "    df = df.lazy()\n",
    "    # Get the core of the visit_detail table\n",
    "    df = build_visit_detail(df)\n",
    "    # Extend the table for processing\n",
    "    df = build_visit_detail_extended(df)\n",
    "    # Initialize counters for the while loop\n",
    "    n_unknown = (\n",
    "        df.filter(pl.col(\"main_visit\") == \"Unknown\").select(pl.len()).collect().item()\n",
    "    )\n",
    "    n_iter = 0\n",
    "    df = df.collect()\n",
    "\n",
    "    # -- Loop --\n",
    "    while n_unknown > 0 and n_iter < n_iter_max:\n",
    "        if verbose > 0:\n",
    "            print(f\"Iter {n_iter:>2}: {n_unknown} unknown rows left.\")\n",
    "        if verbose > 1:\n",
    "            if isinstance(df, pl.LazyFrame):\n",
    "                print(df.filter(pl.col(\"main_visit\") == \"Unknown\").head(5).collect())\n",
    "            else:\n",
    "                print(df.filter(pl.col(\"main_visit\") == \"Unknown\").head(5))\n",
    "\n",
    "        # Look for next batch of main_visits\n",
    "        df = identify_next_main_visits(df)\n",
    "\n",
    "        # Identify and update completely contained visits\n",
    "        df = identify_contained_rows(df)\n",
    "        df = update_contained_rows(df)\n",
    "\n",
    "        # Identify and update partially contained visits\n",
    "        df = identify_partial_rows(df)\n",
    "        df = update_partial_rows(df)\n",
    "\n",
    "        # Identify and update not contained visits\n",
    "        df = identify_not_contained_rows(df)\n",
    "        df = update_not_contained_rows(df)\n",
    "\n",
    "        # Update conditions\n",
    "        if isinstance(df, pl.LazyFrame):\n",
    "            n_unknown = (\n",
    "                df.filter(pl.col(\"main_visit\") == \"Unknown\")\n",
    "                .select(pl.len())\n",
    "                .collect()\n",
    "                .item()\n",
    "            )\n",
    "        else:\n",
    "            n_unknown = (\n",
    "                df.filter(pl.col(\"main_visit\") == \"Unknown\").select(pl.len()).item()\n",
    "            )\n",
    "        n_iter += 1\n",
    "\n",
    "    # Assign an unique visit_occurrence_id only to main_visits\n",
    "    df = assign_visit_occurrence_id(df)\n",
    "\n",
    "    # Drop the extra helper columns\n",
    "    df = df.drop(\n",
    "        # Drop helpers\n",
    "        pl.col(\"visit_detail_id_original\"),\n",
    "        pl.col(\"is_contained\"),\n",
    "        pl.col(\"is_partial\"),\n",
    "        pl.col(\"not_contained\"),\n",
    "    )\n",
    "\n",
    "    # -- Build the core of the visit_detal table --\n",
    "    visit_detail = df\n",
    "\n",
    "    # Drop visit_occurrence columns\n",
    "    visit_detail = visit_detail.drop(\n",
    "        pl.col(\"visit_start_datetime\"),\n",
    "        pl.col(\"visit_end_datetime\"),\n",
    "        pl.col(\n",
    "            \"main_visit\"\n",
    "        ),  # This one is dropped here so it can be used for visit_occurrence\n",
    "    )\n",
    "\n",
    "    # -- Build the core of the visit_occurrence table --\n",
    "    visit_occurrence = df\n",
    "\n",
    "    # Get only main visits\n",
    "    visit_occurrence = df.filter(pl.col(\"main_visit\") == \"Yes\").drop(\n",
    "        pl.col(\"main_visit\")\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    visit_occurrence = visit_occurrence.rename(\n",
    "        {\n",
    "            \"visit_detail_type_concept_id\": \"visit_type_concept_id\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Drop columns from visit_detail\n",
    "    visit_occurrence = visit_occurrence.drop(\n",
    "        pl.col(\"visit_detail_start_datetime\"),\n",
    "        pl.col(\"visit_detail_end_datetime\"),\n",
    "        pl.col(\"visit_detail_id\"),\n",
    "        pl.col(\"parent_visit_detail_id\"),\n",
    "    )\n",
    "\n",
    "    # n_unknown = df.filter(pl.col(\"main_visit\") == \"Unknown\").select(pl.len()).collect().item() # Lazy\n",
    "    n_unknown = (\n",
    "        df.filter(pl.col(\"main_visit\") == \"Unknown\").select(pl.len()).item()\n",
    "    )  # Not Lazy\n",
    "\n",
    "    # return visit_detail.collect(), visit_occurrence.collect()\n",
    "    return visit_detail, visit_occurrence\n",
    "\n",
    "\n",
    "visit_detail, visit_occurrence = build_visit_occurrence(df_raw_pl, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that visit_detail has the correct visit_occurrence_id asssignation\n",
    "assert (\n",
    "    np.all(\n",
    "        visit_detail.select(\"visit_occurrence_id\")\n",
    "        == pl.Series([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 3, 4, 5, 6, 6])\n",
    "    )\n",
    "    == True\n",
    ")\n",
    "# Check that visit_occurrence has the correct shape\n",
    "assert visit_occurrence.shape[0] == 7\n",
    "# Check that visit_occurrence has the correct people\n",
    "assert (\n",
    "    np.all(visit_occurrence.select(\"person_id\") == pl.Series([1, 1, 2, 2, 3, 3, 4]))\n",
    "    == True\n",
    ")\n",
    "# Check that visit_occurrence has the same start dates\n",
    "assert (pl.Series(\n",
    "    [\n",
    "        \"2020-01-01 00:00:00\",\n",
    "        \"2020-03-01 00:00:00\",\n",
    "        \"2021-01-01 00:00:00\",\n",
    "        \"2021-02-01 00:00:00\",\n",
    "        \"2021-02-01 00:00:00\",\n",
    "        \"2021-03-01 00:00:00\",\n",
    "        \"2022-03-01 00:00:00\",\n",
    "    ]\n",
    ").str.to_datetime() == visit_occurrence[\"visit_start_datetime\"]).all()\n",
    "# Check that visit_occurrence has the same start dates\n",
    "assert (pl.Series(\n",
    "    [\n",
    "        \"2020-02-08 00:00:00\",\n",
    "        \"2020-04-08 00:00:00\",\n",
    "        \"2021-01-01 00:00:00\",\n",
    "        \"2021-02-01 00:00:00\",\n",
    "        \"2021-02-01 00:00:00\",\n",
    "        \"2021-03-01 00:00:00\",\n",
    "        \"2022-04-01 00:00:00\",\n",
    "    ]\n",
    ").str.to_datetime() == visit_occurrence[\"visit_end_datetime\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 5 build_visit_occurrence(df_raw_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 20.9 ms ± 2.45 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "- 65.2 ms ± 6.47 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "- 46.7 ms ± 3.65 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "  - Creo que está afectando el que el clúster esté saturadillo. \n",
    "- 21.3 ms ± 2.27 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "- 20.8 ms ± 1.22 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- Verificar tests.\n",
    "- Hacer benchmarks\n",
    "- Limpiar este notebook para que sólo tenga la explicación del código actual\n",
    "  - Traducir a inglés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test piping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw_pl.lazy()\n",
    "visit_detail, visit_occurrence = build_visit_occurrence(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Old Recursive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def find_overlap_index(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Finds all rows that:\n",
    "       - belong to the same person_id\n",
    "       - are contained with the previous row.\n",
    "       - are not single day visits\n",
    "    and removes them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas Dataframe with at least four columns.\n",
    "        Assumes first column is person_id, second column is\n",
    "        start_date and third column is end_date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        pandas Series with bools. True if row is contained\n",
    "        with the previous row, False otherwise.\n",
    "    \"\"\"\n",
    "    # 1. Check that current and previous patient are the same\n",
    "    idx_person = df.iloc[:, 0] == df.iloc[:, 0].shift(1)\n",
    "    # 2. Check that current start_date is later that previous start_date\n",
    "    idx_start = df.iloc[:, 1] >= df.iloc[:, 1].shift(1)\n",
    "    # 3. Check that current end_date is sooner that previous end_date\n",
    "    idx_end = df.iloc[:, 2] <= df.iloc[:, 2].shift(1)\n",
    "    # 4. Check that current interval and previos interval are not both single_day\n",
    "    interval = df.iloc[:, 2] - df.iloc[:, 1]\n",
    "    idx_int_curr = interval <= pd.Timedelta(1, unit=\"D\")\n",
    "    idx_int_prev = interval.shift(1) <= pd.Timedelta(1, unit=\"D\")\n",
    "    idx_interval = ~(idx_int_curr & idx_int_prev)\n",
    "    # 5. If everything past is true, I can drop the row\n",
    "    return idx_start & idx_end & idx_person & idx_interval\n",
    "\n",
    "\n",
    "def remove_overlap(\n",
    "    df: pd.DataFrame,\n",
    "    sorting_columns: tuple,\n",
    "    ascending_order: tuple,\n",
    "    verbose: int = 0,\n",
    "    _counter: int = 0,\n",
    "    _counter_lim: int = 1000,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Removes all rows that are completely contained within\n",
    "    another row. It will not remove rows that are only partially\n",
    "    contained within the previous one.\n",
    "\n",
    "    The function works by sorting the rows by columns. If two or\n",
    "    more rows are overlapping, only the top one will be kept.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas dataframe with overlapping rows to be removed.\n",
    "        Selection of columns is done by selecting ncols in order.\n",
    "        This allows its use for different tables with columns\n",
    "        that have the same purpose but different names.\n",
    "    sorting_columns : tuple\n",
    "        Columns to use for sorting.\n",
    "        Usually, expects 4 columns: 'person_id', 'start_date', 'end_date'\n",
    "        and some '*_concept_id', like 'visit_concept_id'.\n",
    "    ascending_order : tuple\n",
    "        List of bools indicating if each row should have ascending or descending\n",
    "        order.\n",
    "        Important! Usually all are true except end_date column. See Notes.\n",
    "    verbose : int, optional, default 0\n",
    "        Information output\n",
    "        - 0 No info\n",
    "        - 2 Show number of iterations\n",
    "        - 3 Show an example of the first row being removed and\n",
    "            the row that contains it.\n",
    "    _counter : int\n",
    "        Iteration control param. Number of iterations.\n",
    "        0 will be used to begin and function will take over.\n",
    "    _counter_lim : int, optional, default 1000\n",
    "        Iteration control param. Limit of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of input dataframe with contained rows removed.\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    The usual behavior is to have 'person_id', 'start_date' and 'end_date'\n",
    "    as first columns, in ascending, ascending and descending order, respectively.\n",
    "    This ensures that:\n",
    "    - All records for the same person are together (sorting by person_id first)\n",
    "    - Earlier records are placed at the top (sorting by ascending start_date)\n",
    "    - Longer duration visits are placed at the top (sorting by descending end_date)\n",
    "\n",
    "    Bear in mind that missing values will be placed at the bottom by default. Any extra\n",
    "    columns provided will leave any missing values out in case of overlapping records.\n",
    "    \"\"\"\n",
    "    # == Preparation =================================================\n",
    "    # Sanity checks\n",
    "    if len(sorting_columns) != len(ascending_order):\n",
    "        raise ValueError(\n",
    "            \"'sorting_columns' and 'ascending_order' lengths must be equal.\"\n",
    "        )\n",
    "\n",
    "    cond_sort = sorting_columns[:3] != [\"person_id\", \"start_date\", \"end_date\"]\n",
    "    cond_asce = ascending_order[:3] != [True, True, False]\n",
    "    if cond_sort or cond_asce:\n",
    "        warnings.warn(\n",
    "            \"Sorting and ascending initial columns are not the expected order. \\\n",
    "                 Make sure data output is correct.\"\n",
    "        )\n",
    "\n",
    "    # Sort the dataframe if first iteration\n",
    "    if _counter == 0:\n",
    "        if verbose > 0:\n",
    "            print(\"Removing overlapping rows...\")\n",
    "        if verbose > 1:\n",
    "            print(f\" Iter 0 => {df.shape[0]} initial rows.\")\n",
    "        df = df.sort_values(sorting_columns, ascending=ascending_order)\n",
    "\n",
    "    # == Find indexes ================================================\n",
    "    # Get the rows\n",
    "    idx_to_remove = find_overlap_index(df)\n",
    "\n",
    "    # == Main \"loop\" =================================================\n",
    "    # Prepare next loop\n",
    "    idx_to_remove_sum = idx_to_remove.sum()\n",
    "    _counter += 1\n",
    "    # If there's still room to go, go\n",
    "    if (idx_to_remove_sum != 0) and (_counter < _counter_lim):\n",
    "        if verbose > 1:\n",
    "            # Show iteration and number of rows removed\n",
    "            print(f\" Iter {_counter} => {idx_to_remove_sum} rows removed.\")\n",
    "        if verbose > 2:\n",
    "            # Get first removed row and show container and contained row\n",
    "            idx_max = df.index.get_loc(idx_to_remove.idxmax())\n",
    "            print(f\"{df.iloc[(idx_max-1):idx_max+1, :4]}\")\n",
    "        return remove_overlap(\n",
    "            df.loc[~idx_to_remove], sorting_columns, ascending_order, verbose, _counter\n",
    "        )\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pyarrow as pa\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import bps_to_omop.utils.process_dates as pro_dat\n",
    "\n",
    "table_raw = pa.Table.from_pandas(df_raw)\n",
    "table_raw = table_raw.cast(\n",
    "    pa.schema(\n",
    "        [\n",
    "            (\"person_id\", pa.int64()),\n",
    "            (\"start_date\", pa.date64()),\n",
    "            (\"end_date\", pa.date64()),\n",
    "            (\"type_concept\", pa.int64()),\n",
    "            (\"should_remain\", pa.int64()),\n",
    "            (\"visit_concept_id\", pa.int64()),\n",
    "            (\"provider_id\", pa.int64()),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "# Define sorting order\n",
    "sorting_columns = [\"person_id\", \"start_date\", \"end_date\", \"visit_concept_id\"]\n",
    "ascending_order = [True, True, False, True]\n",
    "\n",
    "df_rare = table_raw.to_pandas()\n",
    "df_done = remove_overlap(df_rare, sorting_columns, ascending_order, verbose=1)\n",
    "df_done.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 5 remove_overlap(df_rare, sorting_columns, ascending_order, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt polars to remove_overlap\n",
    "\n",
    "The old method did something wrong, it did not joined together dates that were partially contained. Some rows that should be removed are not.\n",
    "\n",
    "To measure speed with big datasets, we need to verify that both functions return the same results. To do this, we will adapt the functions created for the polars use to match the expected result of the old implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the latest visit_detail_end_datetime from the partially contained visits\n",
    "def retrieve_newest_not_contained_adapted(df):\n",
    "    newest_not_contained = (\n",
    "        df.filter(pl.col(\"main_visit\") == \"Unknown\")\n",
    "        .group_by(\"person_id\")\n",
    "        .agg(\n",
    "            pl.col(\"visit_detail_id\").first().alias(\"visit_detail_id_newest\"),\n",
    "            pl.col(\"visit_detail_start_datetime\")\n",
    "            .first()\n",
    "            .alias(\"visit_detail_start_datetime_newest\"),\n",
    "            pl.col(\"visit_detail_end_datetime\")\n",
    "            .first()\n",
    "            .alias(\"visit_detail_end_datetime_newest\"),\n",
    "        )\n",
    "    )\n",
    "    return newest_not_contained\n",
    "\n",
    "\n",
    "def update_not_contained_rows_adapted(df):\n",
    "\n",
    "    newest_not_contained = retrieve_newest_not_contained_adapted(df)\n",
    "\n",
    "    # Join back to the main dataframe and update visit_end_datetime\n",
    "    df = df.join(\n",
    "        newest_not_contained,\n",
    "        on=\"person_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    # Update the values on visit_detail_id_original, visit_start_datetime and visit_end_datetime\n",
    "    df = df.with_columns(\n",
    "        visit_detail_id_original=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "        .then(pl.col(\"visit_detail_id_newest\"))\n",
    "        .otherwise(pl.col(\"visit_detail_id_original\")),\n",
    "        visit_start_datetime=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "        .then(pl.col(\"visit_detail_start_datetime_newest\"))\n",
    "        .otherwise(pl.col(\"visit_start_datetime\")),\n",
    "        visit_end_datetime=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "        .then(pl.col(\"visit_detail_end_datetime_newest\"))\n",
    "        .otherwise(pl.col(\"visit_end_datetime\")),\n",
    "    ).drop(\n",
    "        pl.col(\n",
    "            \"visit_detail_id_newest\",\n",
    "            \"visit_detail_start_datetime_newest\",\n",
    "            \"visit_detail_end_datetime_newest\",\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_overlap_polars(df, verbose=0):\n",
    "    # -- Initialization --\n",
    "    # Get the core of the visit_detail table\n",
    "    df = build_visit_detail(df)\n",
    "    # Extend the table for processing\n",
    "    df = build_visit_detail_extended(df)\n",
    "    # Initialize counters for the while loop\n",
    "    n_unknown = df.filter(pl.col(\"main_visit\") == \"Unknown\").height\n",
    "    n_iter = 0\n",
    "\n",
    "    # -- Loop --\n",
    "    while n_unknown > 0 and n_iter < 1000:\n",
    "        if verbose > 0:\n",
    "            print(f\"Iter {n_iter:>2}: {n_unknown} unknown rows left.\")\n",
    "        if verbose > 1:\n",
    "            print(df.head(20))\n",
    "\n",
    "        # Look for next batch of main_visits\n",
    "        df = identify_next_main_visits(df)\n",
    "\n",
    "        # Identify and mark completely contained visits\n",
    "        df = identify_contained_rows(df)\n",
    "        df = update_contained_rows(df)\n",
    "\n",
    "        # Identify and mark partially contained visits\n",
    "        df = identify_partial_rows(df)\n",
    "        df = update_partial_rows(df)\n",
    "\n",
    "        # Identify and mark not contained visits\n",
    "        df = identify_not_contained_rows(df)\n",
    "        df = update_not_contained_rows(df)\n",
    "\n",
    "        # Update conditions\n",
    "        n_unknown = df.filter(pl.col(\"main_visit\") == \"Unknown\").height\n",
    "        n_iter += 1\n",
    "\n",
    "    # Assign an unique visit_occurrence_id only to main_visits\n",
    "    df = assign_visit_occurrence_id(df)\n",
    "\n",
    "    # Drop the extra helper columns\n",
    "    df = df.drop(\n",
    "        # Drop helpers\n",
    "        pl.col(\"visit_detail_id_original\"),\n",
    "        pl.col(\"is_contained\"),\n",
    "        pl.col(\"is_partial\"),\n",
    "        pl.col(\"not_contained\"),\n",
    "    )\n",
    "\n",
    "    # -- Build the core of the visit_detal table --\n",
    "    visit_detail = df\n",
    "\n",
    "    # Drop visit_occurrence columns\n",
    "    visit_detail = visit_detail.drop(\n",
    "        pl.col(\"visit_start_datetime\"),\n",
    "        pl.col(\"visit_end_datetime\"),\n",
    "        pl.col(\n",
    "            \"main_visit\"\n",
    "        ),  # This one is dropped here so it can be used for visit_occurrence\n",
    "    )\n",
    "\n",
    "    # -- Build the core of the visit_occurrence table --\n",
    "    visit_occurrence = df\n",
    "\n",
    "    # Get only main visits\n",
    "    visit_occurrence = df.filter(pl.col(\"main_visit\") == \"Yes\").drop(\n",
    "        pl.col(\"main_visit\")\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    visit_occurrence = visit_occurrence.rename(\n",
    "        {\n",
    "            \"visit_detail_type_concept_id\": \"visit_type_concept_id\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Drop columns from visit_detail\n",
    "    visit_occurrence = visit_occurrence.drop(\n",
    "        pl.col(\"visit_detail_start_datetime\"),\n",
    "        pl.col(\"visit_detail_end_datetime\"),\n",
    "        pl.col(\"visit_detail_id\"),\n",
    "        pl.col(\"parent_visit_detail_id\"),\n",
    "    )\n",
    "\n",
    "    return visit_detail, visit_occurrence\n",
    "\n",
    "\n",
    "_visit_detail, visit_occurrence_polars = remove_overlap_polars(df_raw_pl, verbose=1)\n",
    "visit_occurrence_polars = visit_occurrence_polars.select(\n",
    "    \"person_id\",\n",
    "    \"visit_start_datetime\",\n",
    "    \"visit_end_datetime\",\n",
    "    \"visit_type_concept_id\",\n",
    "    \"should_remain\",\n",
    "    \"visit_concept_id\",\n",
    "    \"provider_id\",\n",
    ")\n",
    "\n",
    "visit_occurrence_polars = visit_occurrence_polars.with_columns(\n",
    "    pl.col(\"visit_start_datetime\").dt.to_string().str.to_datetime(),\n",
    "    pl.col(\"visit_end_datetime\").dt.to_string().str.to_datetime(),\n",
    ")\n",
    "\n",
    "print(visit_occurrence_polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the recursive approach here\n",
    "visit_occurrence_recursive = remove_overlap(\n",
    "    df_rare, sorting_columns, ascending_order, verbose=2\n",
    ")\n",
    "visit_occurrence_recursive = (\n",
    "    visit_occurrence_recursive.reset_index(drop=True)\n",
    "    .sort_values([\"person_id\", \"start_date\", \"end_date\", \"visit_concept_id\"])\n",
    "    .rename(\n",
    "        {\n",
    "            \"start_date\": \"visit_start_datetime\",\n",
    "            \"end_date\": \"visit_end_datetime\",\n",
    "            \"type_concept\": \"visit_type_concept_id\",\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "visit_occurrence_recursive = pl.DataFrame(visit_occurrence_recursive)\n",
    "visit_occurrence_recursive = visit_occurrence_recursive.with_columns(\n",
    "    pl.col(\"visit_start_datetime\").dt.to_string().str.to_datetime(),\n",
    "    pl.col(\"visit_end_datetime\").dt.to_string().str.to_datetime(),\n",
    "    pl.col(\"should_remain\").cast(pl.Boolean),\n",
    ")\n",
    "print(visit_occurrence_recursive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polars.testing import assert_frame_equal\n",
    "\n",
    "# assert_frame_equal(visit_occurrence_recursive, visit_occurrence_polars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Test with big datasets\n",
    "\n",
    "Vamos a comparar la velocidad de ambos métodos con datasets grandes.\n",
    "\n",
    "Nos traemos la función para generar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import parquet\n",
    "\n",
    "\n",
    "def create_sample_df(\n",
    "    n: int = 1000,\n",
    "    n_dates: int = 50,\n",
    "    first_date: str = \"2020-01-01\",\n",
    "    last_date: str = \"2023-01-01\",\n",
    "    mean_duration_days: int = 60,\n",
    "    std_duration_days: int = 180,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # == Parameters ==\n",
    "    np.random.seed(42)\n",
    "    pd.options.mode.string_storage = \"pyarrow\"\n",
    "    # Start date from which to start the dates\n",
    "    first_date = pd.to_datetime(first_date)\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    max_days = (last_date - first_date).days\n",
    "\n",
    "    # == Generate IDs randomly ==\n",
    "    # -- Generate the Ids\n",
    "    people = np.random.randint(10000000, 99999999 + 1, size=n)\n",
    "    person_id = np.random.choice(people, n * n_dates)\n",
    "\n",
    "    # == Generate random dates ==\n",
    "    # Generate random integers for days and convert to timedelta\n",
    "    random_days = np.random.randint(0, max_days, size=n * n_dates)\n",
    "    # Create the columns\n",
    "    observation_start_date = first_date + pd.to_timedelta(random_days, unit=\"D\")\n",
    "    # Generate a gaussian sample of dates\n",
    "    random_days = np.random.normal(\n",
    "        mean_duration_days, std_duration_days, size=n * n_dates\n",
    "    )\n",
    "    random_days = np.int32(random_days)\n",
    "    observation_end_date = observation_start_date + pd.to_timedelta(\n",
    "        random_days, unit=\"D\"\n",
    "    )\n",
    "    # Correct end_dates\n",
    "    # => If they are smaller than start_date, take start_date\n",
    "    observation_end_date = np.where(\n",
    "        observation_end_date < observation_start_date,\n",
    "        observation_start_date,\n",
    "        observation_end_date,\n",
    "    )\n",
    "\n",
    "    # == Generate the code ==\n",
    "    period_type_concept_id = np.random.randint(1, 11, size=n * n_dates)\n",
    "\n",
    "    # == Generate the dataframe ==\n",
    "    df_raw = {\n",
    "        \"person_id\": person_id,\n",
    "        \"observation_period_start_date\": observation_start_date,\n",
    "        \"observation_period_end_date\": observation_end_date,\n",
    "        \"period_type_concept_id\": period_type_concept_id,\n",
    "    }\n",
    "    return pd.DataFrame(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "df_raw = create_sample_df(n=1000)\n",
    "df_raw.columns = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "df_raw.loc[:, \"visit_concept_id\"] = 9202\n",
    "\n",
    "df_raw_pl = pl.DataFrame(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nrecursive:')\n",
    "# %timeit -n 10 -r 5 remove_overlap(df_raw, sorting_columns, ascending_order, verbose=0)\n",
    "\n",
    "# print('\\npolars test:')\n",
    "# %timeit -n 10 -r 5 build_visit_occurrence(df_raw_pl, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For n = 100 000\n",
    "\n",
    "    recursive:\n",
    "    3.66 s ± 68.8 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "\n",
    "    polars optimized :\n",
    "    1.08 s ± 50.2 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "\n",
    "For n = 1000\n",
    "\n",
    "    recursive:\n",
    "    34.2 ms ± 482 μs per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "\n",
    "    polars optimized :\n",
    "    25.9 ms ± 558 μs per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "\n",
    "    polars test:\n",
    "    7.72 s ± 111 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "\n",
    "    polars island:\n",
    "    4.06 s ± 377 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the core of the visit_detail table\n",
    "df = build_visit_detail(df_raw_pl)\n",
    "# Extend the table for processing\n",
    "df = build_visit_detail_extended(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify main_visits\n",
    "df = identify_next_main_visits(df)\n",
    "# Update completely contained rows\n",
    "df = identify_contained_rows(df)\n",
    "df = update_contained_rows(df)\n",
    "# Update partially contained rows\n",
    "df = identify_partial_rows(df)\n",
    "df = update_partial_rows(df)\n",
    "# Update not contained rows\n",
    "df = identify_not_contained_rows(df)\n",
    "df = update_not_contained_rows(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify main_visits\n",
    "df = identify_next_main_visits(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update completely contained rows\n",
    "df = identify_contained_rows(df)\n",
    "df = update_contained_rows(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update partially contained rows\n",
    "df = identify_partial_rows(df)\n",
    "df = update_partial_rows(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update not contained rows\n",
    "df = identify_not_contained_rows(df)\n",
    "df = update_not_contained_rows(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_detail, visit_occurrence = build_visit_occurrence(\n",
    "    df_raw_pl, verbose=2, n_iter_max=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Eliminar filas cercanas\n",
    "Una vez que las filas contenidas en otras se han eliminado, el objetivo ahora es eliminar aquellas que están separadas por un número de días menor al que estipulemos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Creación dataset de prueba\n",
    "Vamos a suponer que vamos a agrupar aquellas fechas a menos de 1 año (365 días exactamente) una de otra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nombre_columnas = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "filas = [\n",
    "    # Estas fechas deberían juntarse porque están a menos de 365 dias\n",
    "    # type_concept debería ser 2\n",
    "    (1, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "    (1, \"2020-03-01\", \"2020-04-01\", 2),\n",
    "    (1, \"2020-05-01\", \"2020-12-01\", 2),\n",
    "    # Esta última de la misma persona no\n",
    "    (1, \"2022-01-01\", \"2022-01-01\", 2),\n",
    "    # Estas fechas deberían juntarse porque se pisan\n",
    "    # type_concept debería ser 1\n",
    "    (2, \"2020-01-01\", \"2020-06-01\", 1),\n",
    "    (2, \"2020-03-01\", \"2020-09-01\", 1),\n",
    "    (2, \"2020-06-01\", \"2020-12-01\", 2),\n",
    "    # Estas dos fechas NO deberían juntarse,\n",
    "    # cada uno es su propio periodo\n",
    "    (3, \"2021-01-01\", \"2021-01-01\", 1),\n",
    "    (3, \"2023-02-01\", \"2023-02-01\", 2),\n",
    "    (3, \"2024-03-01\", \"2024-04-01\", 3),\n",
    "    # Se juntarían pero no porque son personas distintas\n",
    "    (4, \"2024-01-01\", \"2024-02-01\", 1),\n",
    "    (5, \"2025-01-01\", \"2025-02-01\", 2),\n",
    "    # Deberían juntarse porque entras ellas hay poca distancia,\n",
    "    # pero si eliminas una de golpe las otras están muy\n",
    "    # separadas y no se juntan.\n",
    "    # type_concept debería ser 2\n",
    "    (6, \"2020-01-01\", \"2020-12-01\", 1),\n",
    "    (6, \"2021-01-01\", \"2021-12-01\", 2),\n",
    "    (6, \"2022-01-01\", \"2022-12-01\", 2),\n",
    "    (6, \"2023-01-01\", \"2023-12-01\", 2),\n",
    "]\n",
    "df_raw = pd.DataFrame.from_records(filas, columns=nombre_columnas)\n",
    "df_raw[\"start_date\"] = pd.to_datetime(df_raw[\"start_date\"])\n",
    "df_raw[\"end_date\"] = pd.to_datetime(df_raw[\"end_date\"])\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponiendo que **agrupamos periodos separados por menos de 365 días** y que **usamos la moda para calcular el `type_concept` final**. El resultado del agrupamiento de los datos creados debería ser el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_columnas = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "filas = [\n",
    "    # Estas fechas deberían juntarse porque están a menos de 365 dias\n",
    "    # type_concept debería ser 2\n",
    "    (1, \"2020-01-01\", \"2020-12-01\", 2),\n",
    "    # Esta última de la misma persona no\n",
    "    (1, \"2022-01-01\", \"2022-01-01\", 2),\n",
    "    # Estas fechas deberían juntarse porque se pisan\n",
    "    # type_concept debería ser 1\n",
    "    (2, \"2020-01-01\", \"2020-12-01\", 1),\n",
    "    # Estas dos fechas NO deberían juntarse,\n",
    "    # cada uno es su propio periodo\n",
    "    (3, \"2021-01-01\", \"2021-01-01\", 1),\n",
    "    (3, \"2023-02-01\", \"2023-02-01\", 2),\n",
    "    (3, \"2024-03-01\", \"2024-04-01\", 3),\n",
    "    # Se juntarían pero no porque son personas distintas\n",
    "    (4, \"2024-01-01\", \"2024-02-01\", 1),\n",
    "    (5, \"2025-01-01\", \"2025-02-01\", 2),\n",
    "    # Deberían juntarse porque entras ellas hay poca distancia,\n",
    "    # pero si eliminas una de golpe las otras están muy\n",
    "    # separadas y no se juntan.\n",
    "    # type_concept debería ser 2\n",
    "    (6, \"2020-01-01\", \"2023-12-01\", 2),\n",
    "]\n",
    "df_result = pd.DataFrame.from_records(filas, columns=nombre_columnas)\n",
    "df_result[\"start_date\"] = pd.to_datetime(df_result[\"start_date\"])\n",
    "df_result[\"end_date\"] = pd.to_datetime(df_result[\"end_date\"])\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Eliminación de filas cercanas\n",
    "\n",
    "Hay varios problemas\n",
    "- Si te encuentras varias filas que cumplen la condición seguidas, puedes perder información si la primera y la última filas están muy separadas.\n",
    "- No se ha encontrado una manera efectiva de hacer esto sin iterar como antes.\n",
    "    - O bien iteras por personas, y no tienes que vigilar que mezclas personas\n",
    "    - O bien lo haces de golpe, pero es muy complejo llevar la cuenta de los `type_concept` y `person_id` que has eliminado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Usando sólo índices\n",
    "\n",
    "Hay que encontrar la primera y última fila de cada persona y también aquellos casos en los que sólo haya una única fila.\n",
    "\n",
    "Luego hay que buscar también aquellos casos en los que la siguiente fila esté muy alejada, lo que implicaría que hemos encontrado una brecha en el periodo de observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw files\n",
    "df_rare = df_raw.sort_values(\n",
    "    [\"person_id\", \"start_date\", \"end_date\"], ascending=[True, True, False]\n",
    ")\n",
    "# It is VERY important to reset the index to make sure we can\n",
    "# retrieve them realiably after sorting them.\n",
    "df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "# Create index for first, last or only person in dataset\n",
    "df_rare[\"idx_person_first\"] = (\n",
    "    df_rare[\"person_id\"] == df_rare[\"person_id\"].shift(-1)\n",
    ") & (df_rare[\"person_id\"] != df_rare[\"person_id\"].shift(1))\n",
    "df_rare[\"idx_person_last\"] = (\n",
    "    df_rare[\"person_id\"] != df_rare[\"person_id\"].shift(-1)\n",
    ") & (df_rare[\"person_id\"] == df_rare[\"person_id\"].shift(1))\n",
    "df_rare[\"idx_person_only\"] = (\n",
    "    df_rare[\"person_id\"] != df_rare[\"person_id\"].shift(-1)\n",
    ") & (df_rare[\"person_id\"] != df_rare[\"person_id\"].shift(1))\n",
    "# Create index if the break is too big and needs to be kept\n",
    "n_days = 365\n",
    "df_rare[\"next_interval\"] = df_rare[\"start_date\"].shift(-1) - df_rare[\"end_date\"]\n",
    "df_rare[\"idx_interval\"] = df_rare[\"next_interval\"] >= pd.Timedelta(n_days, unit=\"D\")\n",
    "# Combine all to see which rows remain\n",
    "df_rare[\"to_remain\"] = (\n",
    "    df_rare[\"idx_person_first\"]\n",
    "    | df_rare[\"idx_person_last\"]\n",
    "    | df_rare[\"idx_person_only\"]\n",
    "    | df_rare[\"idx_interval\"]\n",
    ")\n",
    "\n",
    "df_rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta gente me tengo que quedar seguro :\n",
    "- `idx_person_first == True`\n",
    "- `idx_person_last == True`\n",
    "- `idx_person_only == True`\n",
    "\n",
    "1. Si para una persona sólo hay 1 `idx_person_only == True`, me quedo ese y a correr. En este caso no hay que hacer nada, esa fila tiene la primera y la última fecha del paciente.\n",
    "\n",
    "2. Si para una persona sólo hay 1 `idx_person_first == True` y 1 `idx_person_last == True`, entonces tengo el principio y el final. \n",
    "    1. Si no hay ningún `idx_interval == True`, junto la `start_date` del `idx_person_first == True` y la `end_date` del `idx_person_last == True`.\n",
    "    2. Si hay algún `idx_interval == True`, tengo que tener en cuenta que esas filas indican brechas en el periodo de observación. La fila donde `idx_interval == True` indica que es la última del periodo y que la siguiente es el comienzo de otro.\n",
    "\n",
    "Básicamente hay que registrar por un lado las `start_date`, con sus respectivos `person_id`, y por otro lado las nuevas `end_date`. Vamos a escribir el código que registra esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create an initial dataframe with only person_id and\n",
    "# start_date. The end_date rows and type_concept will be added\n",
    "# later as new columns.\n",
    "\n",
    "# == start_date and person_id ==========================================\n",
    "# To retrieve the start_date we need the indexes of:\n",
    "# - single day periods (idx_person_only == True)\n",
    "# - first dates (idx_person_first == True)\n",
    "# - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "# Get the person condition indexes\n",
    "idx_start = df_rare.index[\n",
    "    df_rare[\"idx_person_only\"]\n",
    "    | df_rare[\"idx_person_first\"]\n",
    "    | df_rare[\"idx_interval\"].shift(1)\n",
    "]\n",
    "# Get the interval indexes\n",
    "df_done = df_rare.loc[idx_start, [\"person_id\", \"start_date\"]]\n",
    "df_done\n",
    "\n",
    "# == end_date ==========================================================\n",
    "# Get the indexes\n",
    "idx_end = df_rare.index[\n",
    "    df_rare[\"idx_person_only\"] | df_rare[\"idx_person_last\"] | df_rare[\"idx_interval\"]\n",
    "]\n",
    "# Append values found to final dataframe\n",
    "df_done[\"end_date\"] = df_rare.loc[idx_end, [\"end_date\"]].values\n",
    "\n",
    "df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar los `type_concept`, podemos usar los índices del principio y el final, hacer un zip y, como deberían estar en orden. Tendré una lista con las parejas inicio final de cada periodo.\n",
    "\n",
    "Si busco todos los `type_concept` dentro de esos periodos, puedo hacer la moda y asignar el `type_concept` más común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "# I can iterate over idx_start and idx_end to get the\n",
    "# periods\n",
    "mode_values = []\n",
    "for i in np.arange(len(idx_start)):\n",
    "    df_tmp = df_rare.loc[idx_start[i] : idx_end[i]]\n",
    "    print(f\"{i=}\")\n",
    "    print(df_tmp[[\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]])\n",
    "    mode = st.mode(df_tmp[\"type_concept\"].values)\n",
    "    print(f\"mode is {mode}\", \"\\n\")\n",
    "    mode_values.append(mode[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to dataframe\n",
    "df_done[\"type_concept\"] = mode_values\n",
    "df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y listo, ya tengo el dataframe con sólo los inicios y finales de los periodos, incluyendo el type_concept más común calculado usando la moda. Comprobamos que es igual que los resultados esperados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_person = df_done[\"person_id\"].values == df_result[\"person_id\"].values\n",
    "print(f\"{'Person_id col is correct:':<28} {check_person.all()}\")\n",
    "check_start_date = df_done[\"start_date\"].values == df_result[\"start_date\"].values\n",
    "print(f\"{'start_date col is correct:':<28} {check_start_date.all()}\")\n",
    "check_end_date = df_done[\"end_date\"].values == df_result[\"end_date\"].values\n",
    "print(f\"{'end_date col is correct:':<28} {check_end_date.all()}\")\n",
    "check_type_concept = df_done[\"type_concept\"].values == df_result[\"type_concept\"].values\n",
    "print(f\"{'type_concept col is correct:'::<28} {check_type_concept.all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Prueba con función recursive (NOT FINISHED)\n",
    "\n",
    "ESTO ESTÁ AQUÍ PARA FUTURAS REFERENCIAS. EL CÓDIGO NO ESTÁ TERMINADO PORQUE EL MÉTODO POR ÍNDICES FUNCIONA LO SUFICIENTEMENTE BIEN Y NO HAY GARANTÍAS DE QUE ESTO LO MEJORE.\n",
    "\n",
    "Parece que lo mejor (cof) va a ser repetir la estrategia anterior e iterar recursivamente. Así además nos aseguramos que podemos llevar la cuenta de los type_concept y quedarnos con el más representativo.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_neighbors_index(df: pd.DataFrame,\n",
    "#                          n_days: int) -> pd.Series:\n",
    "\n",
    "#     # 1. Check that current and next patient are the same\n",
    "#     idx_person = df.iloc[:, 0] == df.iloc[:, 0].shift(-1)\n",
    "#     # 2. Check that current end_date and next start_date\n",
    "#     # are closer than n_days\n",
    "#     idx_interval = (\n",
    "#         (df.iloc[:, 2] - df.iloc[:, 1].shift(-1)) <=\n",
    "#         pd.Timedelta(n_days, unit='D')\n",
    "#     )\n",
    "#     # 4. If everything past is true, I can drop the row\n",
    "#     return idx_person & idx_interval\n",
    "\n",
    "\n",
    "# def remove_all_neighbors_recursive_v1(\n",
    "#         df: pd.DataFrame,\n",
    "#         n_days: int,\n",
    "#         verbose: int = 0,\n",
    "#         _counter: int = 0,\n",
    "#         _counter_lim: int = 1000) -> pd.DataFrame:\n",
    "\n",
    "#     # Get the rows\n",
    "#     idx_to_remove = find_neighbors_index(df, n_days)\n",
    "#     # Prepare next loop\n",
    "#     idx_to_remove_sum = idx_to_remove.sum()\n",
    "#     _counter += 1\n",
    "#     # If there's still room to go, go\n",
    "#     if (idx_to_remove_sum != 0) and (_counter < _counter_lim):\n",
    "#         if verbose >= 1:\n",
    "#             print(f\"Iter {_counter} => {idx_to_remove_sum} rows removed.\")\n",
    "#         if verbose >= 2:\n",
    "#             print(df[idx_to_remove].head(10))\n",
    "\n",
    "#         # Modify end_dates\n",
    "#         df.iloc[:,2] = np.where(idx_to_remove,\n",
    "#                                 df.iloc[:,2].shift(-1),\n",
    "#                                 df.iloc[:,2])\n",
    "#         return remove_all_neighbors_recursive_v1(\n",
    "#             df[idx_to_remove], verbose, _counter)\n",
    "#     else:\n",
    "#         return df\n",
    "\n",
    "# n_days = 365\n",
    "# df_rare = df_raw.sort_values(\n",
    "#     ['person_id', 'start_date', 'end_date'],\n",
    "#     ascending=[True, True, True])\n",
    "# df_rare = remove_all_neighbors_recursive_v1(df_rare, n_days, verbose=2)\n",
    "# df_done = df_rare.sort_index()\n",
    "# df_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_days = 365\n",
    "# df = df_raw.sort_values(\n",
    "#     ['person_id', 'start_date', 'end_date'],\n",
    "#     ascending=[True, True, True])\n",
    "\n",
    "# # >>> Iter 1\n",
    "# idx_to_remove = find_neighbors_index(df, n_days)\n",
    "# print(f\"Iter {1} => {idx_to_remove.sum()} rows removed.\")\n",
    "# print(df[idx_to_remove])\n",
    "# # <<<\n",
    "\n",
    "# # Record changes\n",
    "# df['to_join'] = idx_to_remove\n",
    "# df['new_end_date'] = np.where(df['to_join'],\n",
    "#                               df.iloc[:, 2].shift(-1),\n",
    "#                               df.iloc[:, 2])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # >>> # Iter 2\n",
    "# df.iloc[:,2] = np.where(idx_to_remove,\n",
    "#                         df.iloc[:,2].shift(-1),\n",
    "#                         df.iloc[:,2])\n",
    "# idx_to_remove = find_neighbors_index(df, n_days)\n",
    "# # <<<\n",
    "\n",
    "# # Record changes\n",
    "# df['to_join'] = idx_to_remove\n",
    "# df['new_end_date'] = np.where(df['to_join'],\n",
    "#                               df.iloc[:, 2].shift(-1),\n",
    "#                               df.iloc[:, 2])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # >>> # Iter 3\n",
    "# df.iloc[:,2] = np.where(idx_to_remove,\n",
    "#                         df.iloc[:,2].shift(-1),\n",
    "#                         df.iloc[:,2])\n",
    "# idx_to_remove = find_neighbors_index(df, n_days)\n",
    "# # <<<\n",
    "\n",
    "# # Record changes\n",
    "# df['to_join'] = idx_to_remove\n",
    "# df['new_end_date'] = np.where(df['to_join'],\n",
    "#                               df.iloc[:, 2].shift(-1),\n",
    "#                               df.iloc[:, 2])\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Todo junto\n",
    "\n",
    "Ahora juntamos todo en una función, para poder medir el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "def find_person_index(df: pd.DataFrame) -> tuple[pd.Series]:\n",
    "    \"\"\"Finds all rows that are contained with the previous\n",
    "    row, making sure they belong to the same person_id.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas Dataframe with at least three columns.\n",
    "        Assumes first column is person_id, second column is\n",
    "        start_date and third column is end_date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pd.Series]\n",
    "        Tuple with three pandas Series with bools:\n",
    "        - idx_person_first, True if first row of the person\n",
    "        - idx_person_last, True if last row of the person\n",
    "        - idx_person_only, True if only row of the person\n",
    "        False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create index for first, last or only person in dataset\n",
    "    idx_person_first = (df.iloc[:, 0] == df.iloc[:, 0].shift(-1)) & (\n",
    "        df.iloc[:, 0] != df.iloc[:, 0].shift(1)\n",
    "    )\n",
    "    idx_person_last = (df.iloc[:, 0] != df.iloc[:, 0].shift(-1)) & (\n",
    "        df.iloc[:, 0] == df.iloc[:, 0].shift(1)\n",
    "    )\n",
    "    idx_person_only = (df.iloc[:, 0] != df.iloc[:, 0].shift(-1)) & (\n",
    "        df.iloc[:, 0] != df.iloc[:, 0].shift(1)\n",
    "    )\n",
    "    return (idx_person_first, idx_person_last, idx_person_only)\n",
    "\n",
    "\n",
    "def group_dates(df: pd.DataFrame, n_days: int) -> pd.DataFrame:\n",
    "    \"\"\"Groups rows of dates from the same person that are less\n",
    "    than n_days apart, keeping only the first start_date and\n",
    "    the last end_date, respectively.\n",
    "\n",
    "    It will remove rows that are partially contained within\n",
    "    the previous one.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas dataframe with at least four columns:\n",
    "        ['person_id', 'start_date', 'end_date', 'type_concept'].\n",
    "        Column names do not need to be the same but, the order\n",
    "        must be the same as here.\n",
    "        This allows its use for different tables with columns\n",
    "        that have the same purpose but different names.\n",
    "    verbose : int, optional\n",
    "        Information output, by default 0\n",
    "        - 0 No info\n",
    "        - 1 Show number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of input dataframe with grouped rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # == Preparation ==============================================\n",
    "    # Sort so we know for sure the order is right\n",
    "    df_rare = df.copy().sort_values(\n",
    "        [df.columns[0], df.columns[1], df.columns[2]], ascending=[True, True, False]\n",
    "    )\n",
    "    # It is VERY important to reset the index to make sure we can\n",
    "    # retrieve them realiably after sorting them.\n",
    "    df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "    # == Index look-up ============================================\n",
    "    (idx_person_first, idx_person_last, idx_person_only) = find_person_index(df_rare)\n",
    "    # Create index if the break is too big and needs to be kept\n",
    "    next_interval = df_rare.iloc[:, 1].shift(-1) - df_rare.iloc[:, 2]\n",
    "    idx_interval = next_interval >= pd.Timedelta(n_days, unit=\"D\")\n",
    "\n",
    "    # == Retrieve relevant rows ===================================\n",
    "    # -- start_date and person_id ---------------------------------\n",
    "    # To retrieve the start_date we need the indexes of:\n",
    "    # - single day periods (idx_person_only == True)\n",
    "    # - first dates (idx_person_first == True)\n",
    "    # - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "    # Get the person condition indexes\n",
    "    idx_start = df_rare.index[\n",
    "        idx_person_only | idx_person_first | idx_interval.shift(1)\n",
    "    ]\n",
    "\n",
    "    # -- end_date -------------------------------------------------\n",
    "    # Get the indexes\n",
    "    idx_end = df_rare.index[idx_person_only | idx_person_last | idx_interval]\n",
    "\n",
    "    # == Compute type_concept =====================================\n",
    "    # Iterate over idx_start and idx_end to get the periods\n",
    "    mode_values = []\n",
    "    for i in np.arange(len(idx_start)):\n",
    "        df_tmp = df_rare.loc[idx_start[i] : idx_end[i]]\n",
    "        mode_values.append(st.mode(df_tmp.iloc[:, 3].values)[0])\n",
    "\n",
    "    # == Build final dataframe ====================================\n",
    "    # Create a copy (.loc) with the first two columns\n",
    "    df_done = df_rare.loc[idx_start, [df.columns[0], df.columns[1]]]\n",
    "    # Append values found to final dataframe\n",
    "    df_done[df.columns[2]] = df_rare.loc[idx_end, [df.columns[2]]].values\n",
    "    # Add to dataframe\n",
    "    df_done[df.columns[3]] = mode_values\n",
    "\n",
    "    return df_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Parametros ==\n",
    "n_days = 365\n",
    "\n",
    "# == Creación de datos ==\n",
    "df_done = group_dates(df_raw, n_days)\n",
    "df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "volvemos a comprobar que sale bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_person = df_done[\"person_id\"].values == df_result[\"person_id\"].values\n",
    "print(f\"{'Person_id col is correct:':<28} {check_person.all()}\")\n",
    "check_start_date = df_done[\"start_date\"].values == df_result[\"start_date\"].values\n",
    "print(f\"{'start_date col is correct:':<28} {check_start_date.all()}\")\n",
    "check_end_date = df_done[\"end_date\"].values == df_result[\"end_date\"].values\n",
    "print(f\"{'end_date col is correct:':<28} {check_end_date.all()}\")\n",
    "check_type_concept = df_done[\"type_concept\"].values == df_result[\"type_concept\"].values\n",
    "print(f\"{'type_concept col is correct:'::<28} {check_type_concept.all()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 10 group_dates(df_rare,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prueba con datasets grandes\n",
    "Vamos a comparar si el metodo de pyarrow sigue funcionando más rápido con datasets grandes.\n",
    "\n",
    "Nos traemos la función para generar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import parquet\n",
    "\n",
    "\n",
    "def create_sample_df(\n",
    "    n: int = 1000,\n",
    "    n_dates: int = 50,\n",
    "    first_date: str = \"2020-01-01\",\n",
    "    last_date: str = \"2023-01-01\",\n",
    "    mean_duration_days: int = 60,\n",
    "    std_duration_days: int = 180,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # == Parameters ==\n",
    "    np.random.seed(42)\n",
    "    pd.options.mode.string_storage = \"pyarrow\"\n",
    "    # Start date from which to start the dates\n",
    "    first_date = pd.to_datetime(first_date)\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    max_days = (last_date - first_date).days\n",
    "\n",
    "    # == Generate IDs randomly ==\n",
    "    # -- Generate the Ids\n",
    "    people = np.random.randint(10000000, 99999999 + 1, size=n)\n",
    "    person_id = np.random.choice(people, n * n_dates)\n",
    "\n",
    "    # == Generate random dates ==\n",
    "    # Generate random integers for days and convert to timedelta\n",
    "    random_days = np.random.randint(0, max_days, size=n * n_dates)\n",
    "    # Create the columns\n",
    "    observation_start_date = first_date + pd.to_timedelta(random_days, unit=\"D\")\n",
    "    # Generate a gaussian sample of dates\n",
    "    random_days = np.random.normal(\n",
    "        mean_duration_days, std_duration_days, size=n * n_dates\n",
    "    )\n",
    "    random_days = np.int32(random_days)\n",
    "    observation_end_date = observation_start_date + pd.to_timedelta(\n",
    "        random_days, unit=\"D\"\n",
    "    )\n",
    "    # Correct end_dates\n",
    "    # => If they are smaller than start_date, take start_date\n",
    "    observation_end_date = np.where(\n",
    "        observation_end_date < observation_start_date,\n",
    "        observation_start_date,\n",
    "        observation_end_date,\n",
    "    )\n",
    "\n",
    "    # == Generate the code ==\n",
    "    period_type_concept_id = np.random.randint(1, 11, size=n * n_dates)\n",
    "\n",
    "    # == Generate the dataframe ==\n",
    "    df_raw = {\n",
    "        \"person_id\": person_id,\n",
    "        \"observation_period_start_date\": observation_start_date,\n",
    "        \"observation_period_end_date\": observation_end_date,\n",
    "        \"period_type_concept_id\": period_type_concept_id,\n",
    "    }\n",
    "    return pd.DataFrame(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos traemos la función de pyarrow tal y como estaba el 12/09/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bps_to_omop.general as gen\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import numpy as np\n",
    "\n",
    "# Añadimos el directorio superior al path para poder extraer\n",
    "# las funciones de las carpetas ETL*\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of func_folder to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "\n",
    "def group_dates_original_pyarrow(table_done, n_days):\n",
    "    # -- Thirdly, group up dates -----------------------------------------------\n",
    "    # Agrupamos las fechas usando group_person_dates(). Básicamente calcula la\n",
    "    # distancia temporal entre las filas adyacentes de cada persona, juntándolas\n",
    "    # si es tan por debajo del límite marcado por n_days.\n",
    "    # Agrupamos\n",
    "    table_OBSERVATION_PERIOD = []\n",
    "\n",
    "    person_list = pc.unique(table_done[\"person_id\"])\n",
    "    # Percentage points where you want to print progress\n",
    "    for i, person in enumerate(person_list[:]):\n",
    "        # --Group person\n",
    "        table_person = group_person_dates(table_done, person, n_days)\n",
    "        # Append table\n",
    "        table_OBSERVATION_PERIOD.append(table_person)\n",
    "    # Concatenate\n",
    "    table_OBSERVATION_PERIOD = pa.concat_tables(table_OBSERVATION_PERIOD)\n",
    "    return table_OBSERVATION_PERIOD\n",
    "\n",
    "\n",
    "def group_person_dates(\n",
    "    table_rare: pa.Table, person: str | int, n_days: int\n",
    ") -> pa.Table:\n",
    "    \"\"\"Filters original table for a specific person and reduces\n",
    "    the amount of date records grouping all records that are separated\n",
    "    by n_days or less.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_rare : pa.Table\n",
    "        Table as prepared by 'prepare_table_raw_to_rare()'.\n",
    "    person : str | int\n",
    "        person id, can be an int (the usual) or a string.\n",
    "    n_days : int\n",
    "        number of maximum days between subsequent records.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pa.Table\n",
    "        Table identical to table_rare but with less date records.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter for the current person_id\n",
    "    filt = pc.is_in(\n",
    "        table_rare[\"person_id\"], pa.array([person])  # pylint: disable=E1101\n",
    "    )\n",
    "    table_person = table_rare.filter(filt)\n",
    "    # Retrieve corresponding dates\n",
    "    start_dates = table_person[\"start_date\"]\n",
    "    end_dates = table_person[\"end_date\"]\n",
    "    # Group dates closer\n",
    "    start_dates, end_dates, _ = group_observation_dates(\n",
    "        start_dates, end_dates, n_days, verbose=False\n",
    "    )\n",
    "    # Create person\n",
    "    person_id = pa_utils.create_uniform_int_array(len(start_dates), value=person)\n",
    "    # Retrieve most common period type\n",
    "    period_type_concept_id = pc.mode(  # pylint: disable=E1101\n",
    "        table_person[\"period_type_concept_id\"]\n",
    "    )[0][0]\n",
    "    period_type_concept_id = pa_utils.create_uniform_int_array(\n",
    "        len(start_dates), value=period_type_concept_id\n",
    "    )\n",
    "    # return table\n",
    "    return pa.Table.from_arrays(\n",
    "        [person_id, start_dates, end_dates, period_type_concept_id],\n",
    "        names=[\"person_id\", \"start_date\", \"end_date\", \"period_type_concept_id\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def group_observation_dates(\n",
    "    start_dates: pa.Array, end_dates: pa.Array, n_days: int, verbose: bool = False\n",
    ") -> tuple[pa.Array, pa.Array, None | pa.Table]:\n",
    "    \"\"\"Given a pair of 'start_dates' and 'end_dates', it will\n",
    "        compute the days between each 'end_date' and the next\n",
    "        'start_date' and remove dates that are smaller that a\n",
    "        given number of days ('n_days').\n",
    "    n_days = 365\n",
    "\n",
    "    # Cargamos los datos\n",
    "    df_raw = create_sample_df(n=100)\n",
    "    df_raw.columns = ['person_id', 'start_date', 'end_date', 'type_concept']\n",
    "\n",
    "    df_raw = df_raw.sort_values(\n",
    "        ['person_id', 'start_date', 'end_date', 'type_concept'],\n",
    "        ascending=[True, True, False, True])\n",
    "    df_rare = df_raw.reset_index(drop=True).copy()\n",
    "\n",
    "    print('\\nshift:')\n",
    "    %timeit -n 1 -r 1 group_dates(df_rare,n_days)\n",
    "\n",
    "    df_rare = df_raw.reset_index(drop=True).copy()\n",
    "    df_rare.columns = ['person_id', 'start_date', 'end_date', 'period_type_concept_id']\n",
    "    table_rare = pa.Table.from_pandas(df_rare,preserve_index=False)\n",
    "    print('\\npyarrow:')\n",
    "    %timeit -n 1 -r 1 group_dates_original_pyarrow(table_rare,n_days)\n",
    "        The new dates will only contain start and end dates that have\n",
    "        more than 'n_days' of difference between them.\n",
    "\n",
    "        If dates contain nans/nulls, they will be ignored and grouped\n",
    "        with the closest dates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_dates : pa.Array\n",
    "            Array of start dates\n",
    "        end_dates : pa.Array\n",
    "            Array of end dates\n",
    "        n_days : int\n",
    "            _description_\n",
    "        verbose : bool, optional\n",
    "            _description_, by default False\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[pa.Array, pa.Array, None | pa.Table]\n",
    "            Always return a 3-item tuple.\n",
    "            First item is reduced start dates.\n",
    "            Second item is reduced end dates.\n",
    "            Third item is None if verbose=True,\n",
    "            if verbose=False, is table with start_dates,\n",
    "            end_dates and days between them. Usefull when\n",
    "            verifying dates.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        AssertionError\n",
    "            The resulting starting dates should always come before\n",
    "            their corresponding end dates. Return an AssertionError\n",
    "            otherwise.\n",
    "    \"\"\"\n",
    "    # Get an array of end_dates, taking away the last one\n",
    "    # (last date cannot be compared to the next start date)\n",
    "    from_dates = end_dates[:-1]\n",
    "    # Get an array of start_dates, taking away the first one\n",
    "    # (first date cannot be compared to the previous end date)\n",
    "    to_dates = start_dates[1:]\n",
    "\n",
    "    # -- Compute days between\n",
    "    intervals = pc.days_between(from_dates, to_dates).to_numpy(  # pylint: disable=E1101\n",
    "        zero_copy_only=False\n",
    "    )\n",
    "    # Create an inner table for the calculations if verbose\n",
    "    inner_table = None\n",
    "    if verbose:\n",
    "        inner_table = pa.Table.from_arrays(\n",
    "            [start_dates, end_dates, pa.array(np.append(intervals, np.nan))],\n",
    "            names=[\"start\", \"end\", \"intervals\"],\n",
    "        )\n",
    "\n",
    "    # Filter intervals under some assumption\n",
    "    filt = intervals >= n_days\n",
    "    # => When this filt is 'true', it means that for that index,\n",
    "    # let's call it 'idx', between the end date of 'idx' and the start\n",
    "    # date of 'idx+1' there more than 'n_days' days.\n",
    "    # i.e.:\n",
    "    # (start_date[idx+1] - end_date[idx]).days > n_days\n",
    "\n",
    "    # if no interval is greater, take the first and last rows\n",
    "    if np.nansum(filt) == 0:\n",
    "        idx_end_dates = np.array([len(intervals)])\n",
    "        # Sum 1 to get start dates\n",
    "        idx_start_dates = np.array([0])\n",
    "\n",
    "    # If some filters exist take those\n",
    "    else:\n",
    "        # Get indexes of end_dates\n",
    "        idx_end_dates = filt.nonzero()[0]\n",
    "        # Sum 1 to get corresponding start dates\n",
    "        idx_start_dates = idx_end_dates + 1\n",
    "        # Append last entry as last end_date\n",
    "        idx_end_dates = np.append(idx_end_dates, len(intervals))\n",
    "        # Append first entry as first start_date\n",
    "        idx_start_dates = np.append(0, idx_start_dates)\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(f'{idx_start_dates=}')\n",
    "    #     print(f'{idx_end_dates=}')\n",
    "\n",
    "    # Make sure all end values are after start values\n",
    "    new_start = start_dates.take(idx_start_dates)\n",
    "    new_end = end_dates.take(idx_end_dates)\n",
    "    if pc.any(pc.less(new_end, new_start)).as_py():  # pylint: disable=E1101\n",
    "        if verbose:\n",
    "            print(f\"{start_dates=}\", f\"{end_dates=}\")\n",
    "            print(f\"{new_start=}\", f\"{new_end=}\")\n",
    "        raise AssertionError(\n",
    "            \"Some end dates happen before start dates. Try sorting the original data.\"\n",
    "        )\n",
    "\n",
    "    return (new_start, new_end, inner_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Sanity check\n",
    "### DATA CREATION\n",
    "\n",
    "Probamos primero que los resultados sean iguales con ambas funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "\n",
    "# Cargamos los datos\n",
    "df_raw = create_sample_df(\n",
    "    n=100,\n",
    "    n_dates=10,\n",
    ")\n",
    "df_raw.columns = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "\n",
    "df_raw = df_raw.sort_values(\n",
    "    [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"],\n",
    "    ascending=[True, True, False, True],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how quick `remove_overlap()` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 10 remove_overlap(df_raw,0,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite quick!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove contained dates\n",
    "df_raw = remove_overlap(df_raw, 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[df_raw[\"person_id\"] == 10271836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[df_raw[\"person_id\"] == 23315092]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYARROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == pyarrow method ==\n",
    "df_rare = df_raw.copy()\n",
    "df_rare.columns = [\"person_id\", \"start_date\", \"end_date\", \"period_type_concept_id\"]\n",
    "table_rare = pa.Table.from_pandas(df_rare, preserve_index=False)\n",
    "table_done = group_dates_original_pyarrow(table_rare, n_days)\n",
    "df_done_pyarrow = table_done.to_pandas()\n",
    "df_done_pyarrow = df_done_pyarrow.sort_values(\n",
    "    [\"person_id\", \"start_date\", \"end_date\", \"period_type_concept_id\"],\n",
    "    ascending=[True, True, False, True],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_pyarrow[df_done_pyarrow[\"person_id\"] == 10271836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_pyarrow[df_done_pyarrow[\"person_id\"] == 23315092]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyarrow hace la primera persona (10271836), todas las fechas tienen menos de 365 días entre sí, así que se unen en una sola. Las siguientes son todas de una única fecha por persona hasta 23315092, que tiene dos. Esta también la hace bien.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Shift method ==\n",
    "df_rare = df_raw.copy()\n",
    "df_done_shift = group_dates(df_rare, n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_shift[df_done_shift[\"person_id\"] == 10271836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_shift[df_done_shift[\"person_id\"] == 23315092]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el método shift hace bien la primera persona (10271836) la que tiene dos periodos (23315092). El type_concept cambia del método pyarrow al shift, pero me fio más del shift en este momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Time measurement\n",
    "Ahora probamos a medir el tiempo que tarda cada uno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "\n",
    "# Cargamos los datos\n",
    "df_raw = create_sample_df(n=100)\n",
    "df_raw.columns = ['person_id', 'start_date', 'end_date', 'type_concept']\n",
    "\n",
    "df_raw = df_raw.sort_values(\n",
    "    ['person_id', 'start_date', 'end_date', 'type_concept'],\n",
    "    ascending=[True, True, False, True])\n",
    "df_rare = df_raw.reset_index(drop=True).copy()\n",
    "\n",
    "print('\\nshift:')\n",
    "%timeit -n 1 -r 1 group_dates(df_rare,n_days)\n",
    "\n",
    "df_rare = df_raw.reset_index(drop=True).copy()\n",
    "df_rare.columns = ['person_id', 'start_date', 'end_date', 'period_type_concept_id']\n",
    "table_rare = pa.Table.from_pandas(df_rare,preserve_index=False)\n",
    "print('\\npyarrow:')\n",
    "%timeit -n 1 -r 1 group_dates_original_pyarrow(table_rare,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para n = 1000\n",
    "\n",
    "    shift:\n",
    "    375 ms ± 884 μs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n",
    "    pyarrow:\n",
    "    454 ms ± 664 μs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n",
    "\n",
    "Para n = 10000\n",
    "\n",
    "    shift:\n",
    "    3.69 s ± 4.92 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "    pyarrow:\n",
    "    23.6 s ± 565 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "\n",
    "Para n = 30000\n",
    "\n",
    "    shift:\n",
    "    10.2 s ± 18.5 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "    pyarrow:\n",
    "    3min 10s ± 5.35 s per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
    " \n",
    "\n",
    "Ahora está bastante claro que el método shift funciona mucho más rápido si tenemos muchas personas. Al final en el método original estamos pegando tablas una encima de otra, lo cual resta mucho tiempo. Y esto teniendo en cuenta que el método shift está ordenando dentro de la propia función, cosa que en el de pyarrow dejamos fuera.\n",
    "\n",
    "Quizá si se pudiera implementar con pyarrow un modo siguiendo el patrón de los shift, se podría conseguir algo mejor. Con todo, los 10 s con 30000 paciente y 50 fechas por paciente ya me parece un buen resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba timestamp vs datetime\n",
    "\n",
    "Nos hemos encontrado que el código remove_overlap va mucho más rápido si las fechas están en formato timestamp (pa.timestamp('us)) que si están en datetime (pa.date64()).\n",
    "\n",
    "El problema está en que los datos finales en el proyecto `sarscov` no coinciden si se usa un método o el otro.\n",
    "\n",
    "Probamos a lanzar el código aquí para comprobarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = create_sample_df(n=1000)\n",
    "df_raw.columns = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "df_rare = df_raw.loc[:,['person_id','start_date','end_date','type_concept']]\n",
    "table_raw = pa.Table.from_pandas(df_rare, preserve_index=False)\n",
    "table_raw = table_raw.cast(\n",
    "    pa.schema([\n",
    "        ('person_id', pa.int64()),\n",
    "        ('start_date', pa.timestamp('us')),\n",
    "        ('end_date', pa.timestamp('us')),\n",
    "        ('type_concept', pa.int64()),\n",
    "    ])\n",
    ")\n",
    "df_rare = table_raw.to_pandas()\n",
    "remove_overlap(df_rare,2).info()\n",
    "\n",
    "%timeit -n 10 -r 10 group_dates_original_pyarrow(table_rare,n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "df_rare = df_raw.loc[:,['person_id','start_date','end_date','type_concept']]\n",
    "table_raw = pa.Table.from_pandas(df_rare, preserve_index=False)\n",
    "table_raw = table_raw.cast(\n",
    "    pa.schema([\n",
    "        ('person_id', pa.int64()),\n",
    "        ('start_date', pa.date64()),\n",
    "        ('end_date', pa.date64()),\n",
    "        ('type_concept', pa.int64()),\n",
    "    ])\n",
    ")\n",
    "df_rare = table_raw.to_pandas()\n",
    "remove_overlap(df_rare,2).info()\n",
    "\n",
    "%timeit -n 10 -r 10 group_dates_original_pyarrow(table_rare,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambos formatos funcionan bien, dejando el mismo número de filas.\n",
    "\n",
    "Puede que el problema venga de que algunas fechas en los datos del proyecto vienen con hora. Por ejemplo, todas las de farmacia de dispensación. Si paso estos registros a date64 pierdo la información de la hora, por lo que el orden puede que sea distinto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Nuevo método para calcular type_concept\n",
    "\n",
    "Vamos a comparar el método actual de group_dates con hacer groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from bps_to_omop.general import group_dates, find_person_index\n",
    "\n",
    "\n",
    "def create_sample_data():\n",
    "    nombre_columnas = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "    n_days = 365\n",
    "    df_in = [\n",
    "        # Una única fecha\n",
    "        (1, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        # Dos fechas que se juntan con type_concept iguales\n",
    "        (2, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (2, \"2020-03-01\", \"2020-04-01\", 1),\n",
    "        # Dos fechas que se juntan con type_concept distintos\n",
    "        (3, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (3, \"2020-03-01\", \"2020-04-01\", 2),\n",
    "        # tres fechas que se juntan\n",
    "        (4, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (4, \"2020-03-01\", \"2020-04-01\", 1),\n",
    "        (4, \"2020-05-01\", \"2020-12-01\", 2),\n",
    "        # una persona con dos grupos distintos\n",
    "        (5, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (5, \"2020-03-01\", \"2020-04-01\", 1),\n",
    "        (5, \"2020-05-01\", \"2020-12-01\", 2),\n",
    "        (5, \"2022-01-01\", \"2022-02-01\", 3),\n",
    "        (5, \"2022-03-01\", \"2022-04-01\", 3),\n",
    "        (5, \"2022-05-01\", \"2022-12-01\", 2),\n",
    "    ]\n",
    "    df_in = pd.DataFrame.from_records(df_in, columns=nombre_columnas).assign(\n",
    "        start_date=lambda x: pd.to_datetime(x[\"start_date\"]),\n",
    "        end_date=lambda x: pd.to_datetime(x[\"end_date\"]),\n",
    "    )\n",
    "    return df_in\n",
    "\n",
    "\n",
    "def group_dates_v2(df: pd.DataFrame, n_days: int, verbose: int = 0) -> pd.DataFrame:\n",
    "    # == Preparation ==============================================\n",
    "    if verbose > 0:\n",
    "        print(\"Grouping dates:\")\n",
    "        print(\"- Sorting and preparing data...\")\n",
    "    # Sort so we know for sure the order is right\n",
    "    df_rare = df.copy().sort_values(\n",
    "        [df.columns[0], df.columns[1], df.columns[2]], ascending=[True, True, False]\n",
    "    )\n",
    "    # It is VERY important to reset the index to make sure we can\n",
    "    # retrieve them realiably after sorting them.\n",
    "    df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "    # == Index look-up ============================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Looking up indexes...\")\n",
    "    (idx_person_first, idx_person_last, idx_person_only) = find_person_index(df_rare)\n",
    "    # Create index if the break is too big and needs to be kept\n",
    "    next_interval = df_rare.iloc[:, 1].shift(-1) - df_rare.iloc[:, 2]\n",
    "    idx_interval = next_interval >= pd.Timedelta(n_days, unit=\"D\")\n",
    "\n",
    "    # == Retrieve relevant rows ===================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Retrieving rows...\")\n",
    "    # -- start_date and person_id ---------------------------------\n",
    "    # To retrieve the start_date we need the indexes of:\n",
    "    # - single day periods (idx_person_only == True)\n",
    "    # - first dates (idx_person_first == True)\n",
    "    # - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "    # Get the person condition indexes\n",
    "    idx_start = df_rare.index[\n",
    "        idx_person_only | idx_person_first | idx_interval.shift(1)\n",
    "    ]\n",
    "\n",
    "    # -- end_date -------------------------------------------------\n",
    "    # Get the indexes\n",
    "    idx_end = df_rare.index[idx_person_only | idx_person_last | idx_interval]\n",
    "\n",
    "    # == Compute type_concept =====================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Computing type_concept...\")\n",
    "    # Iterate over idx_start and idx_end to get the periods\n",
    "    mode_values = []\n",
    "    for i in np.arange(len(idx_start)):\n",
    "        df_tmp = df_rare.loc[idx_start[i] : idx_end[i]]\n",
    "        mode_values.append(st.mode(df_tmp.iloc[:, 3].values)[0])\n",
    "\n",
    "        if (verbose > 1) and ((i) % int(len(idx_start) / 4) == 0):\n",
    "            print(f\"  - ({(i+1)/len(idx_start)*100:.1f} %) {(i+1)}/{len(idx_start)}\")\n",
    "    if verbose > 1:\n",
    "        print(f\"  - (100.0 %) {len(idx_start)}/{len(idx_start)}\")\n",
    "\n",
    "    # == Build final dataframe ====================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Closing up...\")\n",
    "    # Create a copy (.loc) with the first two columns\n",
    "    df_done = df_rare.loc[idx_start, [df.columns[0], df.columns[1]]]\n",
    "    # Append values found to final dataframe\n",
    "    df_done[df.columns[2]] = df_rare.loc[idx_end, [df.columns[2]]].values\n",
    "    # Add to dataframe\n",
    "    df_done[df.columns[3]] = mode_values\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"- Done!\")\n",
    "    return df_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "n_days = 365\n",
    "\n",
    "df = create_sample_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(!!)**\n",
    "\n",
    "La idea aquí es que estamos buscando los índice y construyendo manualmente un dataframe con las fechas iniciales y finales.\n",
    "\n",
    "NO podemos usar el truco del groupby para el type concept directamente, ya que no sabemos los intervalos finales.\n",
    "\n",
    "Es decir, podemos agrupar por person_id, pero habría que agrupar también por las fechas, para poder sacar para cada persona y cada observation_period, cuál es el type_concept más frecuente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Preparation ==============================================\n",
    "if verbose > 0:\n",
    "    print(\"Grouping dates:\")\n",
    "    print(\"- Sorting and preparing data...\")\n",
    "# Sort so we know for sure the order is right\n",
    "df_rare = df.copy().sort_values(\n",
    "    [df.columns[0], df.columns[1], df.columns[2]], ascending=[True, True, False]\n",
    ")\n",
    "# It is VERY important to reset the index to make sure we can\n",
    "# retrieve them realiably after sorting them.\n",
    "df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "# == Index look-up ============================================\n",
    "if verbose > 0:\n",
    "    print(\"- Looking up indexes...\")\n",
    "(idx_person_first, idx_person_last, idx_person_only) = find_person_index(df_rare)\n",
    "# Create index if the break is too big and needs to be kept\n",
    "next_interval = df_rare.iloc[:, 1].shift(-1) - df_rare.iloc[:, 2]\n",
    "idx_interval = next_interval >= pd.Timedelta(n_days, unit=\"D\")\n",
    "\n",
    "# == Retrieve relevant rows ===================================\n",
    "if verbose > 0:\n",
    "    print(\"- Retrieving rows...\")\n",
    "# -- start_date and person_id ---------------------------------\n",
    "# To retrieve the start_date we need the indexes of:\n",
    "# - single day periods (idx_person_only == True)\n",
    "# - first dates (idx_person_first == True)\n",
    "# - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "# Get the person condition indexes\n",
    "idx_start = df_rare.index[idx_person_only | idx_person_first | idx_interval.shift(1)]\n",
    "\n",
    "# -- end_date -------------------------------------------------\n",
    "# Get the indexes\n",
    "idx_end = df_rare.index[idx_person_only | idx_person_last | idx_interval]\n",
    "\n",
    "# == Compute type_concept =====================================\n",
    "if verbose > 0:\n",
    "    print(\"- Computing type_concept...\")\n",
    "# Iterate over idx_start and idx_end to get the periods\n",
    "mode_values = []\n",
    "for i in np.arange(len(idx_start)):\n",
    "    df_tmp = df_rare.loc[idx_start[i] : idx_end[i]]\n",
    "    mode_values.append(st.mode(df_tmp.iloc[:, 3].values)[0])\n",
    "\n",
    "    if (verbose > 1) and ((i) % int(len(idx_start) / 4) == 0):\n",
    "        print(f\"  - ({(i+1)/len(idx_start)*100:.1f} %) {(i+1)}/{len(idx_start)}\")\n",
    "if verbose > 1:\n",
    "    print(f\"  - (100.0 %) {len(idx_start)}/{len(idx_start)}\")\n",
    "\n",
    "# == Build final dataframe ====================================\n",
    "if verbose > 0:\n",
    "    print(\"- Closing up...\")\n",
    "# Create a copy (.loc) with the first two columns\n",
    "df_done = df_rare.loc[idx_start, [df.columns[0], df.columns[1]]]\n",
    "# Append values found to final dataframe\n",
    "df_done[df.columns[2]] = df_rare.loc[idx_end, [df.columns[2]]].values\n",
    "# # Add to dataframe\n",
    "df_done[df.columns[3]] = mode_values\n",
    "\n",
    "if verbose > 0:\n",
    "    print(\"- Done!\")\n",
    "df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Unfinished testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def assign_groups_masking(\n",
    "    indices: np.ndarray, starts: np.ndarray, ends: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Group assignment using boolean masking\"\"\"\n",
    "    group_ids = np.zeros(len(indices), dtype=int)\n",
    "    for group_num, (start, end) in enumerate(zip(starts, ends), 1):\n",
    "        mask = (indices >= start) & (indices <= end)\n",
    "        group_ids[mask] = group_num\n",
    "    return group_ids\n",
    "\n",
    "\n",
    "def assign_groups_searchsorted(\n",
    "    indices: np.ndarray, starts: np.ndarray, ends: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Group assignment using searchsorted\"\"\"\n",
    "    boundaries = np.sort(np.concatenate([starts, ends + 1]))\n",
    "    return np.searchsorted(boundaries, indices, side=\"right\") // 2\n",
    "\n",
    "\n",
    "def generate_test_case(n_rows: int, n_groups: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate test data with given size and number of groups\"\"\"\n",
    "    # Create roughly equal-sized groups\n",
    "    group_size = n_rows // n_groups\n",
    "    starts = np.arange(0, n_rows, group_size)\n",
    "    ends = starts + group_size - 1\n",
    "    ends[-1] = n_rows - 1  # Adjust last group\n",
    "    return starts, ends\n",
    "\n",
    "\n",
    "def run_benchmark():\n",
    "    # Test configurations\n",
    "    row_sizes = [10_000, 100_000, 1_000_000]\n",
    "    group_configs = [\n",
    "        (\"Few Large Groups\", lambda x: max(5, x // 1_000_000)),\n",
    "        (\"Medium Groups\", lambda x: max(50, x // 100_000)),\n",
    "        (\"Many Small Groups\", lambda x: max(500, x // 10_000)),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for n_rows in row_sizes:\n",
    "        indices = np.arange(n_rows)\n",
    "\n",
    "        for group_desc, group_func in group_configs:\n",
    "            n_groups = group_func(n_rows)\n",
    "            starts, ends = generate_test_case(n_rows, n_groups)\n",
    "\n",
    "            # Warm-up run\n",
    "            _ = assign_groups_masking(indices, starts, ends)\n",
    "            _ = assign_groups_searchsorted(indices, starts, ends)\n",
    "\n",
    "            # Timing masking approach\n",
    "            start_time = time.perf_counter()\n",
    "            for _ in range(5):  # Multiple runs for more stable results\n",
    "                _ = assign_groups_masking(indices, starts, ends)\n",
    "            masking_time = (time.perf_counter() - start_time) / 5\n",
    "\n",
    "            # Timing searchsorted approach\n",
    "            start_time = time.perf_counter()\n",
    "            for _ in range(5):\n",
    "                _ = assign_groups_searchsorted(indices, starts, ends)\n",
    "            searchsorted_time = (time.perf_counter() - start_time) / 5\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"Rows\": n_rows,\n",
    "                    \"Groups\": n_groups,\n",
    "                    \"Configuration\": group_desc,\n",
    "                    \"Masking Time\": masking_time,\n",
    "                    \"Searchsorted Time\": searchsorted_time,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run benchmark\n",
    "results_df = run_benchmark()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nDetailed Benchmark Results:\")\n",
    "print(\"=\" * 80)\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"\\nConfiguration: {row['Configuration']}\")\n",
    "    print(f\"Data Size: {row['Rows']:,} rows, {row['Groups']:,} groups\")\n",
    "    print(f\"Masking Time: {row['Masking Time']*1000:.2f}ms\")\n",
    "    print(f\"Searchsorted Time: {row['Searchsorted Time']*1000:.2f}ms\")\n",
    "    speedup = row[\"Masking Time\"] / row[\"Searchsorted Time\"]\n",
    "    faster_method = \"searchsorted\" if speedup > 1 else \"masking\"\n",
    "    print(\n",
    "        f\"Winner: {faster_method} ({abs(speedup):,.2f}x {'faster' if speedup > 1 else 'slower'})\"\n",
    "    )\n",
    "\n",
    "# Calculate and print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "for config in results_df[\"Configuration\"].unique():\n",
    "    config_results = results_df[results_df[\"Configuration\"] == config]\n",
    "    print(f\"\\n{config}:\")\n",
    "    avg_speedup = (\n",
    "        config_results[\"Masking Time\"] / config_results[\"Searchsorted Time\"]\n",
    "    ).mean()\n",
    "    print(f\"Average speedup using searchsorted: {avg_speedup:.2f}x\")\n",
    "\n",
    "# Validation of correctness\n",
    "print(\"\\nValidating correctness of implementations...\")\n",
    "test_indices = np.arange(1000)\n",
    "test_starts = np.array([0, 200, 400, 600, 800])\n",
    "test_ends = np.array([199, 399, 599, 799, 999])\n",
    "\n",
    "masking_results = assign_groups_masking(test_indices, test_starts, test_ends)\n",
    "searchsorted_results = assign_groups_searchsorted(test_indices, test_starts, test_ends)\n",
    "\n",
    "if np.array_equal(masking_results, searchsorted_results):\n",
    "    print(\"✓ Both implementations produce identical results\")\n",
    "else:\n",
    "    print(\"⚠ WARNING: Implementations produce different results!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
