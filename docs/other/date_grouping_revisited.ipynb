{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New observation_period_grouping\n",
    "\n",
    "Resulta que el código que agrupa las fechas en OBSERVATION_PERIOD no es del todo correcto. No tiene en cuenta las citas contenidas una dentro de otra. Esto ya lo hemos hecho para VISIT_OCCURRENCE. Ya que además hemos descubierto que sin depender de concatenar dataframes es todo mucho (**MUCHO**) más rápido, vamos a aprovechar para reescribirlo.\n",
    "\n",
    "Primero hay que eliminar las citas que vengan contenidas en otra previa. Esto ya se hizo para VISIT_OCCURRENCE, pero vamos a intentar reescribirlo como una función recursiva.\n",
    "\n",
    "Luego, con el dataframe limpio de citas que se superponen, haremos otra función que calcule las distancias entre citas y elimine las que estén cerca.\n",
    "\n",
    "**12/09/2024** - Las funciones generadas y descritas en este documento se han movido a `ETL1_transform.general`. Sustituyen a las creadas para las tablas OBSERVATION_PERIOD y VISIT_OCCURRENCE, así que estas se han borrado de sus respectivos archivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Eliminar overlap\n",
    "\n",
    "El problema consiste en identificar si, para un paciente dado, el sistema tiene citas que se superponen. Para comprobarlo habría que ordenar las filas por cada persona. \n",
    "\n",
    "Vamos a ordenar primero por person_id de manera ascendente, para que todas las interacciones de una persona estén juntas. Segundo por start_date de manera ascendente, para que las fechas iniciales estén ordenadas en el tiempo. **Tercero, vamos a ordenar por end_date pero de manera descendente**. Esto me asegura que, en una serie de citas que empiezan el mismo día, la primera fila sea la más duradera y, por tanto, tendrá más posibilidades de englobar a las demás.\n",
    "\n",
    "Si un start_date de una fila está dentro del intervalo definido en el start_date y end_date anterior, significa que podemos eliminar la fila anterior.\n",
    "\n",
    "La siguiente clave está en la jerarquía de las citas, si dos citas se superponen y una tiene código de visita a hospital (cód. 8756) y otra de receta farmacia (cód. 581458), el evento más general es la visita al hospital, por lo que es el que debe prevalecer si hay que elegir cual quitar.\n",
    "\n",
    "## 1.1 Creación dataset de prueba\n",
    "Vamos a crear un dataframe a mano que tenga todos los problemas que nos podamos encontrar:\n",
    "- Fechas posteriores completamente contenidas en fechas anteriores\n",
    "    * (2020-01-01, 2020-02-01) contiene a (2020-01-02, 2020-02-02) y a (2020-01-04, 2020-02-04) \n",
    "        - Aquí quiero borrar la segunda.\n",
    "- Fechas posteriores que se superpongan parcialmente a fechas anteriores\n",
    "    * (2020-03-01, 2020-04-01) contiene parcialmente a (2020-03-15, 2020-04-15) \n",
    "        - Aquí quiero combinar la más antigua con la más nueva => (2020-03-01, 2020-04-15).\n",
    "- Asegurarse de que no se mezclen datos de dos personas distintas.\n",
    "    * Comprobar que nunca se combinan datos con person_id distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nombre_columnas = [\n",
    "    \"person_id\",\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "    \"type_concept\",\n",
    "    \"should_remain\",\n",
    "    \"visit_concept_id\",\n",
    "    \"provider_id\",\n",
    "]\n",
    "filas = [\n",
    "    # == Problema de fechas ==\n",
    "    # -- Visita principal 1 --\n",
    "    (\n",
    "        1,  # Person_id\n",
    "        \"2020-01-01\",  # start_date\n",
    "        \"2020-02-01\",  # end_date\n",
    "        1,  # type_concept\n",
    "        True,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # Completamente solapada, mismo type_concept\n",
    "    (\n",
    "        1,  # Person_id\n",
    "        \"2020-01-02\",  # start_date\n",
    "        \"2020-01-02\",  # end_date\n",
    "        1,  # type_concept\n",
    "        False,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # Completamente solapada, distinto type_concept\n",
    "    (\n",
    "        1,  # Person_id\n",
    "        \"2020-01-04\",  # start_date\n",
    "        \"2020-01-04\",  # end_date\n",
    "        2,  # type_concept\n",
    "        False,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # Parcialmente solapada, mismo type_concept\n",
    "    (\n",
    "        1,  # Person_id\n",
    "        \"2020-01-06\",  # start_date\n",
    "        \"2020-02-06\",  # end_date\n",
    "        1,  # type_concept\n",
    "        False,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # Parcialmente solapada, distinto type_concept\n",
    "    (\n",
    "        1,  # Person_id\n",
    "        \"2020-01-08\",  # start_date\n",
    "        \"2020-02-08\",  # end_date\n",
    "        2,  # type_concept\n",
    "        False,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # -- Visita principal 2 --\n",
    "    (\n",
    "        1,  # Person_id\n",
    "        \"2020-03-01\",  # start_date\n",
    "        \"2020-04-01\",  # end_date\n",
    "        1,  # type_concept\n",
    "        True,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # Completamente solapada, mismo type_concept\n",
    "    (\n",
    "        1,  # Person_id\n",
    "        \"2020-03-02\",  # start_date\n",
    "        \"2020-03-02\",  # end_date\n",
    "        1,  # type_concept\n",
    "        False,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # Completamente solapada, distinto type_concept\n",
    "    (\n",
    "        1,  # Person_id\n",
    "        \"2020-03-04\",  # start_date\n",
    "        \"2020-03-04\",  # end_date\n",
    "        2,  # type_concept\n",
    "        False,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # Parcialmente solapada, mismo type_concept\n",
    "    (\n",
    "        1,  # Person_id\n",
    "        \"2020-03-06\",  # start_date\n",
    "        \"2020-04-06\",  # end_date\n",
    "        1,  # type_concept\n",
    "        False,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # Parcialmente solapada, distinto type_concept\n",
    "    (\n",
    "        1,  # Person_id\n",
    "        \"2020-03-08\",  # start_date\n",
    "        \"2020-04-08\",  # end_date\n",
    "        2,  # type_concept\n",
    "        False,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # == Problema de person_id ==\n",
    "    # Tres personas distintas\n",
    "    (\n",
    "        2,  # Person_id\n",
    "        \"2021-01-01\",  # start_date\n",
    "        \"2021-01-01\",  # end_date\n",
    "        1,  # type_concept\n",
    "        True,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    (\n",
    "        2,  # Person_id\n",
    "        \"2021-02-01\",  # start_date\n",
    "        \"2021-02-01\",  # end_date\n",
    "        1,  # type_concept\n",
    "        True,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # Comparte fecha con 1\n",
    "    (\n",
    "        3,  # Person_id\n",
    "        \"2021-02-01\",  # start_date\n",
    "        \"2021-02-01\",  # end_date\n",
    "        2,  # type_concept\n",
    "        True,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # No omparte fecha con 1\n",
    "    (\n",
    "        3,  # Person_id\n",
    "        \"2021-03-01\",  # start_date\n",
    "        \"2021-03-01\",  # end_date\n",
    "        2,  # type_concept\n",
    "        True,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # == Problema de type_concept ==\n",
    "    (\n",
    "        4,  # Person_id\n",
    "        \"2022-03-01\",  # start_date\n",
    "        \"2022-04-01\",  # end_date\n",
    "        1,  # type_concept\n",
    "        True,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "    # Misma persona y fecha, sólo debe quedar type_concept == 1\n",
    "    (\n",
    "        4,  # Person_id\n",
    "        \"2022-03-01\",  # start_date\n",
    "        \"2022-04-01\",  # end_date\n",
    "        2,  # type_concept\n",
    "        False,  # should_remain\n",
    "        9202,  # visit_concept_id\n",
    "        0,  # provider_id\n",
    "    ),\n",
    "]\n",
    "df_raw = pd.DataFrame.from_records(filas, columns=nombre_columnas)\n",
    "df_raw[\"start_date\"] = pd.to_datetime(df_raw[\"start_date\"])\n",
    "df_raw[\"end_date\"] = pd.to_datetime(df_raw[\"end_date\"])\n",
    "(print(df_raw.info()))\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "df.groupby(\"person_id\")[\"type_concept\"].first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Eliminar overlap\n",
    "La idea de este código está clara. Se parte de un dataframe que tiene las columnas `person_id`, `start_date`, `end_date`, `type_concept`. Se ordena en el siguiente orden:\n",
    "1. person_id, ascendente\n",
    "2. start_date, ascendente\n",
    "3. end_date, descendente\n",
    "4. type_concept, ascendente\n",
    "\n",
    "* De este modo nos aseguramos de que para cada `start_date`, la primera fila tiene la `end_date` más alejada, que es la que puede contener a las otras filas que tengan la misma `start_date`.\n",
    "* La columna type_concept tiene que transformarse previamente en tipo categoría con un orden que predefinamos, para que así podamos efectuar el orden. Este orden representará la prioridad del código. Cuando todo lo anterior sea igual, la que permanecerá será aquella fila que esté más arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars approach\n",
    "\n",
    "Es posible que se pueda hacer todo el proceso jugando con columnas. Si además lo hacemos con polars, será bastante más rápido.\n",
    "\n",
    "La idea es construir en un único dataframe las tablas VISIT_DETAIL y VISIT_OCCURRENCE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine this as a visit in a timeline\n",
    "\n",
    "```python\n",
    "-(- - -)- - - - - - - -\n",
    " |     |\n",
    " |     |-> End of the visit\n",
    " |\n",
    " |-> Start of the visit\n",
    "```\n",
    "We want to find the **main visits**:\n",
    "\n",
    "- A main visit contains other visits. \n",
    "- Main visit can be next to each other, ie: The end of the 1st can be the start of the 2nd\n",
    "- Main visits can not overlap each other, ie: The end of the 1st can not be after the start of the 2nd\n",
    "- Only main visits can populate the VISIT_OCCURRENCE table\n",
    "\n",
    "If we sort the visits by person_id (asc), start_date (asc), end_date (desc) y type_concet (asc), we will have something like this for each person:\n",
    "```python\n",
    "-(- - -)- - - - - - - - \n",
    "-(- -)- - - - - - - - -\n",
    "-(-)- - - - - - - - - -\n",
    "- -(- - -)- - - - - - -\n",
    "- -(- -)- - - - - - - -\n",
    "- -(-)- - - - - - - - - \n",
    "- - -(- - -)- - - - - -\n",
    "- - -(- -)- - - - - - -\n",
    "- - -(-)- - - - - - - -\n",
    "- - - - - -(- - -)- - - \n",
    "- - - - - -(- -)- - - -\n",
    "- - - - - -(-)- - - - -\n",
    "```\n",
    "\n",
    "If we compare each visit with the **FIRST ONE**, there are different cases here:\n",
    "1. **COMPLETELY CONTAINED VISITS**: Contained visits are completely contained in the first visit we are considering (This includes consecutive single day visits)\n",
    "2. **PARTIALLY CONTAINED VISITS**:The start of the visit happens after the start of the 1st, but end of the happens after the end of the 1st\n",
    "    - Starts afterwards, but extends further into the future.\n",
    "3. **NOT CONTAINED VISITS**: The start of the visit is after the 1st. It is a \"new\" main visit.\n",
    "\n",
    "```python\n",
    "-(- - -)- - - - - - - - # This is a main visit\n",
    "-(- -)- - - - - - - - - 1. contained\n",
    "-(-)- - - - - - - - - - 1. contained\n",
    "- -(- - -)- - - - - - - 2. partial\n",
    "- -(- -)- - - - - - - - 1. contained\n",
    "- -(-)- - - - - - - - - 1. contained\n",
    "- - -(- - -)- - - - - - 2. partial\n",
    "- - -(- -)- - - - - - - 2. partial\n",
    "- - -(-)- - - - - - - - 1. contained\n",
    "- - - - - -(- - -)- - - # This is the next main visit\n",
    "- - - - - -(- -)- - - - 1. contained\n",
    "- - - - - -(-)- - - - - 1. contained\n",
    "```\n",
    "\n",
    "Completely contained visits are the easy ones. We can link them to the main visit and remove them.\n",
    "\n",
    "Partially contained visits are problematic. These will force us to extend our initial visit further into the future, essentially creating a new record.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos por juntar todos los archivos. Al hacerlo tenemos esencialmente la tabla VISIT_DETAIL. \n",
    "\n",
    "Con esta tabla podemos:\n",
    "\n",
    "1. Construir el esqueleto de VISIT_DETAIL\n",
    "   1. Ordenar según el orden person_id (asc), start_date (asc), end_date (desc) y type_concet (asc).\n",
    "   2. Generar el campo visit_detail_id con todas las visitas realizadas.\n",
    "   3. Renombrar columnas para adaptarlas al formato VISIT_DETAIL\n",
    "\n",
    "Con esto, tendríamos el esqueleto de la tabla VISIT_DETAIL. A partir de aquí podemos empezar a construir VISIT_OCCURRENCE sobre VISIT_DETAIL.\n",
    "\n",
    "2. Extendemos la tabla para prepararla para la generación de VISIT_OCCURRENCE.\n",
    "   1. Generamos placeholders para las futuras columnas de VISIT_OCCURRENCE y las columnas temporales que usaremos durante su generación\n",
    "      1. Agrupamos por cada paciente y generamos:\n",
    "        - **visit_start_datetime**: Inicialmente el primer registro de **visit_detail_start_datetime**. Será la futura columna **visit_start_datetime** de VISIT_OCCCURRENCE.\n",
    "        - **visit_end_datetime**: Inicialmente el primer registro de **visit_detail_end_datetime**. Será la futura columna **visit_start_datetime** de VISIT_OCCCURRENCE. Si hay que unir visitas esta columna tendrá la end_date de la última visita unida.\n",
    "        - **visit_detail_id_original**: Inicialmente el primer registro de **visit_detail_id**. Será la futura columna **visit_detail_id** de VISIT_OCCCURRENCE.\n",
    "        - **main_visit**: Inicialmente tendrá un valor constante *\"Unknown\"* para todas las filas. La iremos modificando para identificar las que son main_visit (Cambiará a *\"Yes\"*) de las que no (cambiará a *\"No\"*).\n",
    "      2. Hacemos un join con VISIT_DETAIL para poder comparar fila a fila estos campos con cada visita de cada paciente.\n",
    "   2. Generar una columna *main_visit*.\n",
    "      - Las *main_visit* son aquellas que engloban otro conjunto de visitas. i.e.: son las filas que estarán en la tabla VISIT_OCCURRENCE\n",
    "      - Este campo tiene 3 posibilidades: \n",
    "         - *Unknown:* No se ha evaluado si es **main_visit** o no\n",
    "         - *Yes:* Se ha evaluado y SÍ es **main_visit**\n",
    "         - *No:* Se ha evaluado y NO es **main_visit**\n",
    "3. Identificamos las primeras **main_visit**\n",
    "   1. Serán las más recientes que veriquen:\n",
    "      1. aún estén como *\"Unknown\"*\n",
    "      2. **visit_detail_id** == **visit_detail_id_original**\n",
    "4. Comprobamos si el resto de visitas están:\n",
    "   1. Completamente contenidas.\n",
    "      - Se cambia **main_visit** a *\"No\"*. \n",
    "      - Se asigna una columna **parent_visit_detail** al valor correspondiente de **visit_detail_id**.\n",
    "   2. Parcialmente contenidas.\n",
    "      - Se cambia **main_visit** a *\"No\"*. \n",
    "      - Se extiende cambia la columna **visit_end_datetime** al valor de la columna **visit_detail_end_datetime** más tardía.\n",
    "   3. No contenidas\n",
    "      - Estas filas hay que volver a analizarlas. Algunas de ellas pueden ser una **main_visit**.\n",
    "      1. Para las filas no contenidas, se agrupa por paciente y se extraen las columnas **visit_detail_start_datetime**, **visit_detail_end_datetime** y **visit_detail_id** del registro más reciente.\n",
    "      2. Se hace un join con la tabla anterior, regenerando las columnas **visit_start_datetime**, **visit_end_datetime** y **visit_detail_id_original**.\n",
    "5. Volvemos al paso 3 y repetimos hasta que no haya más columnas **main_visit** == *\"Unknown\"* o se supere un número predeterminado de iteraciones.\n",
    "\n",
    "\n",
    "It is important to note that every additional column that previously existed in VISIT_DETAIL will be carried to VISIT_OCCURRENCE. Due to the initial sorting, only the values of the first record at every grouping step (steps 3 and 4) will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "pl.Config(\n",
    "    tbl_formatting=\"MARKDOWN\",\n",
    "    tbl_rows=20,\n",
    "    set_tbl_width_chars=400,\n",
    "    set_tbl_cols=-1,\n",
    ")\n",
    "\n",
    "df_raw_pl = pl.DataFrame(df_raw)\n",
    "print(df_raw_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_visit_detail(df):\n",
    "    visit_detail = df\n",
    "    # First we do the sorting\n",
    "    visit_detail = visit_detail.sort(\n",
    "        [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"],\n",
    "        descending=[False, False, True, False],\n",
    "    )\n",
    "    # Assign the visit_detail_id\n",
    "    visit_detail = visit_detail.with_columns(visit_detail_id=np.arange(df.shape[0]))\n",
    "    # Rename columns\n",
    "    visit_detail = visit_detail.rename(\n",
    "        {\n",
    "            \"start_date\": \"visit_detail_start_datetime\",\n",
    "            \"end_date\": \"visit_detail_end_datetime\",\n",
    "            \"type_concept\": \"visit_detail_type_concept_id\",\n",
    "        }\n",
    "    )\n",
    "    return visit_detail\n",
    "\n",
    "\n",
    "visit_detail = build_visit_detail(df_raw_pl)\n",
    "print(visit_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_visit_detail_extended(visit_detail):\n",
    "\n",
    "    # Get the first date of every person and\n",
    "    # create a new dataframe with person_id and the first_end_date\n",
    "    visit_occurrence_dates = (\n",
    "        visit_detail.group_by(\"person_id\")\n",
    "        .first()\n",
    "        .select(\n",
    "            \"person_id\",\n",
    "            \"visit_detail_start_datetime\",\n",
    "            \"visit_detail_end_datetime\",\n",
    "            \"visit_detail_id\",\n",
    "        )\n",
    "    )\n",
    "    visit_occurrence_dates = visit_occurrence_dates.rename(\n",
    "        {\n",
    "            \"visit_detail_start_datetime\": \"visit_start_datetime\",\n",
    "            \"visit_detail_end_datetime\": \"visit_end_datetime\",\n",
    "            \"visit_detail_id\": \"visit_detail_id_original\",\n",
    "        }\n",
    "    )\n",
    "    # Join with the first_end_date dataframe\n",
    "    # This is to compare each date of each person_id with the first_date of that person_id\n",
    "    df = visit_detail.join(visit_occurrence_dates, on=\"person_id\", how=\"left\")\n",
    "    # Create a column to mark if we the row is a main_visit\n",
    "    df = df.with_columns(main_visit=pl.lit(\"Unknown\").cast(pl.Enum([\"Yes\", \"No\", \"Unknown\"])))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = build_visit_detail_extended(visit_detail)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This let us identify the first batch of main visits, those that verify `visit_detail_id_original == visit_detail_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify main_visits\n",
    "def identify_next_main_visits(df):\n",
    "    df = df.with_columns(\n",
    "        main_visit=(\n",
    "            pl.when(\n",
    "                (pl.col(\"visit_detail_id_original\") == pl.col(\"visit_detail_id\"))\n",
    "                & (pl.col(\"main_visit\") == \"Unknown\")\n",
    "            )\n",
    "            .then(pl.lit(\"Yes\"))\n",
    "            .otherwise(pl.col(\"main_visit\"))\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = identify_next_main_visits(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_contained_rows(df):\n",
    "    if \"is_contained\" not in df.columns:\n",
    "        df = df.with_columns(is_contained=pl.lit(False))\n",
    "\n",
    "    df = df.with_columns(\n",
    "        is_contained=pl.when(\n",
    "            (pl.col(\"main_visit\") == \"Unknown\")\n",
    "            & (pl.col(\"visit_start_datetime\") <= pl.col(\"visit_detail_start_datetime\"))\n",
    "            & (pl.col(\"visit_end_datetime\") >= pl.col(\"visit_detail_end_datetime\"))\n",
    "        )\n",
    "        .then(True)\n",
    "        .otherwise(pl.col(\"is_contained\"))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def identify_partial_rows(df):\n",
    "    if \"is_partial\" not in df.columns:\n",
    "        df = df.with_columns(is_partial=pl.lit(False))\n",
    "\n",
    "    df = df.with_columns(\n",
    "        is_partial=pl.when(\n",
    "            (pl.col(\"main_visit\") == \"Unknown\")\n",
    "            & (pl.col(\"visit_detail_start_datetime\") <= pl.col(\"visit_end_datetime\"))\n",
    "            & (pl.col(\"visit_detail_end_datetime\") > pl.col(\"visit_end_datetime\"))\n",
    "        )\n",
    "        .then(True)\n",
    "        .otherwise(pl.col(\"is_partial\"))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def identify_not_contained_rows(df):\n",
    "    if \"not_contained\" not in df.columns:\n",
    "        df = df.with_columns(not_contained=pl.lit(False))\n",
    "\n",
    "    df = df.with_columns(\n",
    "        not_contained=pl.when(\n",
    "            (pl.col(\"main_visit\") == \"Unknown\")\n",
    "            & (pl.col(\"visit_detail_start_datetime\") >= pl.col(\"visit_end_datetime\"))\n",
    "        )\n",
    "        .then(True)\n",
    "        .otherwise(pl.col(\"not_contained\"))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = identify_contained_rows(df)\n",
    "df = identify_partial_rows(df)\n",
    "df = identify_not_contained_rows(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si tenemos `is_contained == True`, la fecha está contenida en la que estamos considerando ahora como la principal.\n",
    "  - Podríamos asignar `parent_visit_detail_id = visit_detail_id_original`\n",
    "  - Estas filas no afectan a VISIT_OCCURRENCE. Mantenemos el `visit_occurrence_id = False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_contained_rows(df):\n",
    "\n",
    "    if \"parent_visit_detail_id\" not in df.columns:\n",
    "        df = df.with_columns(parent_visit_detail_id=pl.lit(None))\n",
    "\n",
    "    df = df.with_columns(\n",
    "        # Update main_visit to mark contained visits\n",
    "        main_visit=(\n",
    "            pl.when((pl.col(\"is_contained\") == True))\n",
    "            .then(pl.lit(\"No\"))\n",
    "            .otherwise(pl.col(\"main_visit\"))\n",
    "        ),\n",
    "        # Build the parent_visit_detail_id, since we are here\n",
    "        parent_visit_detail_id=(\n",
    "            pl.when(\n",
    "                (pl.col(\"is_contained\") == True)\n",
    "                & (pl.col(\"visit_detail_id\") != pl.col(\"visit_detail_id_original\"))\n",
    "            )\n",
    "            .then(pl.col(\"visit_detail_id_original\"))\n",
    "            .otherwise(pl.col(\"parent_visit_detail_id\"))\n",
    "        ),\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = update_contained_rows(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si tenemos `is_partial == True`, la fecha está parcialmente contenida en la que estamos considerando ahora como la principal.\n",
    "  - Habría que reemplazar la actual `visit_end_datetime` por su `visit_detail_end_datetime`.\n",
    "  - Esto puede ocurrir en varias filas, habría que coger la última."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the latest visit_detail_end_datetime from the partially contained visits\n",
    "def retrieve_latest_date(df):\n",
    "    latest_date = (\n",
    "        df.filter(pl.col(\"is_partial\") == True)\n",
    "        .group_by(\"person_id\")\n",
    "        .agg(pl.col(\"visit_detail_end_datetime\").max().alias(\"latest_end_datetime\"))\n",
    "    )\n",
    "    return latest_date\n",
    "\n",
    "\n",
    "latest_date = retrieve_latest_date(df)\n",
    "print(latest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join back to the main dataframe and update visit_end_datetime\n",
    "def update_partial_rows(df):\n",
    "\n",
    "    latest_date = retrieve_latest_date(df)\n",
    "\n",
    "    df = (\n",
    "        df.join(\n",
    "            latest_date,\n",
    "            on=\"person_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .with_columns(\n",
    "            main_visit=(\n",
    "                pl.when((pl.col(\"is_partial\") == True))\n",
    "                .then(pl.lit(\"No\"))\n",
    "                .otherwise(pl.col(\"main_visit\"))\n",
    "            ),\n",
    "            visit_end_datetime=pl.when(pl.col(\"main_visit\") == \"Yes\")\n",
    "            .then(\n",
    "                pl.coalesce(\n",
    "                    [pl.col(\"latest_end_datetime\"), pl.col(\"visit_detail_end_datetime\")]\n",
    "                )\n",
    "            )\n",
    "            .otherwise(pl.col(\"visit_end_datetime\")),\n",
    "        )\n",
    "        .drop(\"latest_end_datetime\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = update_partial_rows(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si tenemos `not_contained == True`, no están contenidas en la visita que estábamos considerando.\n",
    "  - Habría que comenzar de nuevo usando la primera de estas visitas `not_contained`\n",
    "  - Esto puede ocurrir en varias filas, habría que coger la primera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the latest visit_detail_end_datetime from the partially contained visits\n",
    "def retrieve_newest_not_contained(df):\n",
    "    newest_not_contained = (\n",
    "        df.filter(pl.col(\"not_contained\") == True)\n",
    "        .group_by(\"person_id\")\n",
    "        .agg(\n",
    "            pl.col(\"visit_detail_id\").first().alias(\"visit_detail_id_newest\"),\n",
    "            pl.col(\"visit_detail_start_datetime\")\n",
    "            .first()\n",
    "            .alias(\"visit_detail_start_datetime_newest\"),\n",
    "            pl.col(\"visit_detail_end_datetime\")\n",
    "            .first()\n",
    "            .alias(\"visit_detail_end_datetime_newest\"),\n",
    "        )\n",
    "    )\n",
    "    return newest_not_contained\n",
    "\n",
    "\n",
    "newest_not_contained = retrieve_newest_not_contained(df)\n",
    "print(newest_not_contained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_not_contained_rows(df):\n",
    "\n",
    "    newest_not_contained = retrieve_newest_not_contained(df)\n",
    "\n",
    "    # Join back to the main dataframe and update visit_end_datetime\n",
    "    df = df.join(\n",
    "        newest_not_contained,\n",
    "        on=\"person_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    # Update the values on visit_detail_id_original, visit_start_datetime and visit_end_datetime\n",
    "    df = df.with_columns(\n",
    "        visit_detail_id_original=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "        .then(pl.col(\"visit_detail_id_newest\"))\n",
    "        .otherwise(pl.col(\"visit_detail_id_original\")),\n",
    "        visit_start_datetime=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "        .then(pl.col(\"visit_detail_start_datetime_newest\"))\n",
    "        .otherwise(pl.col(\"visit_start_datetime\")),\n",
    "        visit_end_datetime=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "        .then(pl.col(\"visit_detail_end_datetime_newest\"))\n",
    "        .otherwise(pl.col(\"visit_end_datetime\")),\n",
    "    ).drop(\n",
    "        pl.col(\n",
    "            \"visit_detail_id_newest\",\n",
    "            \"visit_detail_start_datetime_newest\",\n",
    "            \"visit_detail_end_datetime_newest\",\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = update_not_contained_rows(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify main_visits\n",
    "df = identify_next_main_visits(df)\n",
    "# Now, we can identify all the completely contained cases\n",
    "df = identify_contained_rows(df)\n",
    "df = identify_partial_rows(df)\n",
    "df = identify_not_contained_rows(df)\n",
    "# Update completely contained rows\n",
    "df = update_contained_rows(df)\n",
    "# Update partially contained rows\n",
    "df = update_partial_rows(df)\n",
    "# Update not contained rows\n",
    "df = update_not_contained_rows(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to assign a unique visit_occurrence_id only for main visits.\n",
    "\n",
    "This functions does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_visit_occurrence_id(visit_occurrence):\n",
    "    # Get the number of main visits. ie: Unique entries in VISIT_OCCURRENCE table\n",
    "    n_main_visits = visit_occurrence.filter(pl.col(\"main_visit\") == \"Yes\").height\n",
    "\n",
    "    # Create a helper column to track main visit sequence\n",
    "    visit_occurrence = visit_occurrence.with_columns(\n",
    "        is_main_visit=(pl.col(\"main_visit\") == \"Yes\")\n",
    "    )\n",
    "\n",
    "    # Assign a unique identifier only to main visits using row_number and clean the helper\n",
    "    visit_occurrence = visit_occurrence.with_columns(\n",
    "        visit_occurrence_id=pl.when(pl.col(\"is_main_visit\"))\n",
    "        .then(pl.col(\"is_main_visit\").cast(pl.Int32).cum_sum() - 1)\n",
    "        .otherwise(None)\n",
    "    ).drop(\"is_main_visit\")\n",
    "\n",
    "    # Fill the rest using forward fill (ffill)\n",
    "    visit_occurrence = visit_occurrence.with_columns(\n",
    "        visit_occurrence_id=pl.col(\"visit_occurrence_id\").forward_fill()\n",
    "    )\n",
    "\n",
    "    return visit_occurrence\n",
    "\n",
    "visit_occurrence = assign_visit_occurrence_id(df)\n",
    "visit_occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build the VISIT_DETAIL and VISIT_OCCURRENCE table by removing all the helper columns we have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_visit_occurrence(df, verbose = 0):\n",
    "    # -- Initialization --\n",
    "    # Get the core of the visit_detail table\n",
    "    df = build_visit_detail(df)\n",
    "    # Extend the table for processing\n",
    "    df = build_visit_detail_extended(df)\n",
    "    # Initialize counters for the while loop\n",
    "    n_unknown = df.filter(pl.col(\"main_visit\") == \"Unknown\").height\n",
    "    n_iter = 0\n",
    "\n",
    "    # -- Loop --\n",
    "    while n_unknown > 0 and n_iter < 10:\n",
    "        if verbose > 0:\n",
    "            print(f\"Iter {n_iter:>2}: {n_unknown} unknown rows left.\")\n",
    "        if verbose > 1:\n",
    "            print(df.filter(pl.col(\"main_visit\") == \"Unknown\").head(10))\n",
    "\n",
    "        # Look for next batch of main_visits\n",
    "        df = identify_next_main_visits(df)\n",
    "\n",
    "        # Identify and mark completely contained visits\n",
    "        df = identify_contained_rows(df)\n",
    "        df = update_contained_rows(df)\n",
    "\n",
    "        # Identify and mark partially contained visits\n",
    "        df = identify_partial_rows(df)\n",
    "        df = update_partial_rows(df)\n",
    "\n",
    "        # Identify and mark not contained visits\n",
    "        df = identify_not_contained_rows(df)\n",
    "        df = update_not_contained_rows(df)\n",
    "\n",
    "        # Update conditions\n",
    "        n_unknown = df.filter(pl.col(\"main_visit\") == \"Unknown\").height\n",
    "        n_iter += 1\n",
    "\n",
    "    # Assign an unique visit_occurrence_id only to main_visits\n",
    "    df = assign_visit_occurrence_id(df)    \n",
    "    \n",
    "    # Drop the extra helper columns\n",
    "    df = df.drop(\n",
    "        # Drop helpers\n",
    "        pl.col(\"visit_detail_id_original\"),\n",
    "        pl.col(\"is_contained\"),\n",
    "        pl.col(\"is_partial\"),\n",
    "        pl.col(\"not_contained\"),\n",
    "    )\n",
    "    \n",
    "    # -- Build the core of the visit_detal table --\n",
    "    visit_detail = df\n",
    "    \n",
    "    # Drop visit_occurrence columns\n",
    "    visit_detail = visit_detail.drop(\n",
    "        pl.col(\"visit_start_datetime\"),\n",
    "        pl.col(\"visit_end_datetime\"),\n",
    "        pl.col(\"main_visit\"), # This one is dropped here so it can be used for visit_occurrence\n",
    "    )\n",
    "    \n",
    "    # -- Build the core of the visit_occurrence table --\n",
    "    visit_occurrence = df\n",
    "    \n",
    "    # Get only main visits\n",
    "    visit_occurrence = df.filter(pl.col(\"main_visit\") == \"Yes\").drop(pl.col(\"main_visit\"))\n",
    "\n",
    "    # Rename columns \n",
    "    visit_occurrence = visit_occurrence.rename(\n",
    "        {\n",
    "            \"visit_detail_type_concept_id\": \"visit_type_concept_id\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Drop columns from visit_detail\n",
    "    visit_occurrence = visit_occurrence.drop(\n",
    "        pl.col(\"visit_detail_start_datetime\"),\n",
    "        pl.col(\"visit_detail_end_datetime\"),\n",
    "        pl.col(\"visit_detail_id\"),\n",
    "        pl.col(\"parent_visit_detail_id\"),\n",
    "    )\n",
    "\n",
    "    return visit_detail, visit_occurrence\n",
    "\n",
    "visit_detail, visit_occurrence = build_visit_occurrence(df_raw_pl,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_detail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 5 build_visit_occurrence(df_raw_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- Verificar tests.\n",
    "- Hacer benchmarks\n",
    "- Limpiar este notebook para que sólo tenga la explicación del código actual\n",
    "  - Traducir a inglés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing this\n",
    "\n",
    "This approach is not faster really, let's try to fix it.\n",
    "\n",
    "Main problem seems to be that we are creating lots of dataframes along the way. We should be creating filters instead, and then apply the filters.\n",
    "\n",
    "Maybe we can just update a single series with the main visits, as we were, but leaving the original dataframe as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Initialization --\n",
    "# Get the core of the visit_detail table\n",
    "df = build_visit_detail(df_raw_pl)\n",
    "# Extend the table for processing\n",
    "df = build_visit_detail_extended(df)\n",
    "# Initialize is_contained, is_partial, not_contained and parent_visit_detail_id\n",
    "df = df.with_columns(\n",
    "    parent_visit_detail_id=pl.lit(None),\n",
    "    is_contained=pl.lit(False),\n",
    "    is_partial=pl.lit(False),\n",
    "    not_contained=pl.lit(False),\n",
    ")\n",
    "# Initialize counters for the while loop\n",
    "n_unknown = df.filter(pl.col(\"main_visit\") == \"Unknown\").height\n",
    "n_iter = 0\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_pass(df):\n",
    "    # Let's rewrite the inner loop to be done in one pass with polars\n",
    "    result = (\n",
    "        df.lazy()\n",
    "        # Find next batch of main_visits\n",
    "        .with_columns(\n",
    "            main_visit=pl.when(\n",
    "                (pl.col(\"main_visit\") == \"Unknown\")\n",
    "                & (pl.col(\"visit_detail_id_original\") == pl.col(\"visit_detail_id\"))\n",
    "            )\n",
    "            .then(pl.lit(\"Yes\"))\n",
    "            .otherwise(pl.col(\"main_visit\"))\n",
    "        )\n",
    "        # Identify contained, partial and not contained rows\n",
    "        .with_columns(\n",
    "            is_contained=pl.when(\n",
    "                (pl.col(\"main_visit\") == \"Unknown\")\n",
    "                & (pl.col(\"visit_start_datetime\") <= pl.col(\"visit_detail_start_datetime\"))\n",
    "                & (pl.col(\"visit_end_datetime\") >= pl.col(\"visit_detail_end_datetime\"))\n",
    "            )\n",
    "            .then(True)\n",
    "            .otherwise(pl.col(\"is_contained\")),\n",
    "            is_partial=pl.when(\n",
    "                (pl.col(\"main_visit\") == \"Unknown\")\n",
    "                & (pl.col(\"visit_detail_start_datetime\") <= pl.col(\"visit_end_datetime\"))\n",
    "                & (pl.col(\"visit_detail_end_datetime\") > pl.col(\"visit_end_datetime\"))\n",
    "            )\n",
    "            .then(True)\n",
    "            .otherwise(pl.col(\"is_partial\")),\n",
    "            not_contained=pl.when(\n",
    "                (pl.col(\"main_visit\") == \"Unknown\")\n",
    "                & (pl.col(\"visit_detail_start_datetime\") >= pl.col(\"visit_end_datetime\"))\n",
    "            )\n",
    "            .then(True)\n",
    "            .otherwise(pl.col(\"not_contained\")),\n",
    "        )\n",
    "        # -- Contained cases --\n",
    "        # Assign parent_visit_detail_id for contained visits, update the main_visit on contained cases\n",
    "        .with_columns(\n",
    "            parent_visit_detail_id=(\n",
    "                pl.when(\n",
    "                    (pl.col(\"is_contained\") == True)\n",
    "                    & (pl.col(\"visit_detail_id\") != pl.col(\"visit_detail_id_original\"))\n",
    "                )\n",
    "                .then(pl.col(\"visit_detail_id_original\"))\n",
    "                .otherwise(pl.col(\"parent_visit_detail_id\"))\n",
    "            ),\n",
    "            main_visit=(\n",
    "                pl.when((pl.col(\"is_contained\") == True))\n",
    "                .then(pl.lit(\"No\"))\n",
    "                .otherwise(pl.col(\"main_visit\"))\n",
    "            ),\n",
    "        )\n",
    "        # -- Partial Cases --\n",
    "        # Update the main_visit on partial cases and get the latest partial\n",
    "        .with_columns(\n",
    "            main_visit=(\n",
    "                pl.when((pl.col(\"is_partial\") == True))\n",
    "                .then(pl.lit(\"No\"))\n",
    "                .otherwise(pl.col(\"main_visit\"))\n",
    "            ),\n",
    "            latest=pl.col(\"visit_detail_end_datetime\")\n",
    "            .filter(pl.col(\"is_partial\") == True)\n",
    "            .max()\n",
    "            .over(\"person_id\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            visit_end_datetime=pl.when((pl.col(\"visit_detail_id\") == pl.col(\"visit_detail_id_original\")))\n",
    "            .then(pl.coalesce([pl.col(\"latest\"), pl.col(\"visit_detail_end_datetime\")]))\n",
    "            .otherwise(pl.col(\"visit_end_datetime\")),\n",
    "        )\n",
    "        # -- Not contained cases --\n",
    "        # We need to refresh visit_detail_id_original, visit_start_datetime, visit_end_datetime for Unknown cases\n",
    "        .with_columns(\n",
    "            newest_id=(\n",
    "                pl.col(\"visit_detail_id\")\n",
    "                .filter(pl.col(\"not_contained\") == True)\n",
    "                .first()\n",
    "                .over(\"person_id\")\n",
    "            ),\n",
    "            newest_start=(\n",
    "                pl.col(\"visit_detail_start_datetime\")\n",
    "                .filter(pl.col(\"not_contained\") == True)\n",
    "                .first()\n",
    "                .over(\"person_id\")\n",
    "            ),\n",
    "            newest_end=(\n",
    "                pl.col(\"visit_detail_end_datetime\")\n",
    "                .filter(pl.col(\"not_contained\") == True)\n",
    "                .first()\n",
    "                .over(\"person_id\")\n",
    "            ),\n",
    "        )\n",
    "        .with_columns(\n",
    "            visit_detail_id_original=pl.when(pl.col(\"main_visit\") == \"Unknown\")\n",
    "            .then(pl.col(\"newest_id\").fill_null(pl.col(\"visit_detail_id_original\")))\n",
    "            .otherwise(pl.col(\"visit_detail_id_original\")),\n",
    "            visit_start_datetime=pl.when(pl.col(\"main_visit\") == \"Unknown\")\n",
    "            .then(pl.col(\"newest_start\").fill_null(pl.col(\"visit_start_datetime\")))\n",
    "            .otherwise(pl.col(\"visit_start_datetime\")),\n",
    "            visit_end_datetime=pl.when(pl.col(\"main_visit\") == \"Unknown\")\n",
    "            .then(pl.col(\"newest_end\").fill_null(pl.col(\"visit_end_datetime\")))\n",
    "            .otherwise(pl.col(\"visit_end_datetime\")),\n",
    "        )\n",
    "        .select(df.columns)\n",
    "        .collect()\n",
    "    )\n",
    "    return result\n",
    "\n",
    "result = single_pass(df)\n",
    "removed_count = df.height - result.height\n",
    "print(f\"Removed {removed_count} overlapping rows. Final rows: {result.height}\")\n",
    "\n",
    "print(\n",
    "    result.select(\n",
    "        \"person_id\",\n",
    "        \"visit_detail_start_datetime\",\n",
    "        \"visit_detail_end_datetime\",\n",
    "        \"visit_start_datetime\",\n",
    "        \"visit_end_datetime\",\n",
    "        \"main_visit\",\n",
    "        \"is_contained\",\n",
    "        \"is_partial\",\n",
    "        \"not_contained\",\n",
    "        \"visit_detail_id\",\n",
    "        \"visit_detail_id_original\",\n",
    "        # \"newest_id\",\n",
    "        # \"newest_start\",\n",
    "        # \"newest_end\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = single_pass(df)\n",
    "print(\n",
    "    result.select(\n",
    "        \"person_id\",\n",
    "        \"visit_detail_id\",\n",
    "        \"visit_detail_id_original\",\n",
    "        \"visit_detail_start_datetime\",\n",
    "        \"visit_detail_end_datetime\",\n",
    "        \"visit_start_datetime\",\n",
    "        \"visit_end_datetime\",\n",
    "        \"main_visit\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = single_pass(result)\n",
    "print(\n",
    "    result.select(\n",
    "        \"person_id\",\n",
    "        \"visit_detail_id\",\n",
    "        \"visit_detail_id_original\",\n",
    "        \"visit_detail_start_datetime\",\n",
    "        \"visit_detail_end_datetime\",\n",
    "        \"visit_start_datetime\",\n",
    "        \"visit_end_datetime\",\n",
    "        \"main_visit\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_visit_occurrence_v2(df_raw_pl, verbose=0):\n",
    "    # -- Initialization --\n",
    "    # Get the core of the visit_detail table\n",
    "    df = build_visit_detail(df_raw_pl)\n",
    "    # Extend the table for processing\n",
    "    df = build_visit_detail_extended(df)\n",
    "    # Initialize is_contained, is_partial, not_contained and parent_visit_detail_id\n",
    "    df = df.with_columns(\n",
    "        parent_visit_detail_id=pl.lit(None),\n",
    "        is_contained=pl.lit(False),\n",
    "        is_partial=pl.lit(False),\n",
    "        not_contained=pl.lit(False),\n",
    "    )\n",
    "    # Initialize counters for the while loop\n",
    "    n_unknown = df.filter(pl.col(\"main_visit\") == \"Unknown\").height\n",
    "    n_iter = 0\n",
    "    # -- Loop --\n",
    "    while n_unknown > 0 and n_iter < 1000:\n",
    "        if verbose > 0:\n",
    "            print(f\"Iter {n_iter:>2}: {n_unknown} unknown rows left.\")\n",
    "\n",
    "        df = single_pass(df)\n",
    "\n",
    "        # Update conditions\n",
    "        n_unknown = df.filter(pl.col(\"main_visit\") == \"Unknown\").height\n",
    "        n_iter += 1\n",
    "\n",
    "    # Assign an unique visit_occurrence_id only to main_visits\n",
    "    df = assign_visit_occurrence_id(df)    \n",
    "    \n",
    "    # Drop the extra helper columns\n",
    "    df = df.drop(\n",
    "        # Drop helpers\n",
    "        pl.col(\"visit_detail_id_original\"),\n",
    "        pl.col(\"is_contained\"),\n",
    "        pl.col(\"is_partial\"),\n",
    "        pl.col(\"not_contained\"),\n",
    "    )\n",
    "    \n",
    "    # -- Build the core of the visit_detal table --\n",
    "    visit_detail = df\n",
    "    \n",
    "    # Drop visit_occurrence columns\n",
    "    visit_detail = visit_detail.drop(\n",
    "        pl.col(\"visit_start_datetime\"),\n",
    "        pl.col(\"visit_end_datetime\"),\n",
    "        pl.col(\"main_visit\"), # This one is dropped here so it can be used for visit_occurrence\n",
    "    )\n",
    "    \n",
    "    # -- Build the core of the visit_occurrence table --\n",
    "    visit_occurrence = df\n",
    "    \n",
    "    # Get only main visits\n",
    "    visit_occurrence = df.filter(pl.col(\"main_visit\") == \"Yes\").drop(pl.col(\"main_visit\"))\n",
    "\n",
    "    # Rename columns \n",
    "    visit_occurrence = visit_occurrence.rename(\n",
    "        {\n",
    "            \"visit_detail_type_concept_id\": \"visit_type_concept_id\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Drop columns from visit_detail\n",
    "    visit_occurrence = visit_occurrence.drop(\n",
    "        pl.col(\"visit_detail_start_datetime\"),\n",
    "        pl.col(\"visit_detail_end_datetime\"),\n",
    "        pl.col(\"visit_detail_id\"),\n",
    "        pl.col(\"parent_visit_detail_id\"),\n",
    "    )\n",
    "\n",
    "    return visit_detail, visit_occurrence\n",
    "\n",
    "%timeit -n 10 -r 5 build_visit_occurrence_v2(df_raw_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import warnings\n",
    "\n",
    "def remove_overlap_optimized(\n",
    "    df: pl.DataFrame,\n",
    "    person_col: str = \"person_id\", \n",
    "    start_col: str = \"start_date\",\n",
    "    end_col: str = \"end_date\",\n",
    "    type_col: str = \"type_concept\",\n",
    "    verbose: int = 0\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Optimized version that attempts to remove overlaps in fewer iterations\n",
    "    by using window functions and more sophisticated logic.\n",
    "    \"\"\"\n",
    "    if verbose > 0:\n",
    "        print(\"Removing overlapping rows (optimized)...\")\n",
    "        print(f\"Initial rows: {df.height}\")\n",
    "    \n",
    "    # Sort by person, start_date, end_date (desc)\n",
    "    df_sorted = df.sort([person_col, start_col, end_col, type_col], descending=[False, False, True, False])\n",
    "    \n",
    "    # Use window functions to identify overlaps more efficiently\n",
    "    result = (\n",
    "        df_sorted\n",
    "        .lazy()\n",
    "        .with_columns([\n",
    "            # Previous row's end date within same person\n",
    "            pl.col(end_col).shift(1).over(person_col).alias(\"prev_end\"),\n",
    "            pl.col(start_col).shift(1).over(person_col).alias(\"prev_start\"),\n",
    "            pl.col(person_col).shift(1).alias(\"prev_person\")\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # Calculate interval lengths\n",
    "            (pl.col(end_col) - pl.col(start_col)).alias(\"curr_interval\"),\n",
    "            (pl.col(\"prev_end\") - pl.col(\"prev_start\")).alias(\"prev_interval\")\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # Mark rows to keep (inverse of removal logic)\n",
    "            ~(\n",
    "                # Same person\n",
    "                (pl.col(person_col) == pl.col(\"prev_person\")) &\n",
    "                # Current starts after or at same time as previous\n",
    "                (pl.col(start_col) >= pl.col(\"prev_start\")) &\n",
    "                # Current ends before or at same time as previous\n",
    "                (pl.col(end_col) <= pl.col(\"prev_end\")) &\n",
    "                # Not both single-day intervals\n",
    "                ~(\n",
    "                    (pl.col(\"curr_interval\") <= pl.duration(days=1)) &\n",
    "                    (pl.col(\"prev_interval\") <= pl.duration(days=1))\n",
    "                )\n",
    "            ).alias(\"keep_row\")\n",
    "        ])\n",
    "        .filter(pl.col(\"keep_row\"))\n",
    "        .select(df.columns)\n",
    "        .collect()\n",
    "    )\n",
    "    \n",
    "    if verbose > 0:\n",
    "        removed_count = df.height - result.height\n",
    "        print(f\"Removed {removed_count} overlapping rows. Final rows: {result.height}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_visit_occurrence_v2_optimized(df_raw_pl, verbose=0):\n",
    "    # -- Initialization (optimized with chaining) --\n",
    "    df = (df_raw_pl\n",
    "          .pipe(build_visit_detail_optimized)\n",
    "          .pipe(build_visit_detail_extended_optimized)\n",
    "          .with_columns([\n",
    "              pl.lit(None).alias(\"parent_visit_detail_id\"),\n",
    "              pl.lit(False).alias(\"is_contained\"),\n",
    "              pl.lit(False).alias(\"is_partial\"),\n",
    "              pl.lit(False).alias(\"not_contained\"),\n",
    "          ]))\n",
    "    \n",
    "    # Pre-compute unknown mask for efficient counting\n",
    "    unknown_mask = pl.col(\"main_visit\") == \"Unknown\"\n",
    "    n_unknown = df.select(unknown_mask.sum()).item()\n",
    "    n_iter = 0\n",
    "    \n",
    "    # -- Optimized Loop --\n",
    "    while n_unknown > 0 and n_iter < 1000:\n",
    "        if verbose > 0:\n",
    "            print(f\"Iter {n_iter:>2}: {n_unknown} unknown rows left.\")\n",
    "\n",
    "        df = single_pass_optimized(df)\n",
    "\n",
    "        # More efficient unknown counting\n",
    "        n_unknown = df.select(unknown_mask.sum()).item()\n",
    "        n_iter += 1\n",
    "\n",
    "    # Assign visit_occurrence_id only to main visits\n",
    "    df = assign_visit_occurrence_id_optimized(df)    \n",
    "    \n",
    "    # -- Build both tables efficiently --\n",
    "    # Define columns to drop once\n",
    "    helper_cols = [\n",
    "        \"visit_detail_id_original\",\n",
    "        \"is_contained\", \n",
    "        \"is_partial\",\n",
    "        \"not_contained\"\n",
    "    ]\n",
    "    \n",
    "    visit_detail_drop_cols = helper_cols + [\n",
    "        \"visit_start_datetime\",\n",
    "        \"visit_end_datetime\",\n",
    "        \"main_visit\"\n",
    "    ]\n",
    "    \n",
    "    visit_occurrence_drop_cols = helper_cols + [\n",
    "        \"visit_detail_start_datetime\",\n",
    "        \"visit_detail_end_datetime\",\n",
    "        \"visit_detail_id\", \n",
    "        \"parent_visit_detail_id\",\n",
    "        \"main_visit\"\n",
    "    ]\n",
    "    \n",
    "    # Build visit_detail\n",
    "    visit_detail = df.drop(visit_detail_drop_cols)\n",
    "    \n",
    "    # Build visit_occurrence (single operation chain)\n",
    "    visit_occurrence = (df\n",
    "        .filter(pl.col(\"main_visit\") == \"Yes\")\n",
    "        .drop(visit_occurrence_drop_cols)\n",
    "        .rename({\"visit_detail_type_concept_id\": \"visit_type_concept_id\"})\n",
    "    )\n",
    "\n",
    "    return visit_detail, visit_occurrence\n",
    "\n",
    "\n",
    "def build_visit_detail_optimized(df):\n",
    "    \"\"\"Optimized version using lazy evaluation and efficient operations\"\"\"\n",
    "    return (df.lazy()\n",
    "            .sort([\"person_id\", \"start_date\", \"end_date\", \"type_concept\"], \n",
    "                  descending=[False, False, True, False])\n",
    "            .with_row_index(\"visit_detail_id\")  # More efficient than numpy arange\n",
    "            .rename({\n",
    "                \"start_date\": \"visit_detail_start_datetime\",\n",
    "                \"end_date\": \"visit_detail_end_datetime\", \n",
    "                \"type_concept\": \"visit_detail_type_concept_id\",\n",
    "            })\n",
    "            .collect())\n",
    "\n",
    "\n",
    "def build_visit_detail_extended_optimized(visit_detail):\n",
    "    \"\"\"Optimized version with better aggregation\"\"\"\n",
    "    # More efficient aggregation - get first row per person\n",
    "    visit_occurrence_dates = (\n",
    "        visit_detail\n",
    "        .group_by(\"person_id\", maintain_order=True)\n",
    "        .agg([\n",
    "            pl.col(\"visit_detail_start_datetime\").first().alias(\"visit_start_datetime\"),\n",
    "            pl.col(\"visit_detail_end_datetime\").first().alias(\"visit_end_datetime\"),\n",
    "            pl.col(\"visit_detail_id\").first().alias(\"visit_detail_id_original\"),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Single join and column creation\n",
    "    return (visit_detail\n",
    "            .join(visit_occurrence_dates, on=\"person_id\", how=\"left\")\n",
    "            .with_columns(\n",
    "                pl.lit(\"Unknown\")\n",
    "                .cast(pl.Enum([\"Yes\", \"No\", \"Unknown\"]))\n",
    "                .alias(\"main_visit\")\n",
    "            ))\n",
    "\n",
    "\n",
    "def single_pass_optimized(df):\n",
    "    \"\"\"Optimized single_pass with reduced redundancy and better column operations\"\"\"\n",
    "    return (\n",
    "        df.lazy()\n",
    "        # Find next batch of main_visits\n",
    "        .with_columns(\n",
    "            main_visit=pl.when(\n",
    "                (pl.col(\"main_visit\") == \"Unknown\") &\n",
    "                (pl.col(\"visit_detail_id_original\") == pl.col(\"visit_detail_id\"))\n",
    "            )\n",
    "            .then(pl.lit(\"Yes\"))\n",
    "            .otherwise(pl.col(\"main_visit\"))\n",
    "        )\n",
    "        # Identify contained, partial and not contained rows (optimized conditions)\n",
    "        .with_columns([\n",
    "            pl.when(\n",
    "                (pl.col(\"main_visit\") == \"Unknown\") &\n",
    "                (pl.col(\"visit_start_datetime\") <= pl.col(\"visit_detail_start_datetime\")) &\n",
    "                (pl.col(\"visit_end_datetime\") >= pl.col(\"visit_detail_end_datetime\"))\n",
    "            )\n",
    "            .then(True)\n",
    "            .otherwise(pl.col(\"is_contained\"))\n",
    "            .alias(\"is_contained\"),\n",
    "            \n",
    "            pl.when(\n",
    "                (pl.col(\"main_visit\") == \"Unknown\") &\n",
    "                (pl.col(\"visit_detail_start_datetime\") <= pl.col(\"visit_end_datetime\")) &\n",
    "                (pl.col(\"visit_detail_end_datetime\") > pl.col(\"visit_end_datetime\"))\n",
    "            )\n",
    "            .then(True)\n",
    "            .otherwise(pl.col(\"is_partial\"))\n",
    "            .alias(\"is_partial\"),\n",
    "            \n",
    "            pl.when(\n",
    "                (pl.col(\"main_visit\") == \"Unknown\") &\n",
    "                (pl.col(\"visit_detail_start_datetime\") >= pl.col(\"visit_end_datetime\"))\n",
    "            )\n",
    "            .then(True)\n",
    "            .otherwise(pl.col(\"not_contained\"))\n",
    "            .alias(\"not_contained\"),\n",
    "        ])\n",
    "        # -- Contained cases --\n",
    "        .with_columns([\n",
    "            pl.when(\n",
    "                pl.col(\"is_contained\") & \n",
    "                (pl.col(\"visit_detail_id\") != pl.col(\"visit_detail_id_original\"))\n",
    "            )\n",
    "            .then(pl.col(\"visit_detail_id_original\"))\n",
    "            .otherwise(pl.col(\"parent_visit_detail_id\"))\n",
    "            .alias(\"parent_visit_detail_id\"),\n",
    "            \n",
    "            pl.when(pl.col(\"is_contained\"))\n",
    "            .then(pl.lit(\"No\"))\n",
    "            .otherwise(pl.col(\"main_visit\"))\n",
    "            .alias(\"main_visit\"),\n",
    "        ])\n",
    "        # -- Partial Cases --\n",
    "        .with_columns([\n",
    "            pl.when(pl.col(\"is_partial\"))\n",
    "            .then(pl.lit(\"No\"))\n",
    "            .otherwise(pl.col(\"main_visit\"))\n",
    "            .alias(\"main_visit\"),\n",
    "            \n",
    "            pl.col(\"visit_detail_end_datetime\")\n",
    "            .filter(pl.col(\"is_partial\"))\n",
    "            .max()\n",
    "            .over(\"person_id\")\n",
    "            .alias(\"latest\"),\n",
    "        ])\n",
    "        .with_columns(\n",
    "            visit_end_datetime=pl.when(pl.col(\"visit_detail_id\") == pl.col(\"visit_detail_id_original\"))\n",
    "            .then(pl.coalesce([pl.col(\"latest\"), pl.col(\"visit_detail_end_datetime\")]))\n",
    "            .otherwise(pl.col(\"visit_end_datetime\"))\n",
    "        )\n",
    "        # -- Not contained cases --\n",
    "        .with_columns([\n",
    "            pl.col(\"visit_detail_id\")\n",
    "            .filter(pl.col(\"not_contained\"))\n",
    "            .first()\n",
    "            .over(\"person_id\")\n",
    "            .alias(\"newest_id\"),\n",
    "            \n",
    "            pl.col(\"visit_detail_start_datetime\")\n",
    "            .filter(pl.col(\"not_contained\"))\n",
    "            .first()\n",
    "            .over(\"person_id\")\n",
    "            .alias(\"newest_start\"),\n",
    "            \n",
    "            pl.col(\"visit_detail_end_datetime\")\n",
    "            .filter(pl.col(\"not_contained\"))\n",
    "            .first()\n",
    "            .over(\"person_id\")\n",
    "            .alias(\"newest_end\"),\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col(\"main_visit\") == \"Unknown\")\n",
    "            .then(pl.coalesce([pl.col(\"newest_id\"), pl.col(\"visit_detail_id_original\")]))\n",
    "            .otherwise(pl.col(\"visit_detail_id_original\"))\n",
    "            .alias(\"visit_detail_id_original\"),\n",
    "            \n",
    "            pl.when(pl.col(\"main_visit\") == \"Unknown\")\n",
    "            .then(pl.coalesce([pl.col(\"newest_start\"), pl.col(\"visit_start_datetime\")]))\n",
    "            .otherwise(pl.col(\"visit_start_datetime\"))\n",
    "            .alias(\"visit_start_datetime\"),\n",
    "            \n",
    "            pl.when(pl.col(\"main_visit\") == \"Unknown\")\n",
    "            .then(pl.coalesce([pl.col(\"newest_end\"), pl.col(\"visit_end_datetime\")]))\n",
    "            .otherwise(pl.col(\"visit_end_datetime\"))\n",
    "            .alias(\"visit_end_datetime\"),\n",
    "        ])\n",
    "        # Clean up temporary columns\n",
    "        .drop([\"latest\", \"newest_id\", \"newest_start\", \"newest_end\"])\n",
    "        .select(df.columns)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "\n",
    "def assign_visit_occurrence_id_optimized(visit_occurrence):\n",
    "    \"\"\"Optimized version - eliminates unnecessary operations and counts\"\"\"\n",
    "    return (visit_occurrence\n",
    "        .with_columns(\n",
    "            # Direct calculation without helper column or unnecessary cast\n",
    "            visit_occurrence_id=pl.when(pl.col(\"main_visit\") == \"Yes\")\n",
    "            .then((pl.col(\"main_visit\") == \"Yes\").cum_sum() - 1)\n",
    "            .otherwise(None)\n",
    "        )\n",
    "        .with_columns(\n",
    "            visit_occurrence_id=pl.col(\"visit_occurrence_id\").forward_fill()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_visit_occurrence_v2(df_raw_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_visit_occurrence_v2_optimized(df_raw_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 5 build_visit_occurrence_v2_optimized(df_raw_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "def find_overlap_index(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Finds all rows that:\n",
    "       - belong to the same person_id\n",
    "       - are contained with the previous row.\n",
    "       - are not single day visits\n",
    "    and removes them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas Dataframe with at least four columns.\n",
    "        Assumes first column is person_id, second column is\n",
    "        start_date and third column is end_date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        pandas Series with bools. True if row is contained\n",
    "        with the previous row, False otherwise.\n",
    "    \"\"\"\n",
    "    # 1. Check that current and previous patient are the same\n",
    "    idx_person = df.iloc[:, 0] == df.iloc[:, 0].shift(1)\n",
    "    # 2. Check that current start_date is later that previous start_date\n",
    "    idx_start = df.iloc[:, 1] >= df.iloc[:, 1].shift(1)\n",
    "    # 3. Check that current end_date is sooner that previous end_date\n",
    "    idx_end = df.iloc[:, 2] <= df.iloc[:, 2].shift(1)\n",
    "    # 4. Check that current interval and previos interval are not both single_day\n",
    "    interval = df.iloc[:, 2] - df.iloc[:, 1]\n",
    "    idx_int_curr = interval <= pd.Timedelta(1, unit=\"D\")\n",
    "    idx_int_prev = interval.shift(1) <= pd.Timedelta(1, unit=\"D\")\n",
    "    idx_interval = ~(idx_int_curr & idx_int_prev)\n",
    "    # 5. If everything past is true, I can drop the row\n",
    "    return idx_start & idx_end & idx_person & idx_interval\n",
    "\n",
    "\n",
    "def remove_overlap(\n",
    "    df: pd.DataFrame,\n",
    "    sorting_columns: tuple,\n",
    "    ascending_order: tuple,\n",
    "    verbose: int = 0,\n",
    "    _counter: int = 0,\n",
    "    _counter_lim: int = 1000,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Removes all rows that are completely contained within\n",
    "    another row. It will not remove rows that are only partially\n",
    "    contained within the previous one.\n",
    "\n",
    "    The function works by sorting the rows by columns. If two or\n",
    "    more rows are overlapping, only the top one will be kept.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas dataframe with overlapping rows to be removed.\n",
    "        Selection of columns is done by selecting ncols in order.\n",
    "        This allows its use for different tables with columns\n",
    "        that have the same purpose but different names.\n",
    "    sorting_columns : tuple\n",
    "        Columns to use for sorting.\n",
    "        Usually, expects 4 columns: 'person_id', 'start_date', 'end_date'\n",
    "        and some '*_concept_id', like 'visit_concept_id'.\n",
    "    ascending_order : tuple\n",
    "        List of bools indicating if each row should have ascending or descending\n",
    "        order.\n",
    "        Important! Usually all are true except end_date column. See Notes.\n",
    "    verbose : int, optional, default 0\n",
    "        Information output\n",
    "        - 0 No info\n",
    "        - 2 Show number of iterations\n",
    "        - 3 Show an example of the first row being removed and\n",
    "            the row that contains it.\n",
    "    _counter : int\n",
    "        Iteration control param. Number of iterations.\n",
    "        0 will be used to begin and function will take over.\n",
    "    _counter_lim : int, optional, default 1000\n",
    "        Iteration control param. Limit of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of input dataframe with contained rows removed.\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    The usual behavior is to have 'person_id', 'start_date' and 'end_date'\n",
    "    as first columns, in ascending, ascending and descending order, respectively.\n",
    "    This ensures that:\n",
    "    - All records for the same person are together (sorting by person_id first)\n",
    "    - Earlier records are placed at the top (sorting by ascending start_date)\n",
    "    - Longer duration visits are placed at the top (sorting by descending end_date)\n",
    "\n",
    "    Bear in mind that missing values will be placed at the bottom by default. Any extra\n",
    "    columns provided will leave any missing values out in case of overlapping records.\n",
    "    \"\"\"\n",
    "    # == Preparation =================================================\n",
    "    # Sanity checks\n",
    "    if len(sorting_columns) != len(ascending_order):\n",
    "        raise ValueError(\n",
    "            \"'sorting_columns' and 'ascending_order' lengths must be equal.\"\n",
    "        )\n",
    "\n",
    "    cond_sort = sorting_columns[:3] != [\"person_id\", \"start_date\", \"end_date\"]\n",
    "    cond_asce = ascending_order[:3] != [True, True, False]\n",
    "    if cond_sort or cond_asce:\n",
    "        warnings.warn(\n",
    "            \"Sorting and ascending initial columns are not the expected order. \\\n",
    "                 Make sure data output is correct.\"\n",
    "        )\n",
    "\n",
    "    # Sort the dataframe if first iteration\n",
    "    if _counter == 0:\n",
    "        if verbose > 0:\n",
    "            print(\"Removing overlapping rows...\")\n",
    "        if verbose > 1:\n",
    "            print(f\" Iter 0 => {df.shape[0]} initial rows.\")\n",
    "        df = df.sort_values(sorting_columns, ascending=ascending_order)\n",
    "\n",
    "    # == Find indexes ================================================\n",
    "    # Get the rows\n",
    "    idx_to_remove = find_overlap_index(df)\n",
    "\n",
    "    # == Main \"loop\" =================================================\n",
    "    # Prepare next loop\n",
    "    idx_to_remove_sum = idx_to_remove.sum()\n",
    "    _counter += 1\n",
    "    # If there's still room to go, go\n",
    "    if (idx_to_remove_sum != 0) and (_counter < _counter_lim):\n",
    "        if verbose > 1:\n",
    "            # Show iteration and number of rows removed\n",
    "            print(f\" Iter {_counter} => {idx_to_remove_sum} rows removed.\")\n",
    "        if verbose > 2:\n",
    "            # Get first removed row and show container and contained row\n",
    "            idx_max = df.index.get_loc(idx_to_remove.idxmax())\n",
    "            print(f\"{df.iloc[(idx_max-1):idx_max+1, :4]}\")\n",
    "        return remove_overlap(\n",
    "            df.loc[~idx_to_remove], sorting_columns, ascending_order, verbose, _counter\n",
    "        )\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pyarrow as pa\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import bps_to_omop.utils.process_dates as pro_dat\n",
    "\n",
    "table_raw = pa.Table.from_pandas(df_raw)\n",
    "table_raw = table_raw.cast(\n",
    "    pa.schema(\n",
    "        [\n",
    "            (\"person_id\", pa.int64()),\n",
    "            (\"start_date\", pa.date64()),\n",
    "            (\"end_date\", pa.date64()),\n",
    "            (\"type_concept\", pa.int64()),\n",
    "            (\"should_remain\", pa.int64()),\n",
    "            (\"visit_concept_id\", pa.int64()),\n",
    "            (\"provider_id\", pa.int64()),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "# Define sorting order\n",
    "sorting_columns = [\"person_id\", \"start_date\", \"end_date\", \"visit_concept_id\"]\n",
    "ascending_order = [True, True, False, True]\n",
    "\n",
    "df_rare = table_raw.to_pandas()\n",
    "df_done = remove_overlap(df_rare, sorting_columns, ascending_order, verbose=1)\n",
    "df_done.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 5 remove_overlap(df_rare, sorting_columns, ascending_order, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt polars to remove_overlap\n",
    "\n",
    "The old method did something wrong, it did not joined together dates that were partially contained. Some rows that should be removed are not.\n",
    "\n",
    "To measure speed with big datasets, we need to verify that both functions return the same results. To do this, we will adapt the functions created for the polars use to match the expected result of the old implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the latest visit_detail_end_datetime from the partially contained visits\n",
    "def retrieve_newest_not_contained_adapted(df):\n",
    "    newest_not_contained = (\n",
    "        df.filter(pl.col(\"main_visit\") == \"Unknown\")\n",
    "        .group_by(\"person_id\")\n",
    "        .agg(\n",
    "            pl.col(\"visit_detail_id\").first().alias(\"visit_detail_id_newest\"),\n",
    "            pl.col(\"visit_detail_start_datetime\")\n",
    "            .first()\n",
    "            .alias(\"visit_detail_start_datetime_newest\"),\n",
    "            pl.col(\"visit_detail_end_datetime\")\n",
    "            .first()\n",
    "            .alias(\"visit_detail_end_datetime_newest\"),\n",
    "        )\n",
    "    )\n",
    "    return newest_not_contained\n",
    "\n",
    "\n",
    "def update_not_contained_rows_adapted(df):\n",
    "\n",
    "    newest_not_contained = retrieve_newest_not_contained_adapted(df)\n",
    "\n",
    "    # Join back to the main dataframe and update visit_end_datetime\n",
    "    df = df.join(\n",
    "        newest_not_contained,\n",
    "        on=\"person_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    # Update the values on visit_detail_id_original, visit_start_datetime and visit_end_datetime\n",
    "    df = df.with_columns(\n",
    "        visit_detail_id_original=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "        .then(pl.col(\"visit_detail_id_newest\"))\n",
    "        .otherwise(pl.col(\"visit_detail_id_original\")),\n",
    "        visit_start_datetime=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "        .then(pl.col(\"visit_detail_start_datetime_newest\"))\n",
    "        .otherwise(pl.col(\"visit_start_datetime\")),\n",
    "        visit_end_datetime=pl.when((pl.col(\"main_visit\") == \"Unknown\"))\n",
    "        .then(pl.col(\"visit_detail_end_datetime_newest\"))\n",
    "        .otherwise(pl.col(\"visit_end_datetime\")),\n",
    "    ).drop(\n",
    "        pl.col(\n",
    "            \"visit_detail_id_newest\",\n",
    "            \"visit_detail_start_datetime_newest\",\n",
    "            \"visit_detail_end_datetime_newest\",\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_overlap_polars(df, verbose=0):\n",
    "    # -- Initialization --\n",
    "    # Get the core of the visit_detail table\n",
    "    df = build_visit_detail(df)\n",
    "    # Extend the table for processing\n",
    "    df = build_visit_detail_extended(df)\n",
    "    # Initialize counters for the while loop\n",
    "    n_unknown = df.filter(pl.col(\"main_visit\") == \"Unknown\").height\n",
    "    n_iter = 0\n",
    "\n",
    "    # -- Loop --\n",
    "    while n_unknown > 0 and n_iter < 1000:\n",
    "        if verbose > 0:\n",
    "            print(f\"Iter {n_iter:>2}: {n_unknown} unknown rows left.\")\n",
    "        if verbose > 1:\n",
    "            print(df.head(20))\n",
    "\n",
    "        # Look for next batch of main_visits\n",
    "        df = identify_next_main_visits(df)\n",
    "\n",
    "        # Identify and mark completely contained visits\n",
    "        df = identify_contained_rows(df)\n",
    "        df = update_contained_rows(df)\n",
    "\n",
    "        # Identify and mark partially contained visits\n",
    "        df = identify_partial_rows(df)\n",
    "        df = update_partial_rows(df)\n",
    "\n",
    "        # Identify and mark not contained visits\n",
    "        df = identify_not_contained_rows(df)\n",
    "        df = update_not_contained_rows(df)\n",
    "\n",
    "        # Update conditions\n",
    "        n_unknown = df.filter(pl.col(\"main_visit\") == \"Unknown\").height\n",
    "        n_iter += 1\n",
    "\n",
    "    # Assign an unique visit_occurrence_id only to main_visits\n",
    "    df = assign_visit_occurrence_id(df)\n",
    "\n",
    "    # Drop the extra helper columns\n",
    "    df = df.drop(\n",
    "        # Drop helpers\n",
    "        pl.col(\"visit_detail_id_original\"),\n",
    "        pl.col(\"is_contained\"),\n",
    "        pl.col(\"is_partial\"),\n",
    "        pl.col(\"not_contained\"),\n",
    "    )\n",
    "\n",
    "    # -- Build the core of the visit_detal table --\n",
    "    visit_detail = df\n",
    "\n",
    "    # Drop visit_occurrence columns\n",
    "    visit_detail = visit_detail.drop(\n",
    "        pl.col(\"visit_start_datetime\"),\n",
    "        pl.col(\"visit_end_datetime\"),\n",
    "        pl.col(\n",
    "            \"main_visit\"\n",
    "        ),  # This one is dropped here so it can be used for visit_occurrence\n",
    "    )\n",
    "\n",
    "    # -- Build the core of the visit_occurrence table --\n",
    "    visit_occurrence = df\n",
    "\n",
    "    # Get only main visits\n",
    "    visit_occurrence = df.filter(pl.col(\"main_visit\") == \"Yes\").drop(\n",
    "        pl.col(\"main_visit\")\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    visit_occurrence = visit_occurrence.rename(\n",
    "        {\n",
    "            \"visit_detail_type_concept_id\": \"visit_type_concept_id\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Drop columns from visit_detail\n",
    "    visit_occurrence = visit_occurrence.drop(\n",
    "        pl.col(\"visit_detail_start_datetime\"),\n",
    "        pl.col(\"visit_detail_end_datetime\"),\n",
    "        pl.col(\"visit_detail_id\"),\n",
    "        pl.col(\"parent_visit_detail_id\"),\n",
    "    )\n",
    "\n",
    "    return visit_detail, visit_occurrence\n",
    "\n",
    "\n",
    "_visit_detail, visit_occurrence_polars = remove_overlap_polars(df_raw_pl, verbose=1)\n",
    "visit_occurrence_polars = visit_occurrence_polars.select(\n",
    "    \"person_id\",\n",
    "    \"visit_start_datetime\",\n",
    "    \"visit_end_datetime\",\n",
    "    \"visit_type_concept_id\",\n",
    "    \"should_remain\",\n",
    "    \"visit_concept_id\",\n",
    "    \"provider_id\",\n",
    ")\n",
    "\n",
    "visit_occurrence_polars = visit_occurrence_polars.with_columns(\n",
    "    pl.col(\"visit_start_datetime\").dt.to_string().str.to_datetime(),\n",
    "    pl.col(\"visit_end_datetime\").dt.to_string().str.to_datetime(),\n",
    ")\n",
    "\n",
    "print(visit_occurrence_polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the recursive approach here\n",
    "visit_occurrence_recursive = remove_overlap(df_rare, sorting_columns, ascending_order, verbose=2)\n",
    "visit_occurrence_recursive = (\n",
    "    visit_occurrence_recursive.reset_index(drop=True)\n",
    "    .sort_values([\"person_id\", \"start_date\", \"end_date\", \"visit_concept_id\"])\n",
    "    .rename({\n",
    "        \"start_date\":\"visit_start_datetime\",\n",
    "        \"end_date\": \"visit_end_datetime\",\n",
    "        \"type_concept\":\"visit_type_concept_id\",\n",
    "    },axis=1)\n",
    ")\n",
    "\n",
    "visit_occurrence_recursive = pl.DataFrame(visit_occurrence_recursive)\n",
    "visit_occurrence_recursive = visit_occurrence_recursive.with_columns(\n",
    "    pl.col(\"visit_start_datetime\").dt.to_string().str.to_datetime(),\n",
    "    pl.col(\"visit_end_datetime\").dt.to_string().str.to_datetime(),\n",
    "    pl.col(\"should_remain\").cast(pl.Boolean)\n",
    ")\n",
    "print(visit_occurrence_recursive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polars.testing import assert_frame_equal\n",
    "\n",
    "# assert_frame_equal(visit_occurrence_recursive, visit_occurrence_polars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba con datasets grandes\n",
    "\n",
    "Vamos a comparar la velocidad de ambos métodos con datasets grandes.\n",
    "\n",
    "Nos traemos la función para generar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import parquet\n",
    "\n",
    "\n",
    "def create_sample_df(\n",
    "    n: int = 1000,\n",
    "    n_dates: int = 50,\n",
    "    first_date: str = \"2020-01-01\",\n",
    "    last_date: str = \"2023-01-01\",\n",
    "    mean_duration_days: int = 60,\n",
    "    std_duration_days: int = 180,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # == Parameters ==\n",
    "    np.random.seed(42)\n",
    "    pd.options.mode.string_storage = \"pyarrow\"\n",
    "    # Start date from which to start the dates\n",
    "    first_date = pd.to_datetime(first_date)\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    max_days = (last_date - first_date).days\n",
    "\n",
    "    # == Generate IDs randomly ==\n",
    "    # -- Generate the Ids\n",
    "    people = np.random.randint(10000000, 99999999 + 1, size=n)\n",
    "    person_id = np.random.choice(people, n * n_dates)\n",
    "\n",
    "    # == Generate random dates ==\n",
    "    # Generate random integers for days and convert to timedelta\n",
    "    random_days = np.random.randint(0, max_days, size=n * n_dates)\n",
    "    # Create the columns\n",
    "    observation_start_date = first_date + pd.to_timedelta(random_days, unit=\"D\")\n",
    "    # Generate a gaussian sample of dates\n",
    "    random_days = np.random.normal(\n",
    "        mean_duration_days, std_duration_days, size=n * n_dates\n",
    "    )\n",
    "    random_days = np.int32(random_days)\n",
    "    observation_end_date = observation_start_date + pd.to_timedelta(\n",
    "        random_days, unit=\"D\"\n",
    "    )\n",
    "    # Correct end_dates\n",
    "    # => If they are smaller than start_date, take start_date\n",
    "    observation_end_date = np.where(\n",
    "        observation_end_date < observation_start_date,\n",
    "        observation_start_date,\n",
    "        observation_end_date,\n",
    "    )\n",
    "\n",
    "    # == Generate the code ==\n",
    "    period_type_concept_id = np.random.randint(1, 11, size=n * n_dates)\n",
    "\n",
    "    # == Generate the dataframe ==\n",
    "    df_raw = {\n",
    "        \"person_id\": person_id,\n",
    "        \"observation_period_start_date\": observation_start_date,\n",
    "        \"observation_period_end_date\": observation_end_date,\n",
    "        \"period_type_concept_id\": period_type_concept_id,\n",
    "    }\n",
    "    return pd.DataFrame(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check - Return the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "df_raw = create_sample_df(\n",
    "    n=100,\n",
    "    n_dates=10,\n",
    ")\n",
    "df_raw.columns = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "df_raw.loc[:,\"visit_concept_id\"] = 9202\n",
    "\n",
    "# df_raw = df_raw.sort_values(\n",
    "#     [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"],\n",
    "#     ascending=[True, True, False, True],\n",
    "# )\n",
    "df_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==  Recursive approach ==\n",
    "visit_occurrence_recursive = remove_overlap(df_raw, sorting_columns, ascending_order, verbose=2)\n",
    "visit_occurrence_recursive = (\n",
    "    visit_occurrence_recursive.reset_index(drop=True)\n",
    "    .sort_values([\"person_id\", \"start_date\", \"end_date\", \"visit_concept_id\"])\n",
    "    .rename({\n",
    "        \"start_date\":\"visit_start_datetime\",\n",
    "        \"end_date\": \"visit_end_datetime\",\n",
    "        \"type_concept\":\"visit_type_concept_id\",\n",
    "    },axis=1)\n",
    ")\n",
    "\n",
    "visit_occurrence_recursive = pl.DataFrame(visit_occurrence_recursive)\n",
    "visit_occurrence_recursive = visit_occurrence_recursive.with_columns(\n",
    "    pl.col(\"visit_start_datetime\").dt.to_string().str.to_datetime(),\n",
    "    pl.col(\"visit_end_datetime\").dt.to_string().str.to_datetime(),\n",
    ")\n",
    "visit_occurrence_recursive.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == polars approach ==\n",
    "_visit_detail, visit_occurrence_polars = remove_overlap_polars(pl.DataFrame(df_raw), verbose=1)\n",
    "visit_occurrence_polars = visit_occurrence_polars.select(\n",
    "    \"person_id\",\n",
    "    \"visit_start_datetime\",\n",
    "    \"visit_end_datetime\",\n",
    "    \"visit_type_concept_id\",\n",
    "    \"visit_concept_id\",\n",
    ")\n",
    "\n",
    "visit_occurrence_polars = visit_occurrence_polars.with_columns(\n",
    "    pl.col(\"visit_start_datetime\").dt.to_string().str.to_datetime(),\n",
    "    pl.col(\"visit_end_datetime\").dt.to_string().str.to_datetime(),\n",
    ")\n",
    "visit_occurrence_polars.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_visit_occurrence_v2(df_raw_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polars.testing import assert_frame_equal\n",
    "\n",
    "# assert_frame_equal(visit_occurrence_recursive, visit_occurrence_polars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "df_raw = create_sample_df(n=1000)\n",
    "df_raw.columns = ['person_id', 'start_date', 'end_date', 'type_concept']\n",
    "df_raw.loc[:,\"visit_concept_id\"] = 9202\n",
    "\n",
    "df_raw_pl = pl.DataFrame(df_raw)\n",
    "\n",
    "print('\\nrecursive:')\n",
    "%timeit -n 10 -r 5 remove_overlap(df_raw, sorting_columns, ascending_order, verbose=0)\n",
    "\n",
    "# print('\\npolars:') # => Currently broken\n",
    "# %timeit -n 10 -r 5 build_visit_occurrence(df_raw_pl, verbose=0)\n",
    "\n",
    "print('\\npolars optimized :')\n",
    "%timeit -n 10 -r 5 remove_overlap_optimized(df_raw_pl, verbose=0)\n",
    "\n",
    "print('\\npolars test:')\n",
    "%timeit -n 10 -r 5 build_visit_occurrence_v2(df_raw_pl)\n",
    "\n",
    "print('\\npolars test optimized:')\n",
    "%timeit -n 10 -r 5 build_visit_occurrence_v2_optimized(df_raw_pl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For n = 100 000\n",
    "\n",
    "    recursive:\n",
    "    3.66 s ± 68.8 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "\n",
    "    polars optimized :\n",
    "    1.08 s ± 50.2 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Eliminar filas cercanas\n",
    "Una vez que las filas contenidas en otras se han eliminado, el objetivo ahora es eliminar aquellas que están separadas por un número de días menor al que estipulemos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Creación dataset de prueba\n",
    "Vamos a suponer que vamos a agrupar aquellas fechas a menos de 1 año (365 días exactamente) una de otra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nombre_columnas = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "filas = [\n",
    "    # Estas fechas deberían juntarse porque están a menos de 365 dias\n",
    "    # type_concept debería ser 2\n",
    "    (1, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "    (1, \"2020-03-01\", \"2020-04-01\", 2),\n",
    "    (1, \"2020-05-01\", \"2020-12-01\", 2),\n",
    "    # Esta última de la misma persona no\n",
    "    (1, \"2022-01-01\", \"2022-01-01\", 2),\n",
    "    # Estas fechas deberían juntarse porque se pisan\n",
    "    # type_concept debería ser 1\n",
    "    (2, \"2020-01-01\", \"2020-06-01\", 1),\n",
    "    (2, \"2020-03-01\", \"2020-09-01\", 1),\n",
    "    (2, \"2020-06-01\", \"2020-12-01\", 2),\n",
    "    # Estas dos fechas NO deberían juntarse,\n",
    "    # cada uno es su propio periodo\n",
    "    (3, \"2021-01-01\", \"2021-01-01\", 1),\n",
    "    (3, \"2023-02-01\", \"2023-02-01\", 2),\n",
    "    (3, \"2024-03-01\", \"2024-04-01\", 3),\n",
    "    # Se juntarían pero no porque son personas distintas\n",
    "    (4, \"2024-01-01\", \"2024-02-01\", 1),\n",
    "    (5, \"2025-01-01\", \"2025-02-01\", 2),\n",
    "    # Deberían juntarse porque entras ellas hay poca distancia,\n",
    "    # pero si eliminas una de golpe las otras están muy\n",
    "    # separadas y no se juntan.\n",
    "    # type_concept debería ser 2\n",
    "    (6, \"2020-01-01\", \"2020-12-01\", 1),\n",
    "    (6, \"2021-01-01\", \"2021-12-01\", 2),\n",
    "    (6, \"2022-01-01\", \"2022-12-01\", 2),\n",
    "    (6, \"2023-01-01\", \"2023-12-01\", 2),\n",
    "]\n",
    "df_raw = pd.DataFrame.from_records(filas, columns=nombre_columnas)\n",
    "df_raw[\"start_date\"] = pd.to_datetime(df_raw[\"start_date\"])\n",
    "df_raw[\"end_date\"] = pd.to_datetime(df_raw[\"end_date\"])\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponiendo que **agrupamos periodos separados por menos de 365 días** y que **usamos la moda para calcular el `type_concept` final**. El resultado del agrupamiento de los datos creados debería ser el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_columnas = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "filas = [\n",
    "    # Estas fechas deberían juntarse porque están a menos de 365 dias\n",
    "    # type_concept debería ser 2\n",
    "    (1, \"2020-01-01\", \"2020-12-01\", 2),\n",
    "    # Esta última de la misma persona no\n",
    "    (1, \"2022-01-01\", \"2022-01-01\", 2),\n",
    "    # Estas fechas deberían juntarse porque se pisan\n",
    "    # type_concept debería ser 1\n",
    "    (2, \"2020-01-01\", \"2020-12-01\", 1),\n",
    "    # Estas dos fechas NO deberían juntarse,\n",
    "    # cada uno es su propio periodo\n",
    "    (3, \"2021-01-01\", \"2021-01-01\", 1),\n",
    "    (3, \"2023-02-01\", \"2023-02-01\", 2),\n",
    "    (3, \"2024-03-01\", \"2024-04-01\", 3),\n",
    "    # Se juntarían pero no porque son personas distintas\n",
    "    (4, \"2024-01-01\", \"2024-02-01\", 1),\n",
    "    (5, \"2025-01-01\", \"2025-02-01\", 2),\n",
    "    # Deberían juntarse porque entras ellas hay poca distancia,\n",
    "    # pero si eliminas una de golpe las otras están muy\n",
    "    # separadas y no se juntan.\n",
    "    # type_concept debería ser 2\n",
    "    (6, \"2020-01-01\", \"2023-12-01\", 2),\n",
    "]\n",
    "df_result = pd.DataFrame.from_records(filas, columns=nombre_columnas)\n",
    "df_result[\"start_date\"] = pd.to_datetime(df_result[\"start_date\"])\n",
    "df_result[\"end_date\"] = pd.to_datetime(df_result[\"end_date\"])\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Eliminación de filas cercanas\n",
    "\n",
    "Hay varios problemas\n",
    "- Si te encuentras varias filas que cumplen la condición seguidas, puedes perder información si la primera y la última filas están muy separadas.\n",
    "- No se ha encontrado una manera efectiva de hacer esto sin iterar como antes.\n",
    "    - O bien iteras por personas, y no tienes que vigilar que mezclas personas\n",
    "    - O bien lo haces de golpe, pero es muy complejo llevar la cuenta de los `type_concept` y `person_id` que has eliminado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Usando sólo índices\n",
    "\n",
    "Hay que encontrar la primera y última fila de cada persona y también aquellos casos en los que sólo haya una única fila.\n",
    "\n",
    "Luego hay que buscar también aquellos casos en los que la siguiente fila esté muy alejada, lo que implicaría que hemos encontrado una brecha en el periodo de observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw files\n",
    "df_rare = df_raw.sort_values(\n",
    "    [\"person_id\", \"start_date\", \"end_date\"], ascending=[True, True, False]\n",
    ")\n",
    "# It is VERY important to reset the index to make sure we can\n",
    "# retrieve them realiably after sorting them.\n",
    "df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "# Create index for first, last or only person in dataset\n",
    "df_rare[\"idx_person_first\"] = (\n",
    "    df_rare[\"person_id\"] == df_rare[\"person_id\"].shift(-1)\n",
    ") & (df_rare[\"person_id\"] != df_rare[\"person_id\"].shift(1))\n",
    "df_rare[\"idx_person_last\"] = (\n",
    "    df_rare[\"person_id\"] != df_rare[\"person_id\"].shift(-1)\n",
    ") & (df_rare[\"person_id\"] == df_rare[\"person_id\"].shift(1))\n",
    "df_rare[\"idx_person_only\"] = (\n",
    "    df_rare[\"person_id\"] != df_rare[\"person_id\"].shift(-1)\n",
    ") & (df_rare[\"person_id\"] != df_rare[\"person_id\"].shift(1))\n",
    "# Create index if the break is too big and needs to be kept\n",
    "n_days = 365\n",
    "df_rare[\"next_interval\"] = df_rare[\"start_date\"].shift(-1) - df_rare[\"end_date\"]\n",
    "df_rare[\"idx_interval\"] = df_rare[\"next_interval\"] >= pd.Timedelta(n_days, unit=\"D\")\n",
    "# Combine all to see which rows remain\n",
    "df_rare[\"to_remain\"] = (\n",
    "    df_rare[\"idx_person_first\"]\n",
    "    | df_rare[\"idx_person_last\"]\n",
    "    | df_rare[\"idx_person_only\"]\n",
    "    | df_rare[\"idx_interval\"]\n",
    ")\n",
    "\n",
    "df_rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta gente me tengo que quedar seguro :\n",
    "- `idx_person_first == True`\n",
    "- `idx_person_last == True`\n",
    "- `idx_person_only == True`\n",
    "\n",
    "1. Si para una persona sólo hay 1 `idx_person_only == True`, me quedo ese y a correr. En este caso no hay que hacer nada, esa fila tiene la primera y la última fecha del paciente.\n",
    "\n",
    "2. Si para una persona sólo hay 1 `idx_person_first == True` y 1 `idx_person_last == True`, entonces tengo el principio y el final. \n",
    "    1. Si no hay ningún `idx_interval == True`, junto la `start_date` del `idx_person_first == True` y la `end_date` del `idx_person_last == True`.\n",
    "    2. Si hay algún `idx_interval == True`, tengo que tener en cuenta que esas filas indican brechas en el periodo de observación. La fila donde `idx_interval == True` indica que es la última del periodo y que la siguiente es el comienzo de otro.\n",
    "\n",
    "Básicamente hay que registrar por un lado las `start_date`, con sus respectivos `person_id`, y por otro lado las nuevas `end_date`. Vamos a escribir el código que registra esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create an initial dataframe with only person_id and\n",
    "# start_date. The end_date rows and type_concept will be added\n",
    "# later as new columns.\n",
    "\n",
    "# == start_date and person_id ==========================================\n",
    "# To retrieve the start_date we need the indexes of:\n",
    "# - single day periods (idx_person_only == True)\n",
    "# - first dates (idx_person_first == True)\n",
    "# - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "# Get the person condition indexes\n",
    "idx_start = df_rare.index[\n",
    "    df_rare[\"idx_person_only\"]\n",
    "    | df_rare[\"idx_person_first\"]\n",
    "    | df_rare[\"idx_interval\"].shift(1)\n",
    "]\n",
    "# Get the interval indexes\n",
    "df_done = df_rare.loc[idx_start, [\"person_id\", \"start_date\"]]\n",
    "df_done\n",
    "\n",
    "# == end_date ==========================================================\n",
    "# Get the indexes\n",
    "idx_end = df_rare.index[\n",
    "    df_rare[\"idx_person_only\"] | df_rare[\"idx_person_last\"] | df_rare[\"idx_interval\"]\n",
    "]\n",
    "# Append values found to final dataframe\n",
    "df_done[\"end_date\"] = df_rare.loc[idx_end, [\"end_date\"]].values\n",
    "\n",
    "df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar los `type_concept`, podemos usar los índices del principio y el final, hacer un zip y, como deberían estar en orden. Tendré una lista con las parejas inicio final de cada periodo.\n",
    "\n",
    "Si busco todos los `type_concept` dentro de esos periodos, puedo hacer la moda y asignar el `type_concept` más común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "# I can iterate over idx_start and idx_end to get the\n",
    "# periods\n",
    "mode_values = []\n",
    "for i in np.arange(len(idx_start)):\n",
    "    df_tmp = df_rare.loc[idx_start[i] : idx_end[i]]\n",
    "    print(f\"{i=}\")\n",
    "    print(df_tmp[[\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]])\n",
    "    mode = st.mode(df_tmp[\"type_concept\"].values)\n",
    "    print(f\"mode is {mode}\", \"\\n\")\n",
    "    mode_values.append(mode[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to dataframe\n",
    "df_done[\"type_concept\"] = mode_values\n",
    "df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y listo, ya tengo el dataframe con sólo los inicios y finales de los periodos, incluyendo el type_concept más común calculado usando la moda. Comprobamos que es igual que los resultados esperados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_person = df_done[\"person_id\"].values == df_result[\"person_id\"].values\n",
    "print(f\"{'Person_id col is correct:':<28} {check_person.all()}\")\n",
    "check_start_date = df_done[\"start_date\"].values == df_result[\"start_date\"].values\n",
    "print(f\"{'start_date col is correct:':<28} {check_start_date.all()}\")\n",
    "check_end_date = df_done[\"end_date\"].values == df_result[\"end_date\"].values\n",
    "print(f\"{'end_date col is correct:':<28} {check_end_date.all()}\")\n",
    "check_type_concept = df_done[\"type_concept\"].values == df_result[\"type_concept\"].values\n",
    "print(f\"{'type_concept col is correct:'::<28} {check_type_concept.all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Prueba con función recursive (NOT FINISHED)\n",
    "\n",
    "ESTO ESTÁ AQUÍ PARA FUTURAS REFERENCIAS. EL CÓDIGO NO ESTÁ TERMINADO PORQUE EL MÉTODO POR ÍNDICES FUNCIONA LO SUFICIENTEMENTE BIEN Y NO HAY GARANTÍAS DE QUE ESTO LO MEJORE.\n",
    "\n",
    "Parece que lo mejor (cof) va a ser repetir la estrategia anterior e iterar recursivamente. Así además nos aseguramos que podemos llevar la cuenta de los type_concept y quedarnos con el más representativo.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_neighbors_index(df: pd.DataFrame,\n",
    "#                          n_days: int) -> pd.Series:\n",
    "\n",
    "#     # 1. Check that current and next patient are the same\n",
    "#     idx_person = df.iloc[:, 0] == df.iloc[:, 0].shift(-1)\n",
    "#     # 2. Check that current end_date and next start_date\n",
    "#     # are closer than n_days\n",
    "#     idx_interval = (\n",
    "#         (df.iloc[:, 2] - df.iloc[:, 1].shift(-1)) <=\n",
    "#         pd.Timedelta(n_days, unit='D')\n",
    "#     )\n",
    "#     # 4. If everything past is true, I can drop the row\n",
    "#     return idx_person & idx_interval\n",
    "\n",
    "\n",
    "# def remove_all_neighbors_recursive_v1(\n",
    "#         df: pd.DataFrame,\n",
    "#         n_days: int,\n",
    "#         verbose: int = 0,\n",
    "#         _counter: int = 0,\n",
    "#         _counter_lim: int = 1000) -> pd.DataFrame:\n",
    "\n",
    "#     # Get the rows\n",
    "#     idx_to_remove = find_neighbors_index(df, n_days)\n",
    "#     # Prepare next loop\n",
    "#     idx_to_remove_sum = idx_to_remove.sum()\n",
    "#     _counter += 1\n",
    "#     # If there's still room to go, go\n",
    "#     if (idx_to_remove_sum != 0) and (_counter < _counter_lim):\n",
    "#         if verbose >= 1:\n",
    "#             print(f\"Iter {_counter} => {idx_to_remove_sum} rows removed.\")\n",
    "#         if verbose >= 2:\n",
    "#             print(df[idx_to_remove].head(10))\n",
    "\n",
    "#         # Modify end_dates\n",
    "#         df.iloc[:,2] = np.where(idx_to_remove,\n",
    "#                                 df.iloc[:,2].shift(-1),\n",
    "#                                 df.iloc[:,2])\n",
    "#         return remove_all_neighbors_recursive_v1(\n",
    "#             df[idx_to_remove], verbose, _counter)\n",
    "#     else:\n",
    "#         return df\n",
    "\n",
    "# n_days = 365\n",
    "# df_rare = df_raw.sort_values(\n",
    "#     ['person_id', 'start_date', 'end_date'],\n",
    "#     ascending=[True, True, True])\n",
    "# df_rare = remove_all_neighbors_recursive_v1(df_rare, n_days, verbose=2)\n",
    "# df_done = df_rare.sort_index()\n",
    "# df_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_days = 365\n",
    "# df = df_raw.sort_values(\n",
    "#     ['person_id', 'start_date', 'end_date'],\n",
    "#     ascending=[True, True, True])\n",
    "\n",
    "# # >>> Iter 1\n",
    "# idx_to_remove = find_neighbors_index(df, n_days)\n",
    "# print(f\"Iter {1} => {idx_to_remove.sum()} rows removed.\")\n",
    "# print(df[idx_to_remove])\n",
    "# # <<<\n",
    "\n",
    "# # Record changes\n",
    "# df['to_join'] = idx_to_remove\n",
    "# df['new_end_date'] = np.where(df['to_join'],\n",
    "#                               df.iloc[:, 2].shift(-1),\n",
    "#                               df.iloc[:, 2])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # >>> # Iter 2\n",
    "# df.iloc[:,2] = np.where(idx_to_remove,\n",
    "#                         df.iloc[:,2].shift(-1),\n",
    "#                         df.iloc[:,2])\n",
    "# idx_to_remove = find_neighbors_index(df, n_days)\n",
    "# # <<<\n",
    "\n",
    "# # Record changes\n",
    "# df['to_join'] = idx_to_remove\n",
    "# df['new_end_date'] = np.where(df['to_join'],\n",
    "#                               df.iloc[:, 2].shift(-1),\n",
    "#                               df.iloc[:, 2])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # >>> # Iter 3\n",
    "# df.iloc[:,2] = np.where(idx_to_remove,\n",
    "#                         df.iloc[:,2].shift(-1),\n",
    "#                         df.iloc[:,2])\n",
    "# idx_to_remove = find_neighbors_index(df, n_days)\n",
    "# # <<<\n",
    "\n",
    "# # Record changes\n",
    "# df['to_join'] = idx_to_remove\n",
    "# df['new_end_date'] = np.where(df['to_join'],\n",
    "#                               df.iloc[:, 2].shift(-1),\n",
    "#                               df.iloc[:, 2])\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Todo junto\n",
    "\n",
    "Ahora juntamos todo en una función, para poder medir el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "def find_person_index(df: pd.DataFrame) -> tuple[pd.Series]:\n",
    "    \"\"\"Finds all rows that are contained with the previous\n",
    "    row, making sure they belong to the same person_id.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas Dataframe with at least three columns.\n",
    "        Assumes first column is person_id, second column is\n",
    "        start_date and third column is end_date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pd.Series]\n",
    "        Tuple with three pandas Series with bools:\n",
    "        - idx_person_first, True if first row of the person\n",
    "        - idx_person_last, True if last row of the person\n",
    "        - idx_person_only, True if only row of the person\n",
    "        False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create index for first, last or only person in dataset\n",
    "    idx_person_first = (df.iloc[:, 0] == df.iloc[:, 0].shift(-1)) & (\n",
    "        df.iloc[:, 0] != df.iloc[:, 0].shift(1)\n",
    "    )\n",
    "    idx_person_last = (df.iloc[:, 0] != df.iloc[:, 0].shift(-1)) & (\n",
    "        df.iloc[:, 0] == df.iloc[:, 0].shift(1)\n",
    "    )\n",
    "    idx_person_only = (df.iloc[:, 0] != df.iloc[:, 0].shift(-1)) & (\n",
    "        df.iloc[:, 0] != df.iloc[:, 0].shift(1)\n",
    "    )\n",
    "    return (idx_person_first, idx_person_last, idx_person_only)\n",
    "\n",
    "\n",
    "def group_dates(df: pd.DataFrame, n_days: int) -> pd.DataFrame:\n",
    "    \"\"\"Groups rows of dates from the same person that are less\n",
    "    than n_days apart, keeping only the first start_date and\n",
    "    the last end_date, respectively.\n",
    "\n",
    "    It will remove rows that are partially contained within\n",
    "    the previous one.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        pandas dataframe with at least four columns:\n",
    "        ['person_id', 'start_date', 'end_date', 'type_concept'].\n",
    "        Column names do not need to be the same but, the order\n",
    "        must be the same as here.\n",
    "        This allows its use for different tables with columns\n",
    "        that have the same purpose but different names.\n",
    "    verbose : int, optional\n",
    "        Information output, by default 0\n",
    "        - 0 No info\n",
    "        - 1 Show number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of input dataframe with grouped rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # == Preparation ==============================================\n",
    "    # Sort so we know for sure the order is right\n",
    "    df_rare = df.copy().sort_values(\n",
    "        [df.columns[0], df.columns[1], df.columns[2]], ascending=[True, True, False]\n",
    "    )\n",
    "    # It is VERY important to reset the index to make sure we can\n",
    "    # retrieve them realiably after sorting them.\n",
    "    df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "    # == Index look-up ============================================\n",
    "    (idx_person_first, idx_person_last, idx_person_only) = find_person_index(df_rare)\n",
    "    # Create index if the break is too big and needs to be kept\n",
    "    next_interval = df_rare.iloc[:, 1].shift(-1) - df_rare.iloc[:, 2]\n",
    "    idx_interval = next_interval >= pd.Timedelta(n_days, unit=\"D\")\n",
    "\n",
    "    # == Retrieve relevant rows ===================================\n",
    "    # -- start_date and person_id ---------------------------------\n",
    "    # To retrieve the start_date we need the indexes of:\n",
    "    # - single day periods (idx_person_only == True)\n",
    "    # - first dates (idx_person_first == True)\n",
    "    # - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "    # Get the person condition indexes\n",
    "    idx_start = df_rare.index[\n",
    "        idx_person_only | idx_person_first | idx_interval.shift(1)\n",
    "    ]\n",
    "\n",
    "    # -- end_date -------------------------------------------------\n",
    "    # Get the indexes\n",
    "    idx_end = df_rare.index[idx_person_only | idx_person_last | idx_interval]\n",
    "\n",
    "    # == Compute type_concept =====================================\n",
    "    # Iterate over idx_start and idx_end to get the periods\n",
    "    mode_values = []\n",
    "    for i in np.arange(len(idx_start)):\n",
    "        df_tmp = df_rare.loc[idx_start[i] : idx_end[i]]\n",
    "        mode_values.append(st.mode(df_tmp.iloc[:, 3].values)[0])\n",
    "\n",
    "    # == Build final dataframe ====================================\n",
    "    # Create a copy (.loc) with the first two columns\n",
    "    df_done = df_rare.loc[idx_start, [df.columns[0], df.columns[1]]]\n",
    "    # Append values found to final dataframe\n",
    "    df_done[df.columns[2]] = df_rare.loc[idx_end, [df.columns[2]]].values\n",
    "    # Add to dataframe\n",
    "    df_done[df.columns[3]] = mode_values\n",
    "\n",
    "    return df_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Parametros ==\n",
    "n_days = 365\n",
    "\n",
    "# == Creación de datos ==\n",
    "df_done = group_dates(df_raw, n_days)\n",
    "df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "volvemos a comprobar que sale bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_person = df_done[\"person_id\"].values == df_result[\"person_id\"].values\n",
    "print(f\"{'Person_id col is correct:':<28} {check_person.all()}\")\n",
    "check_start_date = df_done[\"start_date\"].values == df_result[\"start_date\"].values\n",
    "print(f\"{'start_date col is correct:':<28} {check_start_date.all()}\")\n",
    "check_end_date = df_done[\"end_date\"].values == df_result[\"end_date\"].values\n",
    "print(f\"{'end_date col is correct:':<28} {check_end_date.all()}\")\n",
    "check_type_concept = df_done[\"type_concept\"].values == df_result[\"type_concept\"].values\n",
    "print(f\"{'type_concept col is correct:'::<28} {check_type_concept.all()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 10 group_dates(df_rare,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prueba con datasets grandes\n",
    "Vamos a comparar si el metodo de pyarrow sigue funcionando más rápido con datasets grandes.\n",
    "\n",
    "Nos traemos la función para generar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import parquet\n",
    "\n",
    "\n",
    "def create_sample_df(\n",
    "    n: int = 1000,\n",
    "    n_dates: int = 50,\n",
    "    first_date: str = \"2020-01-01\",\n",
    "    last_date: str = \"2023-01-01\",\n",
    "    mean_duration_days: int = 60,\n",
    "    std_duration_days: int = 180,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # == Parameters ==\n",
    "    np.random.seed(42)\n",
    "    pd.options.mode.string_storage = \"pyarrow\"\n",
    "    # Start date from which to start the dates\n",
    "    first_date = pd.to_datetime(first_date)\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    max_days = (last_date - first_date).days\n",
    "\n",
    "    # == Generate IDs randomly ==\n",
    "    # -- Generate the Ids\n",
    "    people = np.random.randint(10000000, 99999999 + 1, size=n)\n",
    "    person_id = np.random.choice(people, n * n_dates)\n",
    "\n",
    "    # == Generate random dates ==\n",
    "    # Generate random integers for days and convert to timedelta\n",
    "    random_days = np.random.randint(0, max_days, size=n * n_dates)\n",
    "    # Create the columns\n",
    "    observation_start_date = first_date + pd.to_timedelta(random_days, unit=\"D\")\n",
    "    # Generate a gaussian sample of dates\n",
    "    random_days = np.random.normal(\n",
    "        mean_duration_days, std_duration_days, size=n * n_dates\n",
    "    )\n",
    "    random_days = np.int32(random_days)\n",
    "    observation_end_date = observation_start_date + pd.to_timedelta(\n",
    "        random_days, unit=\"D\"\n",
    "    )\n",
    "    # Correct end_dates\n",
    "    # => If they are smaller than start_date, take start_date\n",
    "    observation_end_date = np.where(\n",
    "        observation_end_date < observation_start_date,\n",
    "        observation_start_date,\n",
    "        observation_end_date,\n",
    "    )\n",
    "\n",
    "    # == Generate the code ==\n",
    "    period_type_concept_id = np.random.randint(1, 11, size=n * n_dates)\n",
    "\n",
    "    # == Generate the dataframe ==\n",
    "    df_raw = {\n",
    "        \"person_id\": person_id,\n",
    "        \"observation_period_start_date\": observation_start_date,\n",
    "        \"observation_period_end_date\": observation_end_date,\n",
    "        \"period_type_concept_id\": period_type_concept_id,\n",
    "    }\n",
    "    return pd.DataFrame(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos traemos la función de pyarrow tal y como estaba el 12/09/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bps_to_omop.general as gen\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import numpy as np\n",
    "\n",
    "# Añadimos el directorio superior al path para poder extraer\n",
    "# las funciones de las carpetas ETL*\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of func_folder to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "\n",
    "def group_dates_original_pyarrow(table_done, n_days):\n",
    "    # -- Thirdly, group up dates -----------------------------------------------\n",
    "    # Agrupamos las fechas usando group_person_dates(). Básicamente calcula la\n",
    "    # distancia temporal entre las filas adyacentes de cada persona, juntándolas\n",
    "    # si es tan por debajo del límite marcado por n_days.\n",
    "    # Agrupamos\n",
    "    table_OBSERVATION_PERIOD = []\n",
    "\n",
    "    person_list = pc.unique(table_done[\"person_id\"])\n",
    "    # Percentage points where you want to print progress\n",
    "    for i, person in enumerate(person_list[:]):\n",
    "        # --Group person\n",
    "        table_person = group_person_dates(table_done, person, n_days)\n",
    "        # Append table\n",
    "        table_OBSERVATION_PERIOD.append(table_person)\n",
    "    # Concatenate\n",
    "    table_OBSERVATION_PERIOD = pa.concat_tables(table_OBSERVATION_PERIOD)\n",
    "    return table_OBSERVATION_PERIOD\n",
    "\n",
    "\n",
    "def group_person_dates(\n",
    "    table_rare: pa.Table, person: str | int, n_days: int\n",
    ") -> pa.Table:\n",
    "    \"\"\"Filters original table for a specific person and reduces\n",
    "    the amount of date records grouping all records that are separated\n",
    "    by n_days or less.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_rare : pa.Table\n",
    "        Table as prepared by 'prepare_table_raw_to_rare()'.\n",
    "    person : str | int\n",
    "        person id, can be an int (the usual) or a string.\n",
    "    n_days : int\n",
    "        number of maximum days between subsequent records.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pa.Table\n",
    "        Table identical to table_rare but with less date records.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter for the current person_id\n",
    "    filt = pc.is_in(\n",
    "        table_rare[\"person_id\"], pa.array([person])  # pylint: disable=E1101\n",
    "    )\n",
    "    table_person = table_rare.filter(filt)\n",
    "    # Retrieve corresponding dates\n",
    "    start_dates = table_person[\"start_date\"]\n",
    "    end_dates = table_person[\"end_date\"]\n",
    "    # Group dates closer\n",
    "    start_dates, end_dates, _ = group_observation_dates(\n",
    "        start_dates, end_dates, n_days, verbose=False\n",
    "    )\n",
    "    # Create person\n",
    "    person_id = pa_utils.create_uniform_int_array(len(start_dates), value=person)\n",
    "    # Retrieve most common period type\n",
    "    period_type_concept_id = pc.mode(  # pylint: disable=E1101\n",
    "        table_person[\"period_type_concept_id\"]\n",
    "    )[0][0]\n",
    "    period_type_concept_id = pa_utils.create_uniform_int_array(\n",
    "        len(start_dates), value=period_type_concept_id\n",
    "    )\n",
    "    # return table\n",
    "    return pa.Table.from_arrays(\n",
    "        [person_id, start_dates, end_dates, period_type_concept_id],\n",
    "        names=[\"person_id\", \"start_date\", \"end_date\", \"period_type_concept_id\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def group_observation_dates(\n",
    "    start_dates: pa.Array, end_dates: pa.Array, n_days: int, verbose: bool = False\n",
    ") -> tuple[pa.Array, pa.Array, None | pa.Table]:\n",
    "    \"\"\"Given a pair of 'start_dates' and 'end_dates', it will\n",
    "    compute the days between each 'end_date' and the next\n",
    "    'start_date' and remove dates that are smaller that a\n",
    "    given number of days ('n_days').\n",
    "n_days = 365\n",
    "\n",
    "# Cargamos los datos\n",
    "df_raw = create_sample_df(n=100)\n",
    "df_raw.columns = ['person_id', 'start_date', 'end_date', 'type_concept']\n",
    "\n",
    "df_raw = df_raw.sort_values(\n",
    "    ['person_id', 'start_date', 'end_date', 'type_concept'],\n",
    "    ascending=[True, True, False, True])\n",
    "df_rare = df_raw.reset_index(drop=True).copy()\n",
    "\n",
    "print('\\nshift:')\n",
    "%timeit -n 1 -r 1 group_dates(df_rare,n_days)\n",
    "\n",
    "df_rare = df_raw.reset_index(drop=True).copy()\n",
    "df_rare.columns = ['person_id', 'start_date', 'end_date', 'period_type_concept_id']\n",
    "table_rare = pa.Table.from_pandas(df_rare,preserve_index=False)\n",
    "print('\\npyarrow:')\n",
    "%timeit -n 1 -r 1 group_dates_original_pyarrow(table_rare,n_days)\n",
    "    The new dates will only contain start and end dates that have\n",
    "    more than 'n_days' of difference between them.\n",
    "\n",
    "    If dates contain nans/nulls, they will be ignored and grouped\n",
    "    with the closest dates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_dates : pa.Array\n",
    "        Array of start dates\n",
    "    end_dates : pa.Array\n",
    "        Array of end dates\n",
    "    n_days : int\n",
    "        _description_\n",
    "    verbose : bool, optional\n",
    "        _description_, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pa.Array, pa.Array, None | pa.Table]\n",
    "        Always return a 3-item tuple.\n",
    "        First item is reduced start dates.\n",
    "        Second item is reduced end dates.\n",
    "        Third item is None if verbose=True,\n",
    "        if verbose=False, is table with start_dates,\n",
    "        end_dates and days between them. Usefull when\n",
    "        verifying dates.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        The resulting starting dates should always come before\n",
    "        their corresponding end dates. Return an AssertionError\n",
    "        otherwise.\n",
    "    \"\"\"\n",
    "    # Get an array of end_dates, taking away the last one\n",
    "    # (last date cannot be compared to the next start date)\n",
    "    from_dates = end_dates[:-1]\n",
    "    # Get an array of start_dates, taking away the first one\n",
    "    # (first date cannot be compared to the previous end date)\n",
    "    to_dates = start_dates[1:]\n",
    "\n",
    "    # -- Compute days between\n",
    "    intervals = pc.days_between(from_dates, to_dates).to_numpy(  # pylint: disable=E1101\n",
    "        zero_copy_only=False\n",
    "    )\n",
    "    # Create an inner table for the calculations if verbose\n",
    "    inner_table = None\n",
    "    if verbose:\n",
    "        inner_table = pa.Table.from_arrays(\n",
    "            [start_dates, end_dates, pa.array(np.append(intervals, np.nan))],\n",
    "            names=[\"start\", \"end\", \"intervals\"],\n",
    "        )\n",
    "\n",
    "    # Filter intervals under some assumption\n",
    "    filt = intervals >= n_days\n",
    "    # => When this filt is 'true', it means that for that index,\n",
    "    # let's call it 'idx', between the end date of 'idx' and the start\n",
    "    # date of 'idx+1' there more than 'n_days' days.\n",
    "    # i.e.:\n",
    "    # (start_date[idx+1] - end_date[idx]).days > n_days\n",
    "\n",
    "    # if no interval is greater, take the first and last rows\n",
    "    if np.nansum(filt) == 0:\n",
    "        idx_end_dates = np.array([len(intervals)])\n",
    "        # Sum 1 to get start dates\n",
    "        idx_start_dates = np.array([0])\n",
    "\n",
    "    # If some filters exist take those\n",
    "    else:\n",
    "        # Get indexes of end_dates\n",
    "        idx_end_dates = filt.nonzero()[0]\n",
    "        # Sum 1 to get corresponding start dates\n",
    "        idx_start_dates = idx_end_dates + 1\n",
    "        # Append last entry as last end_date\n",
    "        idx_end_dates = np.append(idx_end_dates, len(intervals))\n",
    "        # Append first entry as first start_date\n",
    "        idx_start_dates = np.append(0, idx_start_dates)\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(f'{idx_start_dates=}')\n",
    "    #     print(f'{idx_end_dates=}')\n",
    "\n",
    "    # Make sure all end values are after start values\n",
    "    new_start = start_dates.take(idx_start_dates)\n",
    "    new_end = end_dates.take(idx_end_dates)\n",
    "    if pc.any(pc.less(new_end, new_start)).as_py():  # pylint: disable=E1101\n",
    "        if verbose:\n",
    "            print(f\"{start_dates=}\", f\"{end_dates=}\")\n",
    "            print(f\"{new_start=}\", f\"{new_end=}\")\n",
    "        raise AssertionError(\n",
    "            \"Some end dates happen before start dates. Try sorting the original data.\"\n",
    "        )\n",
    "\n",
    "    return (new_start, new_end, inner_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Sanity check\n",
    "### DATA CREATION\n",
    "\n",
    "Probamos primero que los resultados sean iguales con ambas funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "\n",
    "# Cargamos los datos\n",
    "df_raw = create_sample_df(\n",
    "    n=100,\n",
    "    n_dates=10,\n",
    ")\n",
    "df_raw.columns = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "\n",
    "df_raw = df_raw.sort_values(\n",
    "    [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"],\n",
    "    ascending=[True, True, False, True],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how quick `remove_overlap()` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 10 remove_overlap(df_raw,0,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite quick!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove contained dates\n",
    "df_raw = remove_overlap(df_raw, 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[df_raw[\"person_id\"] == 10271836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[df_raw[\"person_id\"] == 23315092]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYARROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == pyarrow method ==\n",
    "df_rare = df_raw.copy()\n",
    "df_rare.columns = [\"person_id\", \"start_date\", \"end_date\", \"period_type_concept_id\"]\n",
    "table_rare = pa.Table.from_pandas(df_rare, preserve_index=False)\n",
    "table_done = group_dates_original_pyarrow(table_rare, n_days)\n",
    "df_done_pyarrow = table_done.to_pandas()\n",
    "df_done_pyarrow = df_done_pyarrow.sort_values(\n",
    "    [\"person_id\", \"start_date\", \"end_date\", \"period_type_concept_id\"],\n",
    "    ascending=[True, True, False, True],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_pyarrow[df_done_pyarrow[\"person_id\"] == 10271836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_pyarrow[df_done_pyarrow[\"person_id\"] == 23315092]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyarrow hace la primera persona (10271836), todas las fechas tienen menos de 365 días entre sí, así que se unen en una sola. Las siguientes son todas de una única fecha por persona hasta 23315092, que tiene dos. Esta también la hace bien.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Shift method ==\n",
    "df_rare = df_raw.copy()\n",
    "df_done_shift = group_dates(df_rare, n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_shift[df_done_shift[\"person_id\"] == 10271836]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done_shift[df_done_shift[\"person_id\"] == 23315092]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el método shift hace bien la primera persona (10271836) la que tiene dos periodos (23315092). El type_concept cambia del método pyarrow al shift, pero me fio más del shift en este momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Time measurement\n",
    "Ahora probamos a medir el tiempo que tarda cada uno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "\n",
    "# Cargamos los datos\n",
    "df_raw = create_sample_df(n=100)\n",
    "df_raw.columns = ['person_id', 'start_date', 'end_date', 'type_concept']\n",
    "\n",
    "df_raw = df_raw.sort_values(\n",
    "    ['person_id', 'start_date', 'end_date', 'type_concept'],\n",
    "    ascending=[True, True, False, True])\n",
    "df_rare = df_raw.reset_index(drop=True).copy()\n",
    "\n",
    "print('\\nshift:')\n",
    "%timeit -n 1 -r 1 group_dates(df_rare,n_days)\n",
    "\n",
    "df_rare = df_raw.reset_index(drop=True).copy()\n",
    "df_rare.columns = ['person_id', 'start_date', 'end_date', 'period_type_concept_id']\n",
    "table_rare = pa.Table.from_pandas(df_rare,preserve_index=False)\n",
    "print('\\npyarrow:')\n",
    "%timeit -n 1 -r 1 group_dates_original_pyarrow(table_rare,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para n = 1000\n",
    "\n",
    "    shift:\n",
    "    375 ms ± 884 μs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n",
    "    pyarrow:\n",
    "    454 ms ± 664 μs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n",
    "\n",
    "Para n = 10000\n",
    "\n",
    "    shift:\n",
    "    3.69 s ± 4.92 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "    pyarrow:\n",
    "    23.6 s ± 565 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "\n",
    "Para n = 30000\n",
    "\n",
    "    shift:\n",
    "    10.2 s ± 18.5 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n",
    "    pyarrow:\n",
    "    3min 10s ± 5.35 s per loop (mean ± std. dev. of 2 runs, 2 loops each)\n",
    " \n",
    "\n",
    "Ahora está bastante claro que el método shift funciona mucho más rápido si tenemos muchas personas. Al final en el método original estamos pegando tablas una encima de otra, lo cual resta mucho tiempo. Y esto teniendo en cuenta que el método shift está ordenando dentro de la propia función, cosa que en el de pyarrow dejamos fuera.\n",
    "\n",
    "Quizá si se pudiera implementar con pyarrow un modo siguiendo el patrón de los shift, se podría conseguir algo mejor. Con todo, los 10 s con 30000 paciente y 50 fechas por paciente ya me parece un buen resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba timestamp vs datetime\n",
    "\n",
    "Nos hemos encontrado que el código remove_overlap va mucho más rápido si las fechas están en formato timestamp (pa.timestamp('us)) que si están en datetime (pa.date64()).\n",
    "\n",
    "El problema está en que los datos finales en el proyecto `sarscov` no coinciden si se usa un método o el otro.\n",
    "\n",
    "Probamos a lanzar el código aquí para comprobarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = create_sample_df(n=1000)\n",
    "df_raw.columns = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "df_rare = df_raw.loc[:,['person_id','start_date','end_date','type_concept']]\n",
    "table_raw = pa.Table.from_pandas(df_rare, preserve_index=False)\n",
    "table_raw = table_raw.cast(\n",
    "    pa.schema([\n",
    "        ('person_id', pa.int64()),\n",
    "        ('start_date', pa.timestamp('us')),\n",
    "        ('end_date', pa.timestamp('us')),\n",
    "        ('type_concept', pa.int64()),\n",
    "    ])\n",
    ")\n",
    "df_rare = table_raw.to_pandas()\n",
    "remove_overlap(df_rare,2).info()\n",
    "\n",
    "%timeit -n 10 -r 10 group_dates_original_pyarrow(table_rare,n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 365\n",
    "df_rare = df_raw.loc[:,['person_id','start_date','end_date','type_concept']]\n",
    "table_raw = pa.Table.from_pandas(df_rare, preserve_index=False)\n",
    "table_raw = table_raw.cast(\n",
    "    pa.schema([\n",
    "        ('person_id', pa.int64()),\n",
    "        ('start_date', pa.date64()),\n",
    "        ('end_date', pa.date64()),\n",
    "        ('type_concept', pa.int64()),\n",
    "    ])\n",
    ")\n",
    "df_rare = table_raw.to_pandas()\n",
    "remove_overlap(df_rare,2).info()\n",
    "\n",
    "%timeit -n 10 -r 10 group_dates_original_pyarrow(table_rare,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambos formatos funcionan bien, dejando el mismo número de filas.\n",
    "\n",
    "Puede que el problema venga de que algunas fechas en los datos del proyecto vienen con hora. Por ejemplo, todas las de farmacia de dispensación. Si paso estos registros a date64 pierdo la información de la hora, por lo que el orden puede que sea distinto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Nuevo método para calcular type_concept\n",
    "\n",
    "Vamos a comparar el método actual de group_dates con hacer groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from bps_to_omop.general import group_dates, find_person_index\n",
    "\n",
    "\n",
    "def create_sample_data():\n",
    "    nombre_columnas = [\"person_id\", \"start_date\", \"end_date\", \"type_concept\"]\n",
    "    n_days = 365\n",
    "    df_in = [\n",
    "        # Una única fecha\n",
    "        (1, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        # Dos fechas que se juntan con type_concept iguales\n",
    "        (2, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (2, \"2020-03-01\", \"2020-04-01\", 1),\n",
    "        # Dos fechas que se juntan con type_concept distintos\n",
    "        (3, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (3, \"2020-03-01\", \"2020-04-01\", 2),\n",
    "        # tres fechas que se juntan\n",
    "        (4, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (4, \"2020-03-01\", \"2020-04-01\", 1),\n",
    "        (4, \"2020-05-01\", \"2020-12-01\", 2),\n",
    "        # una persona con dos grupos distintos\n",
    "        (5, \"2020-01-01\", \"2020-02-01\", 1),\n",
    "        (5, \"2020-03-01\", \"2020-04-01\", 1),\n",
    "        (5, \"2020-05-01\", \"2020-12-01\", 2),\n",
    "        (5, \"2022-01-01\", \"2022-02-01\", 3),\n",
    "        (5, \"2022-03-01\", \"2022-04-01\", 3),\n",
    "        (5, \"2022-05-01\", \"2022-12-01\", 2),\n",
    "    ]\n",
    "    df_in = pd.DataFrame.from_records(df_in, columns=nombre_columnas).assign(\n",
    "        start_date=lambda x: pd.to_datetime(x[\"start_date\"]),\n",
    "        end_date=lambda x: pd.to_datetime(x[\"end_date\"]),\n",
    "    )\n",
    "    return df_in\n",
    "\n",
    "\n",
    "def group_dates_v2(df: pd.DataFrame, n_days: int, verbose: int = 0) -> pd.DataFrame:\n",
    "    # == Preparation ==============================================\n",
    "    if verbose > 0:\n",
    "        print(\"Grouping dates:\")\n",
    "        print(\"- Sorting and preparing data...\")\n",
    "    # Sort so we know for sure the order is right\n",
    "    df_rare = df.copy().sort_values(\n",
    "        [df.columns[0], df.columns[1], df.columns[2]], ascending=[True, True, False]\n",
    "    )\n",
    "    # It is VERY important to reset the index to make sure we can\n",
    "    # retrieve them realiably after sorting them.\n",
    "    df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "    # == Index look-up ============================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Looking up indexes...\")\n",
    "    (idx_person_first, idx_person_last, idx_person_only) = find_person_index(df_rare)\n",
    "    # Create index if the break is too big and needs to be kept\n",
    "    next_interval = df_rare.iloc[:, 1].shift(-1) - df_rare.iloc[:, 2]\n",
    "    idx_interval = next_interval >= pd.Timedelta(n_days, unit=\"D\")\n",
    "\n",
    "    # == Retrieve relevant rows ===================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Retrieving rows...\")\n",
    "    # -- start_date and person_id ---------------------------------\n",
    "    # To retrieve the start_date we need the indexes of:\n",
    "    # - single day periods (idx_person_only == True)\n",
    "    # - first dates (idx_person_first == True)\n",
    "    # - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "    # Get the person condition indexes\n",
    "    idx_start = df_rare.index[\n",
    "        idx_person_only | idx_person_first | idx_interval.shift(1)\n",
    "    ]\n",
    "\n",
    "    # -- end_date -------------------------------------------------\n",
    "    # Get the indexes\n",
    "    idx_end = df_rare.index[idx_person_only | idx_person_last | idx_interval]\n",
    "\n",
    "    # == Compute type_concept =====================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Computing type_concept...\")\n",
    "    # Iterate over idx_start and idx_end to get the periods\n",
    "    mode_values = []\n",
    "    for i in np.arange(len(idx_start)):\n",
    "        df_tmp = df_rare.loc[idx_start[i] : idx_end[i]]\n",
    "        mode_values.append(st.mode(df_tmp.iloc[:, 3].values)[0])\n",
    "\n",
    "        if (verbose > 1) and ((i) % int(len(idx_start) / 4) == 0):\n",
    "            print(f\"  - ({(i+1)/len(idx_start)*100:.1f} %) {(i+1)}/{len(idx_start)}\")\n",
    "    if verbose > 1:\n",
    "        print(f\"  - (100.0 %) {len(idx_start)}/{len(idx_start)}\")\n",
    "\n",
    "    # == Build final dataframe ====================================\n",
    "    if verbose > 0:\n",
    "        print(\"- Closing up...\")\n",
    "    # Create a copy (.loc) with the first two columns\n",
    "    df_done = df_rare.loc[idx_start, [df.columns[0], df.columns[1]]]\n",
    "    # Append values found to final dataframe\n",
    "    df_done[df.columns[2]] = df_rare.loc[idx_end, [df.columns[2]]].values\n",
    "    # Add to dataframe\n",
    "    df_done[df.columns[3]] = mode_values\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(\"- Done!\")\n",
    "    return df_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "n_days = 365\n",
    "\n",
    "df = create_sample_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(!!)**\n",
    "\n",
    "La idea aquí es que estamos buscando los índice y construyendo manualmente un dataframe con las fechas iniciales y finales.\n",
    "\n",
    "NO podemos usar el truco del groupby para el type concept directamente, ya que no sabemos los intervalos finales.\n",
    "\n",
    "Es decir, podemos agrupar por person_id, pero habría que agrupar también por las fechas, para poder sacar para cada persona y cada observation_period, cuál es el type_concept más frecuente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Preparation ==============================================\n",
    "if verbose > 0:\n",
    "    print(\"Grouping dates:\")\n",
    "    print(\"- Sorting and preparing data...\")\n",
    "# Sort so we know for sure the order is right\n",
    "df_rare = df.copy().sort_values(\n",
    "    [df.columns[0], df.columns[1], df.columns[2]], ascending=[True, True, False]\n",
    ")\n",
    "# It is VERY important to reset the index to make sure we can\n",
    "# retrieve them realiably after sorting them.\n",
    "df_rare = df_rare.reset_index(drop=True)\n",
    "\n",
    "# == Index look-up ============================================\n",
    "if verbose > 0:\n",
    "    print(\"- Looking up indexes...\")\n",
    "(idx_person_first, idx_person_last, idx_person_only) = find_person_index(df_rare)\n",
    "# Create index if the break is too big and needs to be kept\n",
    "next_interval = df_rare.iloc[:, 1].shift(-1) - df_rare.iloc[:, 2]\n",
    "idx_interval = next_interval >= pd.Timedelta(n_days, unit=\"D\")\n",
    "\n",
    "# == Retrieve relevant rows ===================================\n",
    "if verbose > 0:\n",
    "    print(\"- Retrieving rows...\")\n",
    "# -- start_date and person_id ---------------------------------\n",
    "# To retrieve the start_date we need the indexes of:\n",
    "# - single day periods (idx_person_only == True)\n",
    "# - first dates (idx_person_first == True)\n",
    "# - Rows just after period breaks, (idx_interval.index + 1)\n",
    "\n",
    "# Get the person condition indexes\n",
    "idx_start = df_rare.index[idx_person_only | idx_person_first | idx_interval.shift(1)]\n",
    "\n",
    "# -- end_date -------------------------------------------------\n",
    "# Get the indexes\n",
    "idx_end = df_rare.index[idx_person_only | idx_person_last | idx_interval]\n",
    "\n",
    "# == Compute type_concept =====================================\n",
    "if verbose > 0:\n",
    "    print(\"- Computing type_concept...\")\n",
    "# Iterate over idx_start and idx_end to get the periods\n",
    "mode_values = []\n",
    "for i in np.arange(len(idx_start)):\n",
    "    df_tmp = df_rare.loc[idx_start[i] : idx_end[i]]\n",
    "    mode_values.append(st.mode(df_tmp.iloc[:, 3].values)[0])\n",
    "\n",
    "    if (verbose > 1) and ((i) % int(len(idx_start) / 4) == 0):\n",
    "        print(f\"  - ({(i+1)/len(idx_start)*100:.1f} %) {(i+1)}/{len(idx_start)}\")\n",
    "if verbose > 1:\n",
    "    print(f\"  - (100.0 %) {len(idx_start)}/{len(idx_start)}\")\n",
    "\n",
    "# == Build final dataframe ====================================\n",
    "if verbose > 0:\n",
    "    print(\"- Closing up...\")\n",
    "# Create a copy (.loc) with the first two columns\n",
    "df_done = df_rare.loc[idx_start, [df.columns[0], df.columns[1]]]\n",
    "# Append values found to final dataframe\n",
    "df_done[df.columns[2]] = df_rare.loc[idx_end, [df.columns[2]]].values\n",
    "# # Add to dataframe\n",
    "df_done[df.columns[3]] = mode_values\n",
    "\n",
    "if verbose > 0:\n",
    "    print(\"- Done!\")\n",
    "df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Unfinished testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def assign_groups_masking(\n",
    "    indices: np.ndarray, starts: np.ndarray, ends: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Group assignment using boolean masking\"\"\"\n",
    "    group_ids = np.zeros(len(indices), dtype=int)\n",
    "    for group_num, (start, end) in enumerate(zip(starts, ends), 1):\n",
    "        mask = (indices >= start) & (indices <= end)\n",
    "        group_ids[mask] = group_num\n",
    "    return group_ids\n",
    "\n",
    "\n",
    "def assign_groups_searchsorted(\n",
    "    indices: np.ndarray, starts: np.ndarray, ends: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Group assignment using searchsorted\"\"\"\n",
    "    boundaries = np.sort(np.concatenate([starts, ends + 1]))\n",
    "    return np.searchsorted(boundaries, indices, side=\"right\") // 2\n",
    "\n",
    "\n",
    "def generate_test_case(n_rows: int, n_groups: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate test data with given size and number of groups\"\"\"\n",
    "    # Create roughly equal-sized groups\n",
    "    group_size = n_rows // n_groups\n",
    "    starts = np.arange(0, n_rows, group_size)\n",
    "    ends = starts + group_size - 1\n",
    "    ends[-1] = n_rows - 1  # Adjust last group\n",
    "    return starts, ends\n",
    "\n",
    "\n",
    "def run_benchmark():\n",
    "    # Test configurations\n",
    "    row_sizes = [10_000, 100_000, 1_000_000]\n",
    "    group_configs = [\n",
    "        (\"Few Large Groups\", lambda x: max(5, x // 1_000_000)),\n",
    "        (\"Medium Groups\", lambda x: max(50, x // 100_000)),\n",
    "        (\"Many Small Groups\", lambda x: max(500, x // 10_000)),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for n_rows in row_sizes:\n",
    "        indices = np.arange(n_rows)\n",
    "\n",
    "        for group_desc, group_func in group_configs:\n",
    "            n_groups = group_func(n_rows)\n",
    "            starts, ends = generate_test_case(n_rows, n_groups)\n",
    "\n",
    "            # Warm-up run\n",
    "            _ = assign_groups_masking(indices, starts, ends)\n",
    "            _ = assign_groups_searchsorted(indices, starts, ends)\n",
    "\n",
    "            # Timing masking approach\n",
    "            start_time = time.perf_counter()\n",
    "            for _ in range(5):  # Multiple runs for more stable results\n",
    "                _ = assign_groups_masking(indices, starts, ends)\n",
    "            masking_time = (time.perf_counter() - start_time) / 5\n",
    "\n",
    "            # Timing searchsorted approach\n",
    "            start_time = time.perf_counter()\n",
    "            for _ in range(5):\n",
    "                _ = assign_groups_searchsorted(indices, starts, ends)\n",
    "            searchsorted_time = (time.perf_counter() - start_time) / 5\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"Rows\": n_rows,\n",
    "                    \"Groups\": n_groups,\n",
    "                    \"Configuration\": group_desc,\n",
    "                    \"Masking Time\": masking_time,\n",
    "                    \"Searchsorted Time\": searchsorted_time,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run benchmark\n",
    "results_df = run_benchmark()\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nDetailed Benchmark Results:\")\n",
    "print(\"=\" * 80)\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"\\nConfiguration: {row['Configuration']}\")\n",
    "    print(f\"Data Size: {row['Rows']:,} rows, {row['Groups']:,} groups\")\n",
    "    print(f\"Masking Time: {row['Masking Time']*1000:.2f}ms\")\n",
    "    print(f\"Searchsorted Time: {row['Searchsorted Time']*1000:.2f}ms\")\n",
    "    speedup = row[\"Masking Time\"] / row[\"Searchsorted Time\"]\n",
    "    faster_method = \"searchsorted\" if speedup > 1 else \"masking\"\n",
    "    print(\n",
    "        f\"Winner: {faster_method} ({abs(speedup):,.2f}x {'faster' if speedup > 1 else 'slower'})\"\n",
    "    )\n",
    "\n",
    "# Calculate and print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "for config in results_df[\"Configuration\"].unique():\n",
    "    config_results = results_df[results_df[\"Configuration\"] == config]\n",
    "    print(f\"\\n{config}:\")\n",
    "    avg_speedup = (\n",
    "        config_results[\"Masking Time\"] / config_results[\"Searchsorted Time\"]\n",
    "    ).mean()\n",
    "    print(f\"Average speedup using searchsorted: {avg_speedup:.2f}x\")\n",
    "\n",
    "# Validation of correctness\n",
    "print(\"\\nValidating correctness of implementations...\")\n",
    "test_indices = np.arange(1000)\n",
    "test_starts = np.array([0, 200, 400, 600, 800])\n",
    "test_ends = np.array([199, 399, 599, 799, 999])\n",
    "\n",
    "masking_results = assign_groups_masking(test_indices, test_starts, test_ends)\n",
    "searchsorted_results = assign_groups_searchsorted(test_indices, test_starts, test_ends)\n",
    "\n",
    "if np.array_equal(masking_results, searchsorted_results):\n",
    "    print(\"✓ Both implementations produce identical results\")\n",
    "else:\n",
    "    print(\"⚠ WARNING: Implementations produce different results!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
