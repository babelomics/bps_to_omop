{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas con OBSERVATION_TABLE simulada\n",
    "\n",
    "Aquí vamos a simular una pequeña base de datos con fechas de pacientes para ver si es más rápido trabajar con pyarrow o con pandas.\n",
    "\n",
    "TO DO:\n",
    "- Rehacer el método shift (v2) sin iterar sobre personas y sin reguardar datos. Hay que basarse en el método de VISIT_OCCURRENCE.\n",
    "- Try with [numba](https://pandas.pydata.org/docs/user_guide/enhancingperf.html#enhancingperf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la base de datos de mentira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import parquet\n",
    "\n",
    "def create_sample_df(n:int = 1000,n_dates:int=50,\n",
    "                     first_date:str='2020-01-01',\n",
    "                     last_date:str='2023-01-01',\n",
    "                     mean_duration_days:int=60,\n",
    "                     std_duration_days:int=180)->pd.DataFrame:\n",
    "\n",
    "    # == Parameters ==    \n",
    "    np.random.seed(42)\n",
    "    pd.options.mode.string_storage = \"pyarrow\"\n",
    "    # Start date from which to start the dates\n",
    "    first_date = pd.to_datetime(first_date)\n",
    "    last_date = pd.to_datetime(last_date)\n",
    "    max_days = (last_date-first_date).days\n",
    "\n",
    "    # == Generate IDs randomly ==\n",
    "    # -- Generate the Ids\n",
    "    people = np.random.randint(10000000, 99999999 + 1, size=n)\n",
    "    person_id = np.random.choice(people,n*n_dates)\n",
    "\n",
    "    # == Generate random dates ==\n",
    "    # Generate random integers for days and convert to timedelta\n",
    "    random_days = np.random.randint(0, max_days, size=n*n_dates)\n",
    "    # Create the columns\n",
    "    observation_start_date = first_date + pd.to_timedelta(random_days, unit='D')\n",
    "    # Generate a gaussian sample of dates\n",
    "    random_days = np.random.normal(mean_duration_days, std_duration_days, size=n*n_dates)\n",
    "    random_days = np.int32(random_days)\n",
    "    observation_end_date = observation_start_date + pd.to_timedelta(random_days, unit='D')\n",
    "    # Correct end_dates\n",
    "    # => If they are smaller than start_date, take start_date\n",
    "    observation_end_date = np.where(observation_end_date<observation_start_date,\n",
    "                                    observation_start_date,observation_end_date)\n",
    "    \n",
    "\n",
    "    # == Generate the code ==\n",
    "    period_type_concept_id = np.random.randint(1, 11, size=n*n_dates)\n",
    "\n",
    "    # == Generate the dataframe ==\n",
    "    df_raw = {'person_id':person_id,'observation_period_start_date':observation_start_date,\n",
    "            'observation_period_end_date':observation_end_date,'period_type_concept_id':period_type_concept_id}\n",
    "    return pd.DataFrame(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación entre métodos\n",
    "\n",
    "**06/09/2024:** Por ahora va ganando la implementación de pyarrow. \n",
    "* **pyarrow** Default hasta ahora.\n",
    "    * => 475 ms ± 5.34 ms per loop (mean ± std. dev. of 10 runs, 5 loops each)\n",
    "* **pandas v1:** Copia pyarrow pero en pandas\n",
    "    * => 793 ms ± 8.65 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "* **pandas v2** Shifting but iterating by person\n",
    "    * => 3.82 s ± 19.6 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "* **pandas v3** Shifting pero sin iterar por persona, operando siempre sobre el mismo dataframe. \n",
    "    * => 4 ms ± 129 µs per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "* **Dask** \n",
    "    * => No ha habido manera de que eso lance. Se basa en usar shift, que es mucho más lento, así que tiene pinta de que no va a ser posible hacerlo por ahí.\n",
    "\n",
    "Parece que lo lento no son los cálculos, si no lo de encadenar los dataframes o las tablas. Haciendo los shifts podemos hacer el cálculo 2 órdenes de magnitud más rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando tablas pyarrow\n",
    "#### v1 - Original, iterando sobre personas\n",
    "Nos traemos las funciones que usaremos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import dask\n",
    "# from dask.distributed import Client\n",
    "\n",
    "\n",
    "def create_uniform_int_array(\n",
    "        length: int,\n",
    "        value: int = 0) -> pa.array:\n",
    "    \"\"\"Create an uniform array with a specific length\n",
    "\n",
    "    By default is an array of zeroes, can be modified\n",
    "    defining a integer value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int\n",
    "        length of the array.\n",
    "    value : int, optional\n",
    "        Value that fills the array, by default 0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pa.array\n",
    "        pyarrow array with int32 datatype.\n",
    "    \"\"\"\n",
    "    # creamos un array de zeros con numpy y\n",
    "    # lo pasamos a pyarrow forzando int32\n",
    "    zeros = pa.array(np.zeros(shape=length), pa.int32())\n",
    "    if value == 0:\n",
    "        return zeros\n",
    "    else:\n",
    "        # Sumamos la cantidad que sea\n",
    "        return pc.add(zeros, value)  # pylint: disable=E1101\n",
    "\n",
    "\n",
    "def group_observation_dates(\n",
    "        start_dates: pa.Array,\n",
    "        end_dates: pa.Array,\n",
    "        n_days: int,\n",
    "        verbose: bool = False) -> tuple[pa.Array, pa.Array, None | pa.Table]:\n",
    "    \"\"\"Given a pair of 'start_dates' and 'end_dates', it will\n",
    "    compute the days between each 'end_date' and the next\n",
    "    'start_date' and remove dates that are smaller that a\n",
    "    given number of days ('n_days').\n",
    "\n",
    "    The new dates will only contain start and end dates that have\n",
    "    more than 'n_days' of difference between them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_dates : pa.Array\n",
    "        Array of start dates\n",
    "    end_dates : pa.Array\n",
    "        Array of end dates\n",
    "    n_days : int\n",
    "        _description_\n",
    "    verbose : bool, optional\n",
    "        _description_, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pa.Array, pa.Array, None | pa.Table]\n",
    "        Always return a 3-item tuple.\n",
    "        First item is reduced start dates.\n",
    "        Second item is reduced end dates.\n",
    "        Third item is None if verbose=True,\n",
    "        if verbose=False, is table with start_dates,\n",
    "        end_dates and days between them. Usefull when\n",
    "        verifying dates.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        The resulting starting dates should always come before\n",
    "        their corresponding end dates. Return an AssertionError\n",
    "        otherwise.\n",
    "    \"\"\"\n",
    "    # Get an array of end_dates, taking away the last one\n",
    "    # (last date cannot be compared to the next start date)\n",
    "    from_dates = end_dates[:-1]\n",
    "    # Get an array of start_dates, taking away the first one\n",
    "    # (first date cannot be compared to the previous end date)\n",
    "    to_dates = start_dates[1:]\n",
    "\n",
    "    # -- Compute days between\n",
    "    intervals = pc.days_between(  # pylint: disable=E1101\n",
    "        from_dates, to_dates).to_numpy()\n",
    "    # Create an inner table for the calculations if verbose\n",
    "    inner_table = None\n",
    "    if verbose:\n",
    "        inner_table = pa.Table.from_arrays(\n",
    "            [start_dates, end_dates, pa.array(np.append(intervals, np.nan))],\n",
    "            names=['start', 'end', 'intervals']).to_pandas()\n",
    "\n",
    "    # Filter intervals under some assumption\n",
    "    filt = intervals >= n_days\n",
    "    # => When this filt is 'true', it means that for that index,\n",
    "    # let's call it 'idx', between the end date of 'idx' and the start\n",
    "    # date of 'idx+1' there more than 'n_days' days.\n",
    "    # i.e.:\n",
    "    # (start_date[idx+1] - end_date[idx]).days > n_days\n",
    "\n",
    "    # if no interval is greater, take the first and last rows\n",
    "    if np.nansum(filt) == 0:\n",
    "        idx_end_dates = np.array([len(intervals)])\n",
    "        # Sum 1 to get start dates\n",
    "        idx_start_dates = np.array([0])\n",
    "\n",
    "    # If some filters exist take those\n",
    "    else:\n",
    "        # Get indexes of end_dates\n",
    "        idx_end_dates = filt.nonzero()[0]\n",
    "        # Sum 1 to get corresponding start dates\n",
    "        idx_start_dates = idx_end_dates+1\n",
    "        # Append last entry as last end_date\n",
    "        idx_end_dates = np.append(idx_end_dates, len(intervals))\n",
    "        # Append first entry as first start_date\n",
    "        idx_start_dates = np.append(0, idx_start_dates)\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(f'{idx_start_dates=}')\n",
    "    #     print(f'{idx_end_dates=}')\n",
    "\n",
    "    # Make sure all end values are after start values\n",
    "    new_start = start_dates.take(idx_start_dates)\n",
    "    new_end = end_dates.take(idx_end_dates)\n",
    "    if pc.any(pc.less(new_end, new_start)).as_py():  # pylint: disable=E1101\n",
    "        if verbose:\n",
    "            print(f\"{start_dates=}\", f\"{end_dates=}\")\n",
    "            print(f\"{new_start=}\", f\"{new_end=}\")\n",
    "        raise AssertionError(\n",
    "            'Some end dates happen before start dates. Try sorting the original data.')\n",
    "\n",
    "    return (new_start, new_end, inner_table)\n",
    "\n",
    "\n",
    "def group_person_dates(\n",
    "        table_rare: pa.Table,\n",
    "        person: str | int,\n",
    "        n_days: int) -> pa.Table:\n",
    "    \"\"\"Filters original table for a specific person and reduces\n",
    "    the amount of date records grouping all records that are separated\n",
    "    by n_days or less.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_rare : pa.Table\n",
    "        Table as prepared by 'prepare_table_raw_to_rare()'.\n",
    "    person : str | int\n",
    "        person id, can be an int (the usual) or a string.\n",
    "    n_days : int\n",
    "        number of maximum days between subsequent records.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pa.Table\n",
    "        Table identical to table_rare but with less date records.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter for the current person_id\n",
    "    filt = pc.is_in(table_rare['person_id'],  # pylint: disable=E1101\n",
    "                    pa.array([person]))\n",
    "    table_person = table_rare.filter(filt)\n",
    "    # Retrieve corresponding dates\n",
    "    start_dates = table_person['observation_period_start_date']\n",
    "    end_dates = table_person['observation_period_end_date']\n",
    "    # Group dates closer\n",
    "    start_dates, end_dates, _ = group_observation_dates(\n",
    "        start_dates, end_dates, n_days, verbose=False)\n",
    "    # Create person\n",
    "    person_id = create_uniform_int_array(len(start_dates),\n",
    "                                         value=person)\n",
    "    # Retrieve most common period type\n",
    "    period_type_concept_id = pc.mode(  # pylint: disable=E1101\n",
    "        table_person['period_type_concept_id'])[0][0]\n",
    "    period_type_concept_id = create_uniform_int_array(len(start_dates),\n",
    "                                                      value=period_type_concept_id)\n",
    "    # return table\n",
    "    return pa.Table.from_arrays(\n",
    "        [person_id, start_dates, end_dates, period_type_concept_id],\n",
    "        names=['person_id', 'observation_period_start_date',\n",
    "               'observation_period_end_date', 'period_type_concept_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "df_raw = create_sample_df()\n",
    "df_raw = df_raw.sort_values(['person_id','observation_period_start_date'])\n",
    "table_pa = pa.Table.from_pandas(df_raw)\n",
    "n_days = 60\n",
    "def serial_grouping():\n",
    "    table_person = []\n",
    "    for person in table_pa['person_id'].unique():\n",
    "        tmp = group_person_dates(table_pa,person,n_days)\n",
    "        table_person.append(tmp)\n",
    "    return table_person\n",
    "\n",
    "%timeit -n 5 -r 10 serial_grouping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v2 - Sin iterar sobre personas\n",
    "Probamos ahora a intentar simplemente buscar los índices que nos convengan, emulando la idea de v3 con pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == NOT FINISHED == #\n",
    "def serial_grouping_v2(table_pa,n_days):\n",
    "    lbl_0 = 'person_id'\n",
    "    lbl_1 = 'observation_period_start_date'\n",
    "    lbl_2 = 'observation_period_end_date'\n",
    "    lbl_3 = 'period_type_concept_id'\n",
    "\n",
    "    # Create necessary columns to do the calculations\n",
    "    df_rare.loc[:,'previous_end_date'] = df_rare[lbl_2].shift(1)\n",
    "    df_rare.loc[:,'previous_interval'] = (\n",
    "        df_rare['previous_end_date']-df_rare[lbl_1]).fillna(pd.Timedelta(n_days*10, 'D'))\n",
    "    # If previous_interval is under required n_days, it means the interval is small enough\n",
    "    df_rare['idx_person'] = df_rare['person_id'] == df_rare['person_id'].shift(1)\n",
    "    df_rare['idx_interval'] = df_rare['previous_interval'] < pd.Timedelta(n_days,unit='D')\n",
    "    df_rare['to_remove'] = df_rare['idx_interval'] & df_rare['idx_person']\n",
    "    return df_rare[df_rare['to_remove']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "n_days = 60\n",
    "df_raw = create_sample_df()\n",
    "df_raw = df_raw.sort_values(['person_id','observation_period_start_date'])\n",
    "table_pa = pa.Table.from_pandas(df_raw)\n",
    "%timeit -n 10 -r 5 serial_grouping_v2(table_pa,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intentamos paralelizar pyarrow con dask.delayed\n",
    "\n",
    "**25/07/2024**: No hay cojones de que funcione con una tabla de pyarrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Client(n_workers=4)\n",
    "\n",
    "\n",
    "# @dask.delayed\n",
    "# def delayed_filter(table, person):\n",
    "#     filt = pc.is_in(table['person_id'], pa.array([person]))\n",
    "#     table_person = table.filter(filt)\n",
    "#     return table_person\n",
    "\n",
    "\n",
    "# @dask.delayed\n",
    "# def delayed_group_observation_dates(start_dates, end_dates, n_days):\n",
    "#     return group_observation_dates(start_dates, end_dates, n_days)\n",
    "\n",
    "\n",
    "# def table_reduced_explicit():\n",
    "#     table_reduced = []\n",
    "#     for person in table_pa['person_id'].unique():\n",
    "#         # ============================================ #\n",
    "#         # Filter for the current person_id\n",
    "#         table_person = delayed_filter(table_pa, person)\n",
    "#         # Retrieve corresponding dates\n",
    "#         start_dates = table_person['observation_period_start_date']\n",
    "#         end_dates = table_person['observation_period_end_date']\n",
    "#         # Group dates closer\n",
    "#         start_dates, end_dates, _ = group_observation_dates(\n",
    "#             start_dates, end_dates, n_days)\n",
    "#         # Create person\n",
    "#         person_id = create_uniform_int_array(len(start_dates),\n",
    "#                                              value=person)\n",
    "#         # Retrieve most common period type\n",
    "#         period_type_concept_id = pc.mode(  # pylint: disable=E1101\n",
    "#             table_person['period_type_concept_id'])[0][0]\n",
    "#         period_type_concept_id = create_uniform_int_array(len(start_dates),\n",
    "#                                                           value=period_type_concept_id)\n",
    "#         table_person = pa.Table.from_arrays(\n",
    "#             [person_id, start_dates, end_dates, period_type_concept_id],\n",
    "#             names=['person_id', 'observation_period_start_date',\n",
    "#                    'observation_period_end_date', 'period_type_concept_id'])\n",
    "#         # ============================================ #\n",
    "\n",
    "#         table_reduced.append(table_person)\n",
    "#     return table_reduced\n",
    "\n",
    "# table_reduced_explicit = pa.concat_tables(table_reduced_explicit)\n",
    "# %timeit serial_grouping_explicit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = table_reduced_explicit() # NO FUNCIONA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando pandas.dataframes\n",
    "\n",
    "#### v1\n",
    "Vamos a probar usando simplemente pandas. Hay que corregir la función para intentar usar el método shift, como se ha hecho con dask, en lugar de hacer indexación explícita. Quizá así es más rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefinimos la función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_observation_dates_df(\n",
    "        start_dates: pd.Series,\n",
    "        end_dates: pd.Series,\n",
    "        n_days: int,\n",
    "        verbose: bool = False) -> tuple[pd.Series, pd.Series, None | pd.DataFrame]:\n",
    "    \"\"\"Given a pair of 'start_dates' and 'end_dates', it will\n",
    "    compute the days between each 'end_date' and the next\n",
    "    'start_date' and remove dates that are smaller that a\n",
    "    given number of days ('n_days').\n",
    "\n",
    "    The new dates will only contain start and end dates that have\n",
    "    more than 'n_days' of difference between them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_dates : pd.Series\n",
    "        Array of start dates\n",
    "    end_dates : pd.Series\n",
    "        Array of end dates\n",
    "    n_days : int\n",
    "        _description_\n",
    "    verbose : bool, optional\n",
    "        _description_, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pd.Series, pd.Series, None | pd.DataFrame]\n",
    "        Always return a 3-item tuple.\n",
    "        First item is reduced start dates.\n",
    "        Second item is reduced end dates.\n",
    "        Third item is None if verbose=True,\n",
    "        if verbose=False, is table with start_dates,\n",
    "        end_dates and days between them. Usefull when\n",
    "        verifying dates.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        The resulting starting dates should always come before\n",
    "        their corresponding end dates. Return an AssertionError\n",
    "        otherwise.\n",
    "    \"\"\"\n",
    "    # Get an array of end_dates, taking away the last one\n",
    "    # (last date cannot be compared to the next start date)\n",
    "    from_dates = end_dates[:-1]\n",
    "    # Get an array of start_dates, taking away the first one\n",
    "    # (first date cannot be compared to the previous end date)\n",
    "    to_dates = start_dates[1:]\n",
    "    intervals = np.int64((to_dates-from_dates)/1e9/3600/24)\n",
    "    # Filter intervals under some assumption\n",
    "    filt = intervals >= n_days\n",
    "\n",
    "    # => When this filt is 'true', it means that for that index,\n",
    "    # let's call it 'idx', between the end date of 'idx' and the start\n",
    "    # date of 'idx+1' there more than 'n_days' days.\n",
    "    # i.e.:\n",
    "    # (start_date[idx+1] - end_date[idx]).days > n_days\n",
    "\n",
    "    # if no interval is greater, take the first and last rows\n",
    "    if np.nansum(filt) == 0:\n",
    "        idx_end_dates = np.array([len(intervals)])\n",
    "        # Sum 1 to get start dates\n",
    "        idx_start_dates = np.array([0])\n",
    "\n",
    "    # If some filters exist take those\n",
    "    else:\n",
    "        # Get indexes of end_dates\n",
    "        idx_end_dates = filt.nonzero()[0]\n",
    "        # Sum 1 to get corresponding start dates\n",
    "        idx_start_dates = idx_end_dates+1\n",
    "        # Append last entry as last end_date\n",
    "        idx_end_dates = np.append(idx_end_dates, len(intervals))\n",
    "        # Append first entry as first start_date\n",
    "        idx_start_dates = np.append(0, idx_start_dates)\n",
    "\n",
    "    # Make sure all end values are after start values\n",
    "    new_start = start_dates.take(idx_start_dates)\n",
    "    new_end = end_dates.take(idx_end_dates)\n",
    "    intervals = np.int64(new_end-new_start)\n",
    "    if np.any(intervals < 0):\n",
    "        if verbose:\n",
    "            print(f\"{start_dates=}\", f\"{end_dates=}\")\n",
    "            print(f\"{new_start=}\", f\"{new_end=}\")\n",
    "        raise AssertionError(\n",
    "            'Some end dates happen before start dates. Try sorting the original data.')\n",
    "\n",
    "    return (new_start, new_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_table_df_reduced(df_raw,n_days):\n",
    "    table_reduced = []\n",
    "    for person in df_raw['person_id'].unique():\n",
    "        # Filter for the current person_id\n",
    "        filt = df_raw['person_id'] == person\n",
    "        table_person = df_raw[filt]\n",
    "        # Group the dates\n",
    "        start_dates = table_person['observation_period_start_date'].values\n",
    "        end_dates = table_person['observation_period_end_date'].values\n",
    "        start_dates, end_dates = group_observation_dates_df(\n",
    "            start_dates, end_dates, n_days)\n",
    "\n",
    "        # Create person\n",
    "        person_id = np.array([person]*len(start_dates))\n",
    "        # Retrieve most common period type\n",
    "        period_type_concept_id = table_person['period_type_concept_id'].mode()[\n",
    "            0]\n",
    "        period_type_concept_id = np.array(\n",
    "            [period_type_concept_id]*len(start_dates))\n",
    "        table_person = pd.DataFrame({'person_id': person_id,\n",
    "                                     'observation_period_start_date': start_dates,\n",
    "                                     'observation_period_end_date': end_dates,\n",
    "                                     'period_type_concept_id': period_type_concept_id})\n",
    "\n",
    "        table_reduced.append(table_person)\n",
    "    return table_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "n_days = 60\n",
    "df_raw = create_sample_df()\n",
    "df_raw = df_raw.sort_values(['person_id','observation_period_start_date'])\n",
    "%timeit -n 10 -r 5 serial_table_df_reduced(df_raw,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v2\n",
    "\n",
    "Vamos a probar haciendo los cálculos extendiendo los dataframes, como tuvimos que hacer con dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_table_df_reduced_v2(df_raw,n_days):\n",
    "    lbl_0 = 'person_id'\n",
    "    lbl_1 = 'observation_period_start_date'\n",
    "    lbl_2 = 'observation_period_end_date'\n",
    "    lbl_4 = 'period_type_concept_id'\n",
    "    table_reduced = []\n",
    "    for person in df_raw[lbl_0].unique():\n",
    "        # Filter for the current person_id\n",
    "        filt = df_raw[lbl_0] == person\n",
    "        table_person = df_raw[filt]\n",
    "\n",
    "        # Create necessary columns to do the calculations\n",
    "        table_person.loc[:,'previous_end_date'] = table_person[lbl_2].shift(1)\n",
    "        table_person.loc[:,'previous_interval'] = (\n",
    "            table_person[lbl_1]-table_person['previous_end_date']).fillna(pd.Timedelta(n_days*10, 'D'))\n",
    "        table_person.loc[:,'next_start_date'] = table_person[lbl_1].shift(-1)\n",
    "        table_person.loc[:,'next_interval'] = (\n",
    "            table_person['next_start_date']-table_person[lbl_2]).fillna(pd.Timedelta(n_days*10, 'D'))\n",
    "\n",
    "        # Filter out significant start_days\n",
    "        new_start_idx = table_person['previous_interval'] > pd.Timedelta(10, 'D')\n",
    "        new_start_dates = table_person.loc[new_start_idx, lbl_1]\n",
    "        new_start_dates = new_start_dates.reset_index(drop=True)\n",
    "        # Filter out significant end_days\n",
    "        new_end_idx = table_person['next_interval'] > pd.Timedelta(10, 'D')\n",
    "        new_end_dates = table_person.loc[new_end_idx, lbl_2]\n",
    "        new_end_dates = new_end_dates.reset_index(drop=True)\n",
    "        # Start new table\n",
    "        new_table_person = pd.concat([new_start_dates, new_end_dates], axis=1)\n",
    "        \n",
    "        # Make sure all end values are after start values\n",
    "        # check = (new_table_person.iloc[:,1]-new_table_person.iloc[:,0]) < pd.Timedelta(0,'D')\n",
    "        # if check.all().compute():\n",
    "        #     raise AssertionError(\n",
    "        #         'Some end dates happen before start dates. Try sorting the original data.')\n",
    "\n",
    "        # Create person\n",
    "        new_table_person.loc[:,lbl_0] = person\n",
    "        # Retrieve most common period type\n",
    "        new_table_person.loc[:,lbl_4] = table_person[lbl_4].mode().values[0]\n",
    "        new_table_person = new_table_person[[lbl_0, lbl_1, lbl_2, lbl_4]]\n",
    "        table_reduced.append(new_table_person)\n",
    "    return table_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True\n",
    "# Cargamos los datos\n",
    "n_days = 60\n",
    "df_raw = create_sample_df()\n",
    "df_raw = df_raw.sort_values(['person_id','observation_period_start_date'])\n",
    "%timeit -n 10 -r 5 serial_table_df_reduced_v2(df_raw,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v3 (shift sin cribar por persona)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serial_table_df_reduced_v3(df_raw,n_days):\n",
    "    lbl_0 = 'person_id'\n",
    "    lbl_1 = 'observation_period_start_date'\n",
    "    lbl_2 = 'observation_period_end_date'\n",
    "    lbl_3 = 'period_type_concept_id'\n",
    "    n_personas = 100\n",
    "    n_fechas = 10\n",
    "    first_date = '2020-01-01'\n",
    "    last_date = '2022-01-01'\n",
    "    mean_duration_days = 60\n",
    "    std_duration_days = 60*2\n",
    "\n",
    "    df_rare = df_raw.copy()\n",
    "    df_rare = df_rare.reset_index(drop=True)\n",
    "    # Create necessary columns to do the calculations\n",
    "    df_rare.loc[:,'previous_end_date'] = df_rare[lbl_2].shift(1)\n",
    "    df_rare.loc[:,'previous_interval'] = (\n",
    "        df_rare['previous_end_date']-df_rare[lbl_1]).fillna(pd.Timedelta(n_days*10, 'D'))\n",
    "    # If previous_interval is under required n_days, it means the interval is small enough\n",
    "    df_rare['idx_person'] = df_rare['person_id'] == df_rare['person_id'].shift(1)\n",
    "    df_rare['idx_interval'] = df_rare['previous_interval'] < pd.Timedelta(n_days,unit='D')\n",
    "    df_rare['to_remove'] = df_rare['idx_interval'] & df_rare['idx_person']\n",
    "    return df_rare[df_rare['to_remove']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "n_days = 60\n",
    "df_raw = create_sample_df()\n",
    "df_raw = df_raw.sort_values(['person_id','observation_period_start_date'])\n",
    "%timeit -n 10 -r 5 serial_table_df_reduced_v3(df_raw,n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminando overlapping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_0 = 'person_id'\n",
    "lbl_1 = 'observation_period_start_date'\n",
    "lbl_2 = 'observation_period_end_date'\n",
    "lbl_3 = 'period_type_concept_id'\n",
    "n = 50\n",
    "n_days = 300\n",
    "first_date = '2020-01-01'\n",
    "last_date = '2022-01-01'\n",
    "mean_duration_days = 60\n",
    "std_duration_days = 365*3\n",
    "df_raw = create_sample_df(n,n_days,first_date,last_date,mean_duration_days,std_duration_days)\n",
    "\n",
    "df_rare = df_raw.copy()\n",
    "df_rare = df_rare.sort_values([lbl_0,lbl_1,lbl_2,lbl_3],ascending=[True,True,False,True])\n",
    "\n",
    "def find_overlap_index(df: pd.DataFrame,\n",
    "                       person_lbl,start_lbl,end_lbl) -> pd.Series:\n",
    "    # 1. Primero compruebo que el paciente anterior sea el mismo\n",
    "    idx_person = df[person_lbl] == df[person_lbl].shift(1)\n",
    "    # 2. Compruebo si la start_date actual es menor que la anterior\n",
    "    idx_start = df[start_lbl] >= df[start_lbl].shift(1)\n",
    "    # 3. Compruebo que la end_date actual es mayor que la anterior\n",
    "    idx_end = df[end_lbl] <= df[end_lbl].shift(1)\n",
    "    # 4. Si todo lo anterior es True, puedo tirarlo porque se cumplen todos los requisitos.\n",
    "    return idx_start & idx_end & idx_person\n",
    "\n",
    "def remove_all_overlap(df: pd.DataFrame,\n",
    "                       counter_lim: int = 1000,\n",
    "                       verbose: bool = False) -> pd.DataFrame:\n",
    "\n",
    "    # Copy the dataframe\n",
    "    df_tmp = df.copy()\n",
    "    cols_to_show = [lbl_0,lbl_1,lbl_2]\n",
    "    # Prepare the while loop\n",
    "    idx_to_remove_sum = 1\n",
    "    counter = 0\n",
    "    if verbose:\n",
    "        print('Cleaning...')\n",
    "    # Start the loop\n",
    "    while (idx_to_remove_sum > 0) and (counter <= counter_lim):\n",
    "        # Get the rows\n",
    "        idx_to_remove = find_overlap_index(df_tmp,lbl_0,lbl_1,lbl_2)\n",
    "        # Prepare next loop\n",
    "        idx_to_remove_sum = idx_to_remove.sum()\n",
    "        counter += 1\n",
    "        # Print the statements\n",
    "        if verbose:\n",
    "            print(f\"{counter} => {idx_to_remove_sum} rows removed. Example:\")\n",
    "        # Show info of first case as an example\n",
    "        if verbose & (idx_to_remove_sum > 0):\n",
    "            idx_first_true = idx_to_remove.idxmax()\n",
    "            print(df_tmp.loc[[idx_first_true-1, idx_first_true], cols_to_show])\n",
    "        # Remove the overlapping rows\n",
    "        df_tmp = df_tmp.loc[~idx_to_remove].reset_index(drop=True)\n",
    "\n",
    "    return df_tmp.reset_index(drop=True)\n",
    "\n",
    "%timeit -n 10 -r 5 remove_all_overlap(df_rare,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando dask.dataframes\n",
    " \n",
    "Vamos a intentar reproducir el código que hemos hecho con pyarrow usando dask.dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# n_days = 60\n",
    "\n",
    "# # Primer leemos como dask.dataframe\n",
    "# table_dd = dd.read_parquet('OBSERVATION_TABLE.parquet')\n",
    "# table_dd = table_dd.sort_values('observation_period_start_date')\n",
    "# table_dd.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos encontramos varios problemas al intentar usar dask:\n",
    "* Al cargar los dataframes de manera lazy, no podemos acceder fácilmente a las filas específicas.\n",
    "    * Es decir, no podemos indexar por fila con iloc[], por ejemplo, por lo que calcular las diferencias entre fechas no se puede hacer al momento.\n",
    "    * Vamos a tener que generar un dataframe previo modificado que tenga las fechas en el orden que queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sacamos los datos de una persona concreta para probar las nuevas columnas\n",
    "# person = 62827729\n",
    "# n_days = 10\n",
    "# filt = table_dd['person_id'] == person\n",
    "# table_person = table_dd[filt]\n",
    "# table_person['previous_end_date'] = table_person['observation_period_end_date'].shift(\n",
    "#     1)\n",
    "# table_person['previous_interval'] = (table_person['observation_period_start_date'] -\n",
    "#                                      table_person['previous_end_date']).fillna(pd.Timedelta(n_days*10, 'D'))\n",
    "# table_person['next_start_date'] = table_person['observation_period_start_date'].shift(\n",
    "#     -1)\n",
    "# table_person['next_interval'] = (table_person['next_start_date'] -\n",
    "#                                  table_person['observation_period_end_date']).fillna(pd.Timedelta(n_days*10, 'D'))\n",
    "# table_person.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta tabla podemos ver las relaciones de cada fila con las filas inmediatemente anteriores y posteriores. Se han calculado los intervalos de tiempos hasta las siguientes fechas, de modo que se pueda ver la distancia temporal entre ellas y compararla con el número de días mínimo requerido (*n_days*).\n",
    "\n",
    "* Si *previous_interval* > *n_days* -> Entonces la distancia entre el final de la interacción anterior y el comienzo de la actual es significativa. \n",
    "    * La start_date de la actual fila debe guardarse como *new_start_date*.\n",
    "* Si *next_interval* > *n_days* -> Entonces la distancia entre el final de la interacción actual y el comienzo de la siguiente es significativa.\n",
    "    * La end_date de la actual fila debe guardarse como *new_end_date*.\n",
    "* La primera y la última fecha siempre tendrán un nan, ya que no tienen con quien compararse. Hemos llenado los nan con n_days*10 para asegurar que siempre cumplen el criterio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hacemos el filtrado en función de del número de días\n",
    "# new_start_idx = table_person['previous_interval'] > pd.Timedelta(10, 'D')\n",
    "# print(new_start_idx.compute(), '\\n\\n== ==\\n')\n",
    "# new_start_dates = table_person.loc[new_start_idx,\n",
    "#                                    'observation_period_start_date']\n",
    "# print(new_start_dates.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hacemos el filtrado en función de del número de días\n",
    "# new_end_idx = table_person['next_interval'] > pd.Timedelta(10, 'D')\n",
    "# print(new_end_idx.compute(), '\\n\\n== ==\\n')\n",
    "# new_end_dates = table_person.loc[new_end_idx, 'observation_period_end_date']\n",
    "# print(new_end_dates.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creamos un nuevo dataframe con estoy  comprobamos que las distancias son correctas\n",
    "# new_table_person = dd.concat([new_start_dates.reset_index(\n",
    "#     drop=True), new_end_dates.reset_index(drop=True)], axis=1)\n",
    "# check = (new_table_person.iloc[:, 1] -\n",
    "#          new_table_person.iloc[:, 0]) < pd.Timedelta(0, 'D')\n",
    "# check.all().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create person\n",
    "# new_table_person['person_id'] = person\n",
    "# new_table_person['period_type_concept_id'] = table_person['period_type_concept_id'].mode(\n",
    "# ).values.compute()[0]\n",
    "# new_table_person = new_table_person[['person_id', 'observation_period_start_date',\n",
    "#                                      'observation_period_end_date', 'period_type_concept_id']]\n",
    "# new_table_person.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "\n",
    "# def serial_table_dd_reduced():\n",
    "#     table_reduced = []\n",
    "#     for person in table_dd['person_id'].unique():\n",
    "#         # Filter for the current person_id\n",
    "#         filt = table_dd['person_id'] == person\n",
    "#         table_person = table_dd[filt]\n",
    "\n",
    "#         # Create necessary columns to do the calculations\n",
    "#         table_person['previous_end_date'] = table_person['observation_period_end_date'].shift(\n",
    "#             1)\n",
    "#         table_person['previous_interval'] = (\n",
    "#             table_person['observation_period_start_date']-table_person['previous_end_date']).fillna(pd.Timedelta(n_days*10, 'D'))\n",
    "#         table_person['next_start_date'] = table_person['observation_period_start_date'].shift(\n",
    "#             -1)\n",
    "#         table_person['next_interval'] = (\n",
    "#             table_person['next_start_date']-table_person['observation_period_end_date']).fillna(pd.Timedelta(n_days*10, 'D'))\n",
    "\n",
    "#         # Filter out significant start_days\n",
    "#         new_start_idx = table_person['previous_interval'] > pd.Timedelta(\n",
    "#             10, 'D')\n",
    "#         new_start_dates = table_person.loc[new_start_idx,\n",
    "#                                            'observation_period_start_date']\n",
    "#         # Filter out significant end_days\n",
    "#         new_end_idx = table_person['next_interval'] > pd.Timedelta(10, 'D')\n",
    "#         new_end_dates = table_person.loc[new_end_idx,\n",
    "#                                          'observation_period_end_date']\n",
    "\n",
    "#         # Make sure all end values are after start values\n",
    "#         new_table_person = dd.concat([new_start_dates.reset_index(\n",
    "#             drop=True), new_end_dates.reset_index(drop=True)], axis=1)\n",
    "#         # check = (new_table_person.iloc[:,1]-new_table_person.iloc[:,0]) < pd.Timedelta(0,'D')\n",
    "#         # if check.all().compute():\n",
    "#         #     raise AssertionError(\n",
    "#         #         'Some end dates happen before start dates. Try sorting the original data.')\n",
    "\n",
    "#         # Create person\n",
    "#         new_table_person['person_id'] = person\n",
    "#         # Retrieve most common period type\n",
    "#         new_table_person['period_type_concept_id'] = table_person['period_type_concept_id'].mode(\n",
    "#         ).values.compute()[0]\n",
    "#         new_table_person = new_table_person[['person_id', 'observation_period_start_date',\n",
    "#                                              'observation_period_end_date', 'period_type_concept_id']]\n",
    "#         table_reduced.append(new_table_person)\n",
    "#     table_reduced = dd.concat(table_reduced)\n",
    "#     return table_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = serial_table_dd_reduced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_reduced = results.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_reduced.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto \"\"funciona\"\", pero es lentísimo. Probablemente porque no está en absoluto optimizado y dask tiene que estar pegando trozos de la misma person_id. Habría que ver cómo decirle que debe trabajar con todos los trozos de un person_id de un tirón, sin hacer particiones.\n",
    "\n",
    "Prueba a lanzarlo sólo para una persona, a ver qué pasa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intentamos paralelizar\n",
    "Esto funciona?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.delayed\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# @dask.delayed\n",
    "# def pick_person(table_dd, person):\n",
    "#     # Filter for the current person_id\n",
    "#     filt = table_dd['person_id'] == person\n",
    "#     return table_dd[filt]\n",
    "\n",
    "\n",
    "# @dask.delayed\n",
    "# def compute_intervals(table_person):\n",
    "#     tmp = table_person\n",
    "#     # Create necessary columns to do the calculations\n",
    "#     tmp['previous_end_date'] = tmp['observation_period_end_date'].shift(1)\n",
    "#     tmp['previous_interval'] = (tmp['observation_period_start_date'] -\n",
    "#                                 tmp['previous_end_date']).fillna(pd.Timedelta(n_days*10, 'D'))\n",
    "#     tmp['next_start_date'] = tmp['observation_period_start_date'].shift(-1)\n",
    "#     tmp['next_interval'] = (\n",
    "#         tmp['next_start_date']-tmp['observation_period_end_date']).fillna(pd.Timedelta(n_days*10, 'D'))\n",
    "#     return tmp\n",
    "\n",
    "\n",
    "# @dask.delayed\n",
    "# def compute_new_dates(table_person, n_days):\n",
    "#     tmp = table_person\n",
    "#     new_start_idx = tmp['previous_interval'] > pd.Timedelta(n_days, 'D')\n",
    "#     new_start_idx = np.array(new_start_idx)\n",
    "#     new_start_dates = tmp.loc[new_start_idx, 'observation_period_start_date']\n",
    "#     # Filter out significant end_days\n",
    "#     new_end_idx = tmp['next_interval'] > pd.Timedelta(n_days, 'D')\n",
    "#     new_end_idx = np.array(new_end_idx)\n",
    "#     new_end_dates = tmp.loc[new_end_idx.compute(),\n",
    "#                             'observation_period_end_date']\n",
    "#     return (new_start_dates, new_end_dates)\n",
    "# # new_start_dates, new_end_dates = ompute_new_dates(table_person, n_days)\n",
    "\n",
    "\n",
    "# # == Main Loop ==\n",
    "# n_days = 10\n",
    "# table_reduced = []\n",
    "# for person in table_dd['person_id'].unique():\n",
    "#     # Filter for the current person_id\n",
    "#     table_person = pick_person(table_dd, person)\n",
    "#     table_person = table_person.persist()\n",
    "\n",
    "#     # Create necessary columns to do the calculations\n",
    "#     table_person = compute_intervals(table_person)\n",
    "\n",
    "#     # Filter out significant start_days\n",
    "#     new_start_dates, new_end_dates = compute_new_dates(table_person, n_days)\n",
    "\n",
    "#     # new_start_dates = dd.Series(new_start_dates.reset_index(drop=True))\n",
    "#     # new_end_dates = new_end_dates.reset_index(drop=True)\n",
    "#     # Make sure all end values are after start values\n",
    "#     # new_table_person = dd.concat([new_start_dates, new_end_dates], axis=1)\n",
    "#     # check = (new_table_person.iloc[:,1]-new_table_person.iloc[:,0]) < pd.Timedelta(0,'D')\n",
    "#     # if check.all().compute():\n",
    "#     #     raise AssertionError(\n",
    "#     #         'Some end dates happen before start dates. Try sorting the original data.')\n",
    "\n",
    "#     # # Create person\n",
    "#     # new_table_person['person_id'] = person\n",
    "#     # # Retrieve most common period type\n",
    "#     # new_table_person['period_type_concept_id'] = 1\n",
    "#     # new_table_person = new_table_person[['person_id', 'observation_period_start_date',\n",
    "#     #                                      'observation_period_end_date', 'period_type_concept_id']]\n",
    "#     # table_reduced.append(new_table_person)\n",
    "# # table_reduced = dd.concat(table_reduced)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_bps_omop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
