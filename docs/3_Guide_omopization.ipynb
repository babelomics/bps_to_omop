{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omopization\n",
    "\n",
    "Once the files have been preprocessed, we can move on to prepare them for omopization. Some general modifications will be done to adapt the data to the omop format. These files are still the original files (after process_rare_files stage), but with changes into column names or extra columns. The actions that will be performed are:\n",
    "\n",
    "- Rename of the column that identifies the patient to 'person_id', as in OMOP.\n",
    "- Rename of date columns to a general 'start_date', 'end_date'. \n",
    "  - Most omop tables have a start_date and end_date, preceeding by a str that identifies the table. We will create those columns for each file. If only one date is present, it will be duplicated as both 'start_date' and 'end_date'.\n",
    "  - This ensures any future process does not have to deal with specific names of each file.\n",
    "- Assign a type_concept in relation to the origin of the information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "An example script that will launch the omopization is [omopization](../examples/omopization.py).\n",
    "\n",
    "This script will:\n",
    "1. Load parameters from the parameters file\n",
    "2. Iterate over the `input_files`:\n",
    "   1. Load the table\n",
    "      - Will remove a pesky '__index_level_0__' column that sometimes appear when moving from pandas to pyarrow, just in case. \n",
    "   2. Generate the person_id column. See `transform_person_id()` in module [person.py](../bps_to_omop/person.py)\n",
    "   3. Find the columns with dates.\n",
    "      - Find which column goes first and last and rename them to start_date and end_date, respectively.\n",
    "      - Will throw an error if order is not consistent or if there are more than three columns with dates.\n",
    "      - See `find_start_end_dates()` in module [extract.py](../bps_to_omop/extract.py)\n",
    "   4. Assign a type_concept column with a code that describes the origin of the information.\n",
    "      - This code will be appear in all omop tables, depending on where each registry ends up.\n",
    "   5. Rearrange columns to an intermediate scheme similar to omop.\n",
    "   6. Saves the new file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file generates the folders needed to store the OMOP tables. An example can be found in [omopization_params.yaml](../examples/omopization_params.yaml). To work it requires the configuration file in the following format:\n",
    "\n",
    " ```yaml\n",
    "input_dir: /path/to/input_dir/ # Common path where all input_files are located\n",
    "output_dir: /path/to/output_dir/ # Common path where all output_files will be located\n",
    "input_files:  \n",
    "  # Path to each file from input_dir\n",
    "  - /path/tp/file_1\n",
    "  - /path/tp/file_2\n",
    "person_columns:\n",
    "  # For each file, name of the column that contains the patient id\n",
    "  file_1: NUHSA_ENCRIPTADO\n",
    "  file_2: NUHSA_ENCRIPTADO\n",
    "date_columns:\n",
    "  # For each file, name of the column or columns that contains the dates\n",
    "  file_1:\n",
    "  - FECHA_INICIO\n",
    "  file_2:\n",
    "  - COD_FEC_INI_DIAGNOSTICO\n",
    "type_concept_mapping:\n",
    "  # For each file, omop code that represent the origin of the information\n",
    "  # See https://github.com/OHDSI/Vocabulary-v5.0/wiki/Vocab.-TYPE_CONCEPT\n",
    "  file_1: 32817\n",
    "  file_2: 32840\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to automatically generate params\n",
    "\n",
    "In this section we will automatically search all files to generate the params file. Take into account this are all dummy variables, and proper configuration is needed for it work.\n",
    "\n",
    "We need to provide:\n",
    "\n",
    "- `params_file` is a path to the parameters file. \n",
    "- `input_dir` is a str that defines the folder where raw data is.\n",
    "- `output_dir` is a str that defines the folder where output is going to be saved.\n",
    "- `input_files` is a list with all files to be processed. They can be filenames or relative paths from `input_dir`.\n",
    "- `str_checklist` is a list with substring that will be used to identify date columns. Anything containing any of these strings will be considered a date column.\n",
    "\n",
    "Take into account that the parameters `input_dir` and `output_dir` are defined in relation to the `data_dir` folder defined in the `.env` file. This way the general location of the files can remain hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from package.datasets import data_dir  \n",
    "# It is advisable to provide the data_dir as a module. load_dotenv can be used too.\n",
    "\n",
    "sys.path.append(\"../external/bps_to_omop/\")  # Make sure the path is right!\n",
    "import bps_to_omop.extract as ext\n",
    "\n",
    "# == Define parameters ======================================================\n",
    "params_file = \"../package/preomop/omopization_params.yaml\"\n",
    "os.remove(params_file)\n",
    "\n",
    "input_dir = \"rare/02_cleaned/\"\n",
    "input_files = [\"01_sociodemo.parquet\", \"02_Patologias_BPS.parquet\", \"03_MPA.parquet\"]\n",
    "output_dir = \"done/\"\n",
    "\n",
    "# string to identify date columns\n",
    "str_checklist = [\"fecha\", \"fec\", \"inicio\", \"fin\", \"f_\"]\n",
    "\n",
    "# == Work with parameters ===================================================\n",
    "# Write to file\n",
    "ext.update_yaml_params(params_file, \"input_dir\", input_dir)\n",
    "ext.update_yaml_params(params_file, \"output_dir\", output_dir)\n",
    "ext.update_yaml_params(params_file, \"input_files\", input_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### person_id fields\n",
    "\n",
    "We need to define the columns that have the ID of each patient in the files. This code basically checks that the first column of the file contains something similar to 'NUHSA' and adds it to the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Create person_id ======================================================\n",
    "# -- Get columns with person_id info\n",
    "person_columns = {}\n",
    "for f in input_files[:]:\n",
    "    # Read data\n",
    "    table_raw = pd.read_parquet(data_dir / input_dir / f).reset_index()\n",
    "    init_cols = table_raw.columns\n",
    "    # Check if first column is like NUHSA\n",
    "    str_to_check = [\"NUHSA\", \"NUSA\"]\n",
    "    person_source_col = table_raw.columns[0]\n",
    "    if not any(x in person_source_col.upper() for x in str_to_check):\n",
    "        raise AssertionError(\n",
    "            f\"First column name ({person_source_col}) does not contain 'NUHSA'\"\n",
    "        )\n",
    "    person_columns[f] = person_source_col\n",
    "\n",
    "# Add to config file\n",
    "ext.update_yaml_params(params_file, \"person_columns\", person_columns)\n",
    "person_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date_fields\n",
    "\n",
    "Now we need to define which columns have the date information. The following code will identify columns whose name refers to something similar to a date and add it to the configuration file.\n",
    "\n",
    "- If there is one date, it will become start_date and end_date. \n",
    "- If there are two dates, the oldest column will be start_date and the most recent will be end_date.\n",
    "- If there are inconsistencies, i.e. not all older dates in one column come after their corresponding recent dates, an error will be raised. This should be fixed upstream. More specifically, in process_rare_files.py or similar.\n",
    "\n",
    "There should not be more than two dates. In that case it should be fixed upstream in process_rare_files.py or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Identify date columns ========================================================\n",
    "# Iteramos sobre los archivos\n",
    "date_columns = {}\n",
    "for f in input_files[:]:\n",
    "    # Look for columns with dates on each file\n",
    "    table_raw = pd.read_parquet(data_dir / input_dir / f).reset_index()\n",
    "    date_columns_names = ext.find_matching_keys(table_raw.columns, str_checklist)\n",
    "    date_columns[f] = date_columns_names\n",
    "\n",
    "# Add to config file\n",
    "ext.update_yaml_params(params_file, \"date_columns\", date_columns)\n",
    "date_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type_concept fields\n",
    "\n",
    "Finally, a code must be assigned to indicate the origin of the information. Here we simply assign an [OMOP code to identify the source of the record](https://github.com/OHDSI/Vocabulary-v5.0/wiki/Vocab.-TYPE_CONCEPT). For a comprehensive list of all valid Type Concept codes check [here](https://athena.ohdsi.org/search-terms/terms?domain=Type+Concept&standardConcept=Standard&page=1&pageSize=15&query=). \n",
    "\n",
    "In this case we are going to assign a code to each file, but there may be files with different information sources that can be included inside.\n",
    "\n",
    "Once the codes are assigned, we add them to the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_concept_mapping = {\n",
    "    \"01_Datos_Sociodemograficos.parquet\": 32817,  # EHR\n",
    "    \"02_Patologias_BPS.parquet\": 32840,  # EHR problem list\n",
    "    \"03_MPA.parquet\": 32856,  # Lab\n",
    "}\n",
    "\n",
    "ext.update_yaml_params(params_file, \"type_concept_mapping\", type_concept_mapping)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
