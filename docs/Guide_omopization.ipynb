{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omopization\n",
    "\n",
    "Once the files have been preprocessed, we can move on to prepare them for omopization. Some general modifications will be done to adapt the data to the omop format. These files are still the original files (after process_rare_files stage), but with changes into column names or extra columns. The actions that will be performed are:\n",
    "\n",
    "- Rename of the column that identifies the patient to 'person_id', as in OMOP.\n",
    "- Rename of date columns to a general 'start_date', 'end_date'. \n",
    "  - Most omop tables have a start_date and end_date, preceeding by a str that identifies the table. We will create those columns for each file. If only one date is present, it will be duplicated as both 'start_date' and 'end_date'.\n",
    "  - This ensures any future process does not have to deal with specific names of each file.\n",
    "- Assign a type_concept in relation to the origin of the information.\n",
    "\n",
    "\n",
    "## Procedure\n",
    "\n",
    "The typical script that will launch the omopization is:\n",
    " \n",
    "```python\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pyarrow.parquet as parquet\n",
    "\n",
    "from package.datasets import data_dir\n",
    "\n",
    "sys.path.append(\"external/bps_to_omop/\")\n",
    "import bps_to_omop.extract as ext\n",
    "import bps_to_omop.general as gen\n",
    "import bps_to_omop.person as per\n",
    "\n",
    "# %%\n",
    "# -- Define parameters ------------------------------------------------\n",
    "params_file = \"./package/preomop/omopization_params.yaml\"\n",
    "\n",
    "# -- Load parameters --------------------------------------------------\n",
    "print(\"Reading parameters...\")\n",
    "\n",
    "# -- Load yaml file and related info\n",
    "params_data = ext.read_yaml_params(params_file)\n",
    "input_dir = data_dir / params_data[\"input_dir\"]\n",
    "output_dir = data_dir / params_data[\"output_dir\"]\n",
    "input_files = params_data[\"input_files\"]\n",
    "person_columns = params_data[\"person_columns\"]\n",
    "date_columns = params_data[\"date_columns\"]\n",
    "type_concept_mapping = params_data[\"type_concept_mapping\"]\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# %%\n",
    "# -- transform the tables ---------------------------------------------\n",
    "print(\"Transforming tables...\")\n",
    "for f in input_files:\n",
    "    print(f\"- {f}\")\n",
    "    # Read data\n",
    "    table = parquet.read_table(input_dir / f)\n",
    "    cols_to_remove = []\n",
    "    # Remove the __index_level_0__ if exists\n",
    "    cols_to_remove += [\"__index_level_0__\"]\n",
    "\n",
    "    # -- person_id --------------------------------------------------------------------------------\n",
    "    # Get the person_id\n",
    "    person_id, person_source_value = per.transform_person_id(table, person_columns[f])\n",
    "    # Remove column from list to keep\n",
    "    cols_to_remove += [person_columns[f]]\n",
    "\n",
    "    # -- start_date and end_date ------------------------------------------------------------------\n",
    "    # Ensure they are ordered, i.e. end_date is after start_date\n",
    "    try:\n",
    "        start_date, end_date = ext.find_start_end_dates(\n",
    "            table, date_columns[f], verbose=0\n",
    "        )\n",
    "    except (ValueError, TypeError) as inst:\n",
    "        print(f\"Error found! {inst}\")\n",
    "        raise inst\n",
    "    # Remove columns from list to keep\n",
    "    cols_to_remove += date_columns[f]\n",
    "\n",
    "    # -- type_concept -----------------------------------------------------------------------------\n",
    "    # Create a columns with the code\n",
    "    type_concept_code = type_concept_mapping[f]\n",
    "    type_concept = gen.create_uniform_int_array(len(table), type_concept_code)\n",
    "\n",
    "    # -- Final steps ------------------------------------------------------------------------------\n",
    "    # Append to old table\n",
    "    print(f\"{f} input and output columns:\")\n",
    "    print(\" >\", table.column_names)\n",
    "    table = table.add_column(0, \"person_id\", person_id)\n",
    "    table = table.add_column(1, \"person_source_value\", person_source_value)\n",
    "    table = table.add_column(1, \"start_date\", start_date)\n",
    "    table = table.add_column(2, \"end_date\", end_date)\n",
    "    table = table.add_column(3, \"type_concept\", type_concept)\n",
    "    # Remove unnecesary columns\n",
    "    cols_to_keep = [col for col in table.column_names if col not in cols_to_remove]\n",
    "    table = table.select(cols_to_keep)\n",
    "    print(\" <\", table.column_names)\n",
    "\n",
    "    # Save to the same file\n",
    "    f_save = output_dir / f\n",
    "    parquet.write_table(table, f_save)\n",
    "\n",
    "print(\"Done!\\n\")\n",
    "```\n",
    "\n",
    "This file generates the folders needed to store the OMOP tables. To work it requires the configuration file in the following format:\n",
    "\n",
    " ```yaml\n",
    "input_dir: /path/to/input_dir/ # Common path where all input_files are located\n",
    "output_dir: /path/to/output_dir/ # Common path where all output_files will be located\n",
    "input_files:  \n",
    "  # Path to each file from input_dir\n",
    "  - /path/tp/file_1\n",
    "  - /path/tp/file_2\n",
    "person_columns:\n",
    "  # For each file, name of the column that contains the patient id\n",
    "  file_1: NUHSA_ENCRIPTADO\n",
    "  file_2: NUHSA_ENCRIPTADO\n",
    "date_columns:\n",
    "  # For each file, name of the column or columns that contains the dates\n",
    "  file_1:\n",
    "  - FECHA_INICIO\n",
    "  file_2:\n",
    "  - COD_FEC_INI_DIAGNOSTICO\n",
    "type_concept_mapping:\n",
    "  # For each file, omop code that represent the origin of the information\n",
    "  # See https://github.com/OHDSI/Vocabulary-v5.0/wiki/Vocab.-TYPE_CONCEPT\n",
    "  file_1: 32817\n",
    "  file_2: 32840\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to automatically generate params\n",
    "\n",
    "In this section we will automatically search all files to generate the params file. Take into account this are all dummy variables, and proper configuration is needed for it work.\n",
    "\n",
    "We need to provide:\n",
    "\n",
    "- `params_file` is a path to the parameters file. \n",
    "- `input_dir` is a str that defines the folder where raw data is.\n",
    "- `output_dir` is a str that defines the folder where output is going to be saved.\n",
    "- `input_files` is a list with all files to be processed. They can be filenames or relative paths from `input_dir`.\n",
    "- `str_checklist` is a list with substring that will be used to identify date columns. Anything containing any of these strings will be considered a date column.\n",
    "\n",
    "Take into account that the parameters `input_dir` and `output_dir` are defined in relation to the `data_dir` folder defined in the `.env` file. This way the general location of the files can remain hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from package.datasets import (\n",
    "    data_dir,\n",
    ")  # It is advisable to provide the data_dir as a module. load_dotenv can ve used too.\n",
    "\n",
    "sys.path.append(\"../external/bps_to_omop/\")  # Make sure the path is right!\n",
    "import bps_to_omop.extract as ext\n",
    "\n",
    "# == Define parameters ======================================================\n",
    "params_file = \"../package/preomop/omopization_params.yaml\"\n",
    "os.remove(params_file)\n",
    "\n",
    "input_dir = \"rare/02_cleaned/\"\n",
    "input_files = [\"01_sociodemo.parquet\", \"02_Patologias_BPS.parquet\", \"03_MPA.parquet\"]\n",
    "output_dir = \"done/\"\n",
    "\n",
    "# string to identify date columns\n",
    "str_checklist = [\"fecha\", \"fec\", \"inicio\", \"fin\", \"f_\"]\n",
    "\n",
    "# == Work with parameters ===================================================\n",
    "# Write to file\n",
    "ext.update_yaml_params(params_file, \"input_dir\", input_dir)\n",
    "ext.update_yaml_params(params_file, \"output_dir\", output_dir)\n",
    "ext.update_yaml_params(params_file, \"input_files\", input_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### person_id fields\n",
    "\n",
    "We need to define the columns that have the ID of each patient in the files. This code basically checks that the first column of the file contains something similar to 'NUHSA' and adds it to the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Create person_id ======================================================\n",
    "# -- Get columns with person_id info\n",
    "person_columns = {}\n",
    "for f in input_files[:]:\n",
    "    # Read data\n",
    "    table_raw = pd.read_parquet(data_dir / input_dir / f).reset_index()\n",
    "    init_cols = table_raw.columns\n",
    "    # Check if first column is like NUHSA\n",
    "    str_to_check = [\"NUHSA\", \"NUSA\"]\n",
    "    person_source_col = table_raw.columns[0]\n",
    "    if not any(x in person_source_col.upper() for x in str_to_check):\n",
    "        raise AssertionError(\n",
    "            f\"First column name ({person_source_col}) does not contain 'NUHSA'\"\n",
    "        )\n",
    "    person_columns[f] = person_source_col\n",
    "\n",
    "# Add to config file\n",
    "ext.update_yaml_params(params_file, \"person_columns\", person_columns)\n",
    "person_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date_fields\n",
    "\n",
    "Now we need to define which columns have the date information. The following code will identify columns whose name refers to something similar to a date and add it to the configuration file.\n",
    "\n",
    "- If there is one date, it will become start_date and end_date. \n",
    "- If there are two dates, the oldest column will be start_date and the most recent will be end_date.\n",
    "- If there are inconsistencies, i.e. not all older dates in one column come after their corresponding recent dates, an error will be raised. This should be fixed upstream. More specifically, in process_rare_files.py or similar.\n",
    "\n",
    "There should not be more than two dates. In that case it should be fixed upstream in process_rare_files.py or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Identify date columns ========================================================\n",
    "# Iteramos sobre los archivos\n",
    "date_columns = {}\n",
    "for f in input_files[:]:\n",
    "    # Look for columns with dates on each file\n",
    "    table_raw = pd.read_parquet(data_dir / input_dir / f).reset_index()\n",
    "    date_columns_names = ext.find_matching_keys(table_raw.columns, str_checklist)\n",
    "    date_columns[f] = date_columns_names\n",
    "\n",
    "# Add to config file\n",
    "ext.update_yaml_params(params_file, \"date_columns\", date_columns)\n",
    "date_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type_concept fields\n",
    "\n",
    "Finally, a code must be assigned to indicate the origin of the information. Here we simply assign an OMOP code to identify the source of the record. In this case we are going to assign a code to each file, but there may be files with different information sources that can be included inside.\n",
    "\n",
    "Once the codes are assigned, we add them to the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_concept_mapping = {\n",
    "    \"01_Datos_Sociodemograficos.parquet\": 32817,  # EHR\n",
    "    \"02_Patologias_BPS.parquet\": 32840,  # EHR problem list\n",
    "    \"03_MPA.parquet\": 32856,  # Lab\n",
    "}\n",
    "\n",
    "ext.update_yaml_params(params_file, \"type_concept_mapping\", type_concept_mapping)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
