{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pyarrow\n",
    "En este documento documentamos casos de uso empleando la librería pyarrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from pyarrow import csv\n",
    "from utils.metadata import folders\n",
    "\n",
    "# Cargamos cosas por si se usan después\n",
    "table_bps = csv.read_csv(f\"{folders['raw']}/01b_Sociodemograficos_fase4.txt\",\n",
    "                    parse_options=pa.csv.ParseOptions(delimiter='|'))\n",
    "\n",
    "table_bps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remapeo\n",
    "### pyarrow\n",
    "#### (F) pa.compute.replace_with_mask()\n",
    "\n",
    "Un mapeado de este tipo (usas un array con trues/falses) para substituir valores es fácil si los arrays son simples usando la función [pyarrow.compute.replace_with_mask](pyarrow.compute.replace_with_mask). Si son ChunkedArray, la máscara resultante también será un ChunkedArray, y ahí falla la función. \n",
    "\n",
    "#### pa.compute.if_else()\n",
    "\n",
    "Una solución, válida para un mapeado pequeño, es usar if-else. básicamente se va sustituyendo en función de dos valores. Si no es uno, pongo otro. Si hay más de dos casos, se pueden encadenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "# Creamos un ChunkedArray de prueba\n",
    "chunk1 = pa.array([1, 0, 1, 1, 0])\n",
    "chunk2 = pa.array([0, 1, 0, 0, 1])\n",
    "chunked_array = pa.chunked_array([chunk1, chunk2])\n",
    "\n",
    "print(\"Original ChunkedArray:\")\n",
    "print(chunked_array)\n",
    "\n",
    "# Usamos if_else para recodificar. Si no es 1, pues metemos el otro valor\n",
    "condition_1 = pc.equal(chunked_array, 1)\n",
    "recoded_array = pc.if_else(condition_1, 8507, 8532)\n",
    "\n",
    "print(\"\\nRecoded ChunkedArray:\")\n",
    "print(recoded_array)\n",
    "\n",
    "# Montamos una tabla para mostrarlo ordenado\n",
    "result = pa.table(\n",
    "    [chunked_array, recoded_array],\n",
    "    names=['original_value', 'recoded_value'])\n",
    "\n",
    "print(\"\\nResult Table:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que se traduce en la siguiente función para generalizar su uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "# Creamos un ChunkedArray de prueba\n",
    "def map_chunkedarray_if_else(\n",
    "        input_field: pa.ChunkedArray,\n",
    "        true_key: int,\n",
    "        value_if_true: int,\n",
    "        value_if_false: int) -> pa.ChunkedArray:\n",
    "    \"\"\"Given a ChunkedArray as input with binary values,\n",
    "    ie. only has two possible values within, recode these\n",
    "    original values to set of binary values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_field : pa.ChunkedArray\n",
    "        input ChunkedArray to recode\n",
    "    true_key : int\n",
    "        original value to recode to 'value_if_true'\n",
    "    value_if_true : int\n",
    "        value to recode to if original value is 'true_key'\n",
    "    value_if_false : int\n",
    "        value to recode to if original value is not 'true_key'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pa.ChunkedArray\n",
    "        resulting ChunkedArray\n",
    "    \"\"\"\n",
    "    assert type(input_field) == pa.ChunkedArray\n",
    "    assert type(true_key) == int\n",
    "    assert type(value_if_true) == int\n",
    "    assert type(value_if_false) == int\n",
    "\n",
    "    # Use if_else to recode the values\n",
    "    condition = pc.equal(chunked_array, true_key)\n",
    "    recoded_array = pc.if_else(condition, value_if_true, value_if_false)\n",
    "\n",
    "    # Combine the original and recoded arrays into a table\n",
    "    return recoded_array\n",
    "\n",
    "\n",
    "# Creamos un ChunkedArray de prueba\n",
    "chunk1 = pa.array([1, 0, 1, 1, 0])\n",
    "chunk2 = pa.array([0, 1, 0, 0, 1])\n",
    "chunked_array = pa.chunked_array([chunk1, chunk2])\n",
    "true_key = 1\n",
    "value_if_true = 8507\n",
    "value_if_false = 8532\n",
    "\n",
    "result = map_chunkedarray_if_else(\n",
    "    chunked_array, true_key, value_if_true, value_if_false)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función no es muy útil realmente, ya que sólo funciona en casos binarios, si no es el caso A, pues es el B. Se pueden añadir más if_else, pero no escala muy bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (F) pa.compute.case_when()\n",
    "\n",
    "Pyarrow también tiene una función llamada 'case_when()' que parece estar pensada para múltiples if-then. Se usa así:\n",
    "\n",
    "    pc.case_when(cond, /, *cases, memory_pool=None)\n",
    "\n",
    "cond debe ser un struct de booleans\n",
    "cases puede ser una mezcla de valores escalares o arrays con:\n",
    "    1- exactamente un hijo por cada condición\n",
    "    2- exactamente un hijo más por cada condición, que seria el valor else\n",
    "    \n",
    "Parece que cond indica una serie de condiciones y luego facilitas una lista con qué\n",
    "hacer en caso de que se cumpla cada condición.\n",
    "\n",
    "Esa lista de condiciones debe ser un struct. ¿Qué es un [struct](https://arrow.apache.org/docs/python/generated/pyarrow.struct.html#pyarrow-struct)? Se supone que es un formato de datos anidado parametrizado por una secuencia ordenada de valores. Ahí te quedas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    (True, pa.bool_()),\n",
    "    (False, pa.bool_()),\n",
    "]\n",
    "\n",
    "struct_type = pa.struct(fields)\n",
    "struct_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pa.compute.take()\n",
    "\n",
    "Se puede intentar con la función take(). Esta función escoge los valores de un array en función de los numeros en otro. Es decir, el array original se toma como un array con posiciones, posiciones que se usan para escoger un número en otro array. Esto se puede usar para mapear.\n",
    "\n",
    "Tiene la problemática de que hay que definir un array con tantas posiciones como el número más alto en nuestro array original. Teniendo en cuenta que los concept_id en una instancia OMOP-CDM puede tomar valores muy grandes, en el entorno de millones. Puede que no sea lo más eficiente en términos de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "# Create a sample ChunkedArray\n",
    "chunk1 = pa.array([1, 2, 3, 4, 5])\n",
    "chunk2 = pa.array([0, 1, 0, 0, 1])\n",
    "original_array = pa.chunked_array([chunk1, chunk2])\n",
    "\n",
    "# Define the mapping array\n",
    "mapping_array = pa.array([\n",
    "    0,  # 1st position (will map to 0 in original array)\n",
    "    0,  # 2nd position (will map to 1 in original array)\n",
    "    200,  # 3rd position (will map to 2 in original array)\n",
    "    300,  # 4th position (will map to 3 in original array)\n",
    "    400,  # 5th position (will map to 4 in original array)\n",
    "    500,  # 6th position (will map to 5 in original array)\n",
    "    600,  # 7th position (will map to 6 in original array)\n",
    "])\n",
    "\n",
    "# Use pc.take to substitute values based on the mapping array\n",
    "mapped_array = pc.take(mapping_array, original_array)\n",
    "\n",
    "# Show the result\n",
    "print(mapped_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy\n",
    "\n",
    "#### numpy.vectorize()\n",
    "\n",
    "El método \"clásico\" sería pasar el array a un numpy.array y una vez allí vectorizar el diccionario de mapeo. El \"problema\" de este método es que el mapeado tiene que ser completo, si hay casos en el array que no estén contemplados en el mapeado, saltará un error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Prepare\n",
    "original_array = pa.chunked_array([[1, 4, 3, 1, 2, 3, 0], [1, 2, 3, 0, 1]])\n",
    "original_array = original_array.to_numpy()\n",
    "print('original_array', original_array, original_array.dtype)\n",
    "# Make the mapping and vectorize it\n",
    "mapping_incomplete = {1: 8532, 0: 8507}\n",
    "mapping_complete = {1: 23, 2: 34, 3: 36, 4: 45, 0: 0}\n",
    "# Apply it and print results\n",
    "result = np.vectorize(mapping_complete.get)(original_array)\n",
    "print('resulting array', result, result.dtype)\n",
    "# Apply non-complete mapping and get the error\n",
    "result = np.vectorize(mapping_incomplete.get)(original_array)\n",
    "print('resulting array', result, result.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy.where()\n",
    "\n",
    "Otra opción, es usar np.where. Esto te permite ir dejando aquello que no esté cambiado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare\n",
    "original_array = pa.chunked_array([[1, 4, 3, 1, 2, 3, 0], [1, 2, 3, 0, 1]])\n",
    "original_array = original_array.to_numpy()\n",
    "print('original_array', original_array, original_array.dtype)\n",
    "# Apply mapping\n",
    "result = original_array.copy()\n",
    "result = np.where(result == 1 , 8532, result)\n",
    "result = np.where(result == 0 , 8507, result)\n",
    "print('original_array', result, result.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparación\n",
    "\n",
    "Vamos a replicar los métodos anteriores en funciones y a aplicarlos al mismo array para medir cuál es el más rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "def pa_if_else(original_array):\n",
    "    # Usamos if_else para recodificar. Si no es 1, pues metemos el otro valor\n",
    "    condition_1 = pc.equal(original_array, 1)\n",
    "    return pc.if_else(condition_1, 8507, 8532)\n",
    "\n",
    "def pa_take(original_array):\n",
    "    # Define the mapping array\n",
    "    mapping_array = pa.array([\n",
    "        8507,  # 1st position (will map to 0 in original array)\n",
    "        8532,  # 2nd position (will map to 1 in original array)\n",
    "        101,  # 3rd position (will map to 2 in original array)\n",
    "        102,  # 4th position (will map to 3 in original array)\n",
    "        103,  # 5th position (will map to 4 in original array)\n",
    "        104,  # 6th position (will map to 5 in original array)\n",
    "        105,  # 7th position (will map to 6 in original array)\n",
    "    ])\n",
    "    # Use pc.take to substitute values based on the mapping array\n",
    "    return pc.take(mapping_array, original_array)\n",
    "\n",
    "def np_vectorize(original_array):\n",
    "    original_array = original_array.to_numpy()\n",
    "    # Make the mapping and vectorize it\n",
    "    mapping_incomplete = {1: 8532, 0: 8507}\n",
    "    mapping_complete = {1: 23, 2: 34, 3: 36, 4: 45, 0: 0}\n",
    "    # Apply it and print results\n",
    "    return np.vectorize(mapping_incomplete.get)(original_array)\n",
    "\n",
    "def np_where(original_array):\n",
    "    original_array = original_array.to_numpy()\n",
    "    # Apply mapping\n",
    "    result = original_array.copy()\n",
    "    result = np.where(result == 1 , 8532, result)\n",
    "    result = np.where(result == 0 , 8507, result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparamos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare chunk\n",
    "chunk1 = pa.array([1, 0, 1, 1, 0])\n",
    "chunk2 = pa.array([0, 1, 0, 0, 1])\n",
    "original_array = pa.chunked_array([chunk1, chunk2])\n",
    "\n",
    "# == == == == == == == == == == == == == == == == == \n",
    "# pyarrow if_else()\n",
    "print('pyarrow if_else():')\n",
    "try:\n",
    "    %timeit pa_if_else(original_array)\n",
    "except:\n",
    "    print(' => Failed!')\n",
    "# == == == == == == == == == == == == == == == == == \n",
    "# pyarrow take()\n",
    "print('pyarrow take():')\n",
    "try:\n",
    "    %timeit pa_take(original_array)\n",
    "except:\n",
    "    print(' => Failed!')\n",
    "# == == == == == == == == == == == == == == == == == \n",
    "# numpy vectorize()\n",
    "print('numpy vectorize:')\n",
    "try:\n",
    "    %timeit np_vectorize(original_array)\n",
    "except:\n",
    "    print(' => Failed!')\n",
    "# == == == == == == == == == == == == == == == == == \n",
    "# numpy where\n",
    "print('numpy where:')\n",
    "try:\n",
    "    %timeit np_where(original_array)\n",
    "except:\n",
    "    print(' => Failed!')\n",
    "# == == == == == == == == == == == == == == == == == \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo más rápido parece ser el np.where(). Esto funciona bien en casos binarios. Vamos a probar casos con un mapeo más complicado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefinimos las funciones para que hagan uso de un mapping array externo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def pa_take_map(original_array, mapping_array):\n",
    "    # En este caso, el mapeado es directamente la lista\n",
    "    mapping_tmp = [mapping_array[k] for k in mapping_array]\n",
    "    # Use pc.take to substitute values based on the mapping array\n",
    "    return pc.take(mapping_tmp, original_array)\n",
    "\n",
    "\n",
    "def np_vectorize_map(original_array, mapping_array):\n",
    "    # convert to numpy\n",
    "    original_array = original_array.to_numpy()\n",
    "    # Apply it and print results\n",
    "    return np.vectorize(mapping_array.get)(original_array)\n",
    "\n",
    "\n",
    "def np_where_map(original_array, mapping_array):\n",
    "    # Convert to numpy\n",
    "    original_array = original_array.to_numpy()\n",
    "    # Apply mapping\n",
    "    result = original_array.copy()\n",
    "    for key in mapping_array:\n",
    "        result = np.where(result == key, mapping_array[key], result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rehacemos las pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Parametros\n",
    "chunk_number = 10\n",
    "chunk_size = 1000\n",
    "mapping_size = 1000\n",
    "\n",
    "\n",
    "def test_func(func, original_array, mapping, n_tries=5):\n",
    "    print(f\" - Testing {func.__name__} ...\")\n",
    "    try:\n",
    "        elapsed_time = []\n",
    "        for n in range(n_tries):\n",
    "            t_start = time.time_ns()\n",
    "            func(original_array, mapping)\n",
    "            elapsed_time.append(time.time_ns() - t_start)\n",
    "        return np.mean(elapsed_time)\n",
    "    except:\n",
    "        print(f'  => {func.__name__} failed!')\n",
    "\n",
    "\n",
    "def test_times(chunk_number, chunk_size, mapping_size):\n",
    "    # == Generamos el mapping ==\n",
    "    # Suponemos un caso con 1000 valores distintos. Como están en orden, esto me asegura que pa.compute.take() puede funcionar.\n",
    "    keys = np.arange(mapping_size)\n",
    "    # Vamos a emparejar estos valores con otros tantos por encima. Nos aseguramos de que no haya reemplazamiento para que el emparejamiento sea unívoco.\n",
    "    values = np.random.choice(keys+mapping_size*10, size=mapping_size, replace=False)\n",
    "    # Generamos un diccionario relacionando unos con otros.\n",
    "    mapping = dict(zip(keys, values))\n",
    "\n",
    "    # == Generación de datos ==\n",
    "    # Vamos a generar un ChunkedArray con 100 valores\n",
    "    np.random.seed(42)\n",
    "    original_array = []\n",
    "    for i in range(chunk_number):\n",
    "        original_array.append(np.random.choice(\n",
    "            keys, size=chunk_size, replace=True))\n",
    "    original_array = pa.chunked_array(original_array)\n",
    "    original_array\n",
    "\n",
    "    print(f\"> Testing for {chunk_number} chunks of {chunk_size} elements using a {mapping_size}-element mapping:\")\n",
    "    result = []\n",
    "    for func in [pa_take_map, np_vectorize_map, np_where_map]:\n",
    "        elapsed_time = test_func(func, original_array, mapping)\n",
    "        result.append({\n",
    "            'chunk_number': chunk_number, 'chunk_size': chunk_size, 'mapping_size': mapping_size,\n",
    "            'func': func.__name__, 'time (us)': np.round(elapsed_time/1e9, 4)})\n",
    "    # == == == == == == == == == == == == == == == == ==\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiempos = []\n",
    "for chunk_n in [100,1000,10000]:\n",
    "    result = test_times(chunk_n,100,100)\n",
    "    tiempos += result\n",
    "for chunk_s in [100,1000,10000]:\n",
    "    result = test_times(100,chunk_s,100)\n",
    "    tiempos += result\n",
    "for mapp_s in [100,1000,10000,100000]:\n",
    "    result = test_times(100,100,mapp_s)\n",
    "    tiempos += result\n",
    "tiempos += result\n",
    "df_general = pd.DataFrame(tiempos)\n",
    "df_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura con todas\n",
    "plt.figure()\n",
    "for func in ['pa_take_map', 'np_vectorize_map', 'np_where_map']:\n",
    "    idx = (df_general['func']==func) & (df_general['chunk_size']==100) & (df_general['mapping_size']==100)\n",
    "    print(df_general.loc[idx])\n",
    "    sns.regplot(df_general.loc[idx] , x='chunk_number',y='time (us)', label=func,ci=None)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Figura con todas\n",
    "plt.figure()\n",
    "for func in ['pa_take_map', 'np_vectorize_map', 'np_where_map']:\n",
    "    idx = (df_general['func']==func) & (df_general['chunk_number']==100) & (df_general['mapping_size']==100)\n",
    "    print(df_general.loc[idx])\n",
    "    sns.regplot(df_general.loc[idx] , x='chunk_size',y='time (us)', label=func,ci=None)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay un incremento del tiempo de procesamiento con el número de chunks, lo cual era previsible. \n",
    "Más o menos se incrementa un orden de magnitud en el tiempo por cada orden de magnitud incrementado en el chunk_size.\n",
    "Ocurre lo mismo cuando aumentamos chunk_size.\n",
    "\n",
    "np.where es el que más tarda, seguido de np.vectorize y finalmente pc.take(). Parece además que tanto np.vectorize() como pc.take() sufren menos si los chunks son más \n",
    "grandes que si hacemos más chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura con todas\n",
    "plt.figure()\n",
    "for func in ['pa_take_map', 'np_vectorize_map', 'np_where_map']:\n",
    "    idx = (df_general['func']==func) & (df_general['chunk_number']==100) & (df_general['chunk_size']==100)\n",
    "    print(df_general.loc[idx])\n",
    "    sns.regplot(df_general.loc[idx] , x='mapping_size',y='time (us)', label=func,ci=None)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando aumentamos eltamaño del mapeo, el único que sufre realmente es np.where(). Las otras dos, o bien suben el tiempo más lentamente, o bien llegan a un tope y listo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen\n",
    "La que peor escala es np.where(). A medida que aumenan el número de chunks el tiempo aumenta linealmente. Sería mejor evitarla en general.\n",
    "\n",
    "La que mejor funciona en general es pa.compute.take(). El problema es definir bien ese mapeo sin que haya errores.\n",
    "\n",
    "La más sencilla de usar y que podría funcionar bien en la mayoría de los casos es np.vectorize().\n",
    "\n",
    "Si es posible, aumentar primero el tamaño de los chunks antes que su número.\n",
    "\n",
    "El tamaño del mapa no es muy importante a menos que se use np.where()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jugando con pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "\n",
    "# Parece que pyarrow hace copias al renombrar variables.\n",
    "# => No hay problemas con modificar variables de las que\n",
    "#    dependen otras variables\n",
    "\n",
    "# Sacamos el array que transformaremos\n",
    "inpField = table_bps['COD_NUHSA']\n",
    "print('inpField',inpField[:5])\n",
    "# Guardamos ya el _source_value\n",
    "person_source_value = inpField\n",
    "print('person_source_value',person_source_value[:5])\n",
    "# Si hay que hacer transformaciones, hacerlas aquí\n",
    "inpField = pc.utf8_slice_codeunits(inpField,2)\n",
    "inpField = inpField.cast(pa.int32())\n",
    "print('inpField',inpField[:5])\n",
    "print('person_source_value',person_source_value[:5])\n",
    "\n",
    "# ==> inpField se ha modificado mientras que person_source_value sigue como estaba \n",
    "#     cuando se le asignó el valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer muchos archivos\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv\n",
    "\n",
    "column_names = {}\n",
    "for f in file_list[0:]:\n",
    "    print(f)\n",
    "    if 'Farmacia' in f.split('/'):\n",
    "        parse_options = pa.csv.ParseOptions(delimiter=';')\n",
    "        read_options = pa.csv.ReadOptions(block_size=5000,skip_rows=5)\n",
    "    else:\n",
    "        parse_options = pa.csv.ParseOptions(delimiter='|')\n",
    "        read_options = pa.csv.ReadOptions(block_size=5000,skip_rows=0)\n",
    "        \n",
    "    with pyarrow.csv.open_csv(f,\n",
    "                              parse_options=parse_options,\n",
    "                              read_options=read_options) as reader:\n",
    "        for next_chunk in reader:\n",
    "            if f not in column_names.keys():\n",
    "                column_names[f] = next_chunk.column_names\n",
    "            else:\n",
    "                break\n",
    "\n",
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando con nans\n",
    "Tengo casos en los que voy a ir comparando fechas y calculando los días entre ellas. Qué pasa si tengo nans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "a = pa.array([8,None],pa.int64())\n",
    "b = pa.array([9,9],pa.int64())\n",
    "print(pc.greater(b,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si comparo dos vectores que incluyen un nan. La función me devuelve nan allá donde lo haya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.all(pc.greater(b,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peero, si tengo un arrays de pyarrow.BooleanScalar y quiero ver si todos son True, **pyarrow ignorará los nan/null y sólo mirará si el resto cumplen la condición**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si calculo los días...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "a = pa.scalar(datetime(2012, 1, 1), type=pa.date64())\n",
    "b = pa.scalar(datetime(2012, 1, 2), type=pa.date64())\n",
    "c = pa.scalar(datetime(2012, 1, 3), type=pa.date64())\n",
    "\n",
    "A = pa.array([a,None],pa.date64())\n",
    "B = pa.array([b,c],pa.date64())\n",
    "\n",
    "pc.days_between(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me devuelve null."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
