{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join raw data\n",
    "\n",
    "In this notebook we will define and create the parameters file to process the raw data. This notebook is expected to be run in one full run, running every cell once configuration is done.\n",
    "\n",
    "The tasks to do are:\n",
    "1. Select which files from the raw directory are going to be used for the OMOP conversion.\n",
    "2. Try to open the files and select the optimal parameters to do so\n",
    "3. Identify the date columns at each file and convert them to a date format\n",
    "4. Save all files in the same directory in parquet file for faster further processing\n",
    "\n",
    "The following snippet of code will retrieve the gathered information and apply the transformation to the files.\n",
    "\n",
    "```python \n",
    "from package.datasets import data_dir\n",
    "\n",
    "import external.bps_to_omop.bps_to_omop.extract as ext\n",
    "\n",
    "# -- PARAMETERS -------------------------------------------------------\n",
    "# -- Get environment variables and create output dir\n",
    "config_file = \"./hepapred/preomop/process_raw_data_params.yaml\"\n",
    "\n",
    "# -- MAIN -------------------------------------------------------------\n",
    "# Apply the changes\n",
    "yaml_dict = ext.read_yaml_config(config_file)\n",
    "ext.apply_modifications(data_dir, config_file, verbose=1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the document will describe how to automatically populate the configuration file. \n",
    "\n",
    "It is also possible to do it manually, here is an example of the expected format:\n",
    "\n",
    "```yaml\n",
    "input_dir: raw/\n",
    "input_files:\n",
    "  input_files:\n",
    "  - path_to/file1.txt\n",
    "  - path_to/file2.txt\n",
    "output_dir: rare/01_parquet/\n",
    "output_files:\n",
    "  path_to/file1.txt: out1.parquet\n",
    "  path_to/file2.txt: out2.parquet\n",
    "read_options:\n",
    "  path_to/file1.txt:\n",
    "    encoding: utf-8\n",
    "    sep: '|'\n",
    "  path_to/file2.txt:\n",
    "    encoding: utf-8\n",
    "    sep: ';'\n",
    "date_columns:\n",
    "  path_to/file1.txt:\n",
    "  - COD_FEC_FALLECIMIENTO\n",
    "  - COD_FEC_NACIMIENTO\n",
    "  path_to/file2.txt:\n",
    "  - COD_FEC_INI_PATOLOGIA\n",
    "date_formats:\n",
    "  path_to/file1.txt:\n",
    "    COD_FEC_FALLECIMIENTO:\n",
    "      errors: coerce\n",
    "      format: ISO8601\n",
    "    COD_FEC_NACIMIENTO:\n",
    "      errors: raise\n",
    "      format: ISO8601\n",
    "  path_to/file2.txt:\n",
    "    COD_FEC_INI_PATOLOGIA:\n",
    "      errors: raise\n",
    "      format: ISO8601\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification of needed files\n",
    "\n",
    "It is often useful to include here a reasoning about what files to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of parameters\n",
    "\n",
    "Now we need to define the parameters to perform the translation.\n",
    "\n",
    "First of all, we need to know where to save the configuration file (`yaml_path`). Usually it is placed next to [process_raw_data.py](../../../hepapred/preomop/process_raw_data.py). \n",
    "\n",
    "The parameters `input_dir` and `output_dir` are defined in relation to the `data_dir` folder defined in the `.env` file. This way the general location of the files can remain hidden.\n",
    "- `input_dir` is a str that defines the folder where raw data is.\n",
    "- `output_dir` is a str that defines the folder where output is going to be saved.\n",
    "- `input_files` is a list with all files to be processed. They can be filenames or relative paths from `input_dir`.\n",
    "- `output_files` is a dict that maps every item in `input_files` to its new name. Make sure to finish it with *.parquet*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Append the location of the submodule bps_to_omop to PATH\n",
    "sys.path.append(\"../external/bps_to_omop/\")\n",
    "\n",
    "import bps_to_omop.extract as ext\n",
    "from hepapred.datasets import data_dir\n",
    "\n",
    "# == Define parameters ===========================================\n",
    "# Parameters file path\n",
    "yaml_path = \"../package/preomop/process_raw_data_params.yaml\"\n",
    "\n",
    "# Path to relevant files\n",
    "input_dir = \"raw/\"\n",
    "# Define saving directory\n",
    "output_dir = \"rare/01_parquet/\"\n",
    "\n",
    "# Input files to be used\n",
    "input_files = [\n",
    "    \"20231112/file1.txt\",\n",
    "    \"20231112/casos/file2.txt\",\n",
    "]\n",
    "# Remap to new names (make sure they are parquet)\n",
    "output_files = {\n",
    "    \"20231112/file1.txt\": \"out1.parquet\",\n",
    "    \"20231112/casos/file2.txt\": \"out2.parquet\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following piece of code write the parameters and prepares the full path to the actual files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Preconfiguration ==========================================\n",
    "# write params to configuration file\n",
    "ext.update_yaml_config(yaml_path, \"input_dir\", input_dir)\n",
    "ext.update_yaml_config(yaml_path, \"input_files\", {\"input_files\": input_files})\n",
    "ext.update_yaml_config(yaml_path, \"output_dir\", output_dir)\n",
    "ext.update_yaml_config(yaml_path, \"output_files\", output_files)\n",
    "\n",
    "# Create the full path to the folder with files\n",
    "raw_data_dir = data_dir / input_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing of reading parameters\n",
    "\n",
    "This section deals with the reading of the files.\n",
    "- `default_params` contains a dictionary with the parameters in pandas.read_csv() that will always work.\n",
    "  - If every file uses the same separator. You can put it here.\n",
    "- `candidate_params` contains a dictionary of dictionaries. Each key is a parameter in pandas.read_csv() and their values are the possible values to try.\n",
    "  - This is useful if some files have a rows to skip and others do not.\n",
    "- `funcs_to_check` contains a list of functions that, applied on the read dataframe, would raise an error if there's something weird.\n",
    "  - This is here for possible future convenience. So far it has been better to just check the output.\n",
    "  - Currently checks that:\n",
    "    - The file is actually read.\n",
    "    - The resulting dataframe has more than 1 column.\n",
    "\n",
    "The function `get_reading_params()` will try every possible combination of parameters and save the first that work on the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Format file config ========================================\n",
    "# Default parameters (works for every file)\n",
    "default_params = {\"sep\": \"|\"}\n",
    "\n",
    "# Possible candidates to try\n",
    "candidate_params = {\n",
    "    \"encoding\": [\"latin9\", \"utf-8\"],\n",
    "}\n",
    "# Extra functions to check if read data makes sense\n",
    "funcs_to_check = []\n",
    "\n",
    "# == Main ======================================================\n",
    "print(\"Extraction config:\")\n",
    "# Retrieve appropiate read_options\n",
    "read_options = ext.get_reading_params(\n",
    "    raw_data_dir,\n",
    "    input_files,\n",
    "    default_params,\n",
    "    candidate_params,\n",
    "    funcs_to_check,\n",
    "    verbose=1,\n",
    ")\n",
    "ext.update_yaml_config(yaml_path, \"read_options\", read_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing of date columns\n",
    "\n",
    "This section deals with the columns of the files containing dates.\n",
    "\n",
    "First, the function `find_matching_keys_on_files()` will try every possible combination of parameters and save the first that work on the configuration file. Default strings are: *\"fecha\", \"fec\", \"inicio\", \"fin\" and \"f_\"* (case insensitive), but more can be added using the parameters `search_words`. See docstring of `find_matching_keys_on_files()` for more information.\n",
    "\n",
    "Secondly, the function `get_date_parser_options()` will try to parse the column dates. To do so it will read a limited number of rows in the file and try to parse the date columns. It will first to try to nicely transform to datetime. If no combination works nicely, it will try to coerce the transformation, reporting (if verbose >= 1) the number of values that were transformed to nans/nulls in the process.\n",
    "\n",
    "- `candidate_params` contains a dictionary of dictionaries. Each key is a parameter in pandas.to_datetime() function and their values are the possible values to try.\n",
    "  - This is useful if different files have different date formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Date extraction config ====================================\n",
    "# Define possible parameters to extract the date\n",
    "candidate_params = {\n",
    "    \"format\": [\n",
    "        \"ISO8601\",  # This is a fast read, eq to \"%Y%m%d\",\"%Y/%m/%d\",\"%Y-%m-%d\",\n",
    "        \"%d/%m/%y\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# == Main =====================================================\n",
    "print(\"Date format config:\")\n",
    "# Get the date column names of the files\n",
    "date_columns = ext.find_matching_keys_on_files(\n",
    "    raw_data_dir, input_files, read_options, verbose=1\n",
    ")\n",
    "# Append info to yaml_dict\n",
    "ext.update_yaml_config(yaml_path, \"date_columns\", date_columns)\n",
    "print('\\n')\n",
    "\n",
    "# Test possible date parser options\n",
    "(date_formats, date_formats_coercions) = ext.get_date_parser_options(\n",
    "    raw_data_dir,\n",
    "    input_files,\n",
    "    date_columns,\n",
    "    candidate_params,\n",
    "    read_options,\n",
    "    verbose=1,\n",
    "    nrows=5000,\n",
    ")\n",
    "\n",
    "# Append info to yaml_dict\n",
    "ext.update_yaml_config(yaml_path, \"date_formats\", date_formats)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
